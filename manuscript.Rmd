---
title             : "Sampling Strategies in Decisions from Experience: Property Representation and Probability Weighting Approximation"
shorttitle        : "Sampling strategies"

author: 
  - name          : "Linus Hof"
    affiliation   : "1"
    corresponding : yes    
    address       : "Department of Psychology, Heidelberg University, Hauptstra√üe 47-51, 69117 Heidelberg, Germany"
    email         : "linushof@posteo.de"
    
affiliation:
  - id            : "1"
    institution   : "Heidelberg University"

note: | 
  **Unsubmitted draft created from the commit with the hash**: ``r repro::current_hash()``
  
authornote: |
 This is a dynamic document which can be reproduced from the accompanying GitHub repository: https://github.com/linushof/sampling-in-dfe. 
 
 The current version of this manuscript is prepared for submission as a master's thesis.
 Supervisors: Thorsten Pachur & Veronika Zilker.   
 
abstract: |
  Add abstract
  
keywords          : ""
wordcount         : ""

bibliography      : references_sampling-in-dfe.bib
csl               : apa.csl

figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
linkcolor         : "blue"
mask              : no
draft             : no

classoption       : "man"
documentclass     : "apa7"
output: papaja::apa6_pdf

header-includes:
   - \usepackage[capposition=top]{floatrow}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, #omit messages
                      fig.align = "l", #align figures to left margin
                      fig.pos = "t" # display figures at the top of a page
                      )
```

```{r packages, include=FALSE}
#load required packages
pacman::p_load(papaja,
               knitr,
               readr,
               tidyr, 
               dplyr,
               ggplot2,
               viridis,
               ggpubr,
               latex2exp)
```

<!-- Introduction -->

The human mind is a cognitive system that operates on inputs from an environment or memory, and in the domain of decision making, such inputs provide information about the choice alternatives between which people can choose.
Indeed, irrespective of their exact mathematical specification or computational implementation, decision models generally take as inputs information about a set of choice alternatives, each possessing some defining *properties*, and return a subset of chosen alternatives as outputs [@heOntologyDecisionModels2020].
Decision making can therefore be understood as an information-processing problem whose solution requires a selection among choice alternatives.

Now, let a *prospect* be a choice alternative that possesses only two kinds of fundamental properties: the possible outcomes of the alternative (e.g., monetary gains or losses of some amount) and the probabilities with which these outcomes occur following the choice of the alternative.
Importantly, the information that inputs provide about these properties of a prospect can come in different forms.
For example, in *decisions from description* (DfD), inputs take the form of explicit and complete descriptions of all outcomes and probabilities.
Yet, in *decisions from experience* [DfE, @hertwigDecisionsExperienceEffect2004], these properties are latent and therefore not known with certainty, but they must be inferred from the relative frequencies with which the outcomes occurred when the prospect was chosen in similar past decisions.
The inputs to DfE therefore take the form of sampled outcomes.
Although these distinct input forms may in principle carry the same information about a given set of prospects, behavioral decision research has so far produced a large body of papers indicating that---taking a quite general perspective---there are robust differences between the choices that are made in DfD and DfE, leading to the notion of the *description-experience gap* [@hertwigDescriptionexperienceGapRisky2009].
In one reading of the gap,^[<!--Open footnote-->See @hertwigConstructBehaviorGap2018, for a discussion on the interpretation of the gap.<!--Close footnote-->]
empirical choice patterns in DfD deviate from the principle of expected value (EV) maximization (see below) as if a small-probability outcome is given more weight than would be warranted by its objective probability;
in turn, in DfE, ceteris paribus, people choose as if the same small-probability outcome is given less weight than would be objectively warranted [e.g., @barronSmallFeedbackbasedDecisions2003; @hertwigDecisionsExperienceEffect2004; @weberPredictingRiskSensitivity2004; @erevAnomaliesForecastsDescriptive2017; see @wulffMetaanalyticReviewTwo2018, for a comprehensive meta-analytic review].
Hereafter, these choice patterns are referred to as the *as-if overweighting* and *as-if underweighting of rare outcomes*, respectively.

Similar to other behavioral phenomena that could be explained by assuming that the mind is sensitive to the composition of the *samples*---i.e., sets of sampled outcomes---on which it operates [see @fiedlerBewareSamplesCognitiveecological2000], the as-if underweighting of rare outcomes in DfE could be explained by assuming that the mind is sensitive to the information provided by the small samples that people tend to rely on [see @foxDecisionsExperienceSampling2006; @hertwigDecisionsExperienceEffect2004; @hertwigDecisionsExperienceWhy2010; @rakowBiasedSamplesNot2008; @plonskyRelianceSmallSamples2015; @wulffMetaanalyticReviewTwo2018].
Specifically, following from the laws of large numbers [see, e.g., @kolmogorovFoundationsTheoryProbability1950], the relative frequencies with which the prospect's outcomes occur in a sample of infinite size converge almost surely to the respective objective probabilities.
In such a case, then, or in the less strict case of large samples, the binomial distributions of the outcomes can be approximated by a symmetric normal distribution [cf. @georgiiStochastikEinfuhrungWahrscheinlichkeitstheorie2015].
However, for samples of small size, the binomial distribution associated with small-probability outcomes is positive skewed, therefore causing the relative frequencies with which these outcomes occur in the sample to be smaller rather than larger than their latent objective probabilities.
Thus, small-probability outcomes tend to be underrepresented in small samples and might not occur at all.
As a consequence, were the mind to use the sampled relative frequencies of outcomes to infer their latent objective probabilities and to follow the principle of EV maximization, the reliance on small samples could explain the as-if underweighting of rare outcomes.

<!--Now, sequential sampling models of decision making [e.g., @bhatiaSequentialSamplingParadoxes2014; @busemeyerDecisionFieldTheory1993; @markantModelingChoiceSearch2015; @ratcliffComparisonSequentialSampling2004] form one class of process models that provide an algorithmic link between the actual sampling process and the choices in DfE.
That is, in one paradigm used to study DfE, namely, the sampling paradigm [see @weberPredictingRiskSensitivity2004], before making a final choice between prospects with latent properties, people can, as much as they want, sequentially sample the prospects' outcomes according to their objective probabilities.
To relate the sampling process to the final choice, sequential sampling models assume that the entire information-processing chain upfront the choice takes the form of an evidence accumulation process over time, with choices becoming more accurate the more evidence must be accumulated.
Specifically, considering the choice between two prospects, these models assume that the prospects' outcomes are sequentially sampled and integrated into dynamic decision variables and that a choice is made once a decision variable exceeds a threshold in favor for one of the prospects.
Notably, since the thresholds determine how much (more) evidence a prospect must accumulate in order to be chosen and since a decrease in the threshold leads to a decrease in the number of outcomes that must be sampled to reach this threshold, a sequential sampling model may be used to relate the reliance on small samples to an as-if underweighting of rare outcomes [see @markantModelingChoiceSearch2015].
That is, for low thresholds, small-probability outcomes are predicted to contribute less (rather than more) to the accumulated evidence than would be warranted by their objective probabilities because of the positive skewness of their binomial distribution.
In turn, the higher a threshold, and thus the higher the number of outcomes that must be sampled, the more should the amount of evidence contributed by small-probability outcomes correspond to what would be objectively warranted.-->

Yet, some studies show that the as-if underweighting of rare outcomes may appear, even if the overall number of sampled outcomes is large and the relative frequencies across all sampled outcomes closely resemble the latent objective probabilities [@hauDescriptionexperienceGapRisky2008; @hauDecisionsExperienceStatistical2010; @ungemachAreProbabilitiesOverweighted2009; see also @wulffMetaanalyticReviewTwo2018].
To this end, this paper argues that there may be a discrepancy between the overall number of sampled outcomes and the size of the samples that the mind actually relies on to evaluate the prospects or to make an inference about their latent properties, respectively.
However, in contrast to other approaches, this paper does not assume that some of the sampled outcomes are discounted [e.g., @hertwigDecisionsExperienceEffect2004] or ignored [e.g., @cohenEffectPerceivedPatterns2021; @plonskyRelianceSmallSamples2015].
Rather, a core assumption of this paper is that any number of outcomes sampled from a prospect may either be integrated into the same sample<!-- ---as it is implied by the sequential sampling models described above--- --> or into multiple respectively smaller samples.
Then, this paper presents a process model that links the actual sampling process to the choices in DfE and demonstrates that a reliance on multiple small samples can cause an as-if underweighting of rare outcomes while fully accounting for a large number of sampled outcomes.

Therefore, consider first the finding that *sampling strategies* were indicative for the final choice in DfE.
Specifically, in one paradigm used to study DfE, namely, the sampling paradigm [see @weberPredictingRiskSensitivity2004], before making a final choice between prospects with latent properties, people can, as much as they want, sequentially sample the prospects' outcomes according to their objective probabilities.
Considering this paradigm and the choice between two prospects, @hillsInformationSearchDecisions2010 distinguished sampling strategies along the frequency of switches between the two prospects during the sampling phase,^[<!--Open footnote-->Note that @hillsInformationSearchDecisions2010 proposed two paradigmatic sampling strategies, where prospects are either switched just once (comprehensive sampling) or after each sampled outcome (piecewise sampling).
Nevertheless, in their analyses of empirical sampling data, they used the number of switches between prospects relative to the overall number of sampled outcomes from both prospects (switching frequency) as an approximate measure for the two proposed sampling strategies, and further acknowledged that "many [sampling] strategies will fall on the continuum between [the two paradigmatic strategies]" [@hillsInformationSearchDecisions2010, p. 1788].
Respectively, in the current paper sampling strategies are defined as switching probabilities (see below).
<!--Close footnote-->] 
and proposed two different decision strategies that people may adopt.
That is, for a summary decision strategy, the prospect is chosen that produced the greater mean across all sampled outcomes.
In turn, for a round-wise decision strategy, the prospect is chosen that produced the greater outcome most of the time.
In other words, the round-wise strategy assumes that over multiple rounds one outcome from each of the two prospects is sampled.
In each round, then, the two sampled outcomes are compared and the prospect is chosen that wins most of these comparison rounds.
Then, @hillsInformationSearchDecisions2010 found that the choices of people who switched frequently between prospects during the sampling phase were better predicted by assuming that they adopted a round-wise strategy (rather than a summary strategy), and that this predictive pattern reversed for people who switched infrequently.

Now, one explanation for such a reversed predictive pattern would be if the differences in the sampling strategies determine which of the two decision strategies is used.
To this end, this paper presents a model that provides an algorithmic link between the *switching probability*---i.e., the probability of sampling outcomes from different prospects in direct succession during the sampling phase---, the decision strategy, and the respective choice.
That is, the proposed model is such that the higher (lower) the switching probability during the sampling phase, the more the evaluation of the prospects resembles the round-wise strategy (summary strategy).
However, to allow for a gradual transition from the summary to the round-wise strategy with increases in the switching probability, the model also captures hybrids of both strategies.
Specifically, the proposed model accumulates the evidence obtained from the comparisons between sample means---i.e. won comparisons---with the size of the samples underlying each mean comparison decreasing with an increase in the switching probability, and thresholds determining how much (more) of such mean comparisons a prospect must win in order to be chosen.
Thus, the model presented in this paper is such that the higher (lower) the switching probabilities and the higher (lower) the thresholds, the more the evaluation of prospects or the evidence accumulation process, respectively, resembles the round-wise strategy (summary strategy).

Moreover, this paper demonstrates in a computational analysis that the evaluation of prospects, while taking into account an increasing number of sampled outcomes, can rely on multiple small samples, causing an as-if underweighting of rare outcomes.
Specifically, considering choices between a two-outcome prospect and a safe prospect, the proposed model is used to simulate the evidence accumulation process for different combinations of switching probabilities and threshold values.
Then, on the basis of choice rates and parameter estimates of cumulative prospect theory [CPT, @tverskyAdvancesProspectTheory1992; @wakkerAxiomatizationCumulativeProspect1993], it is shown that the simulated choices converge to the solutions of the principle of EV maximization for decreasing switching probabilities, but reflect an increasingly pronounced as-if underweighting of rare outcomes, the more the switching probabilities increase. 
The analysis also shows that these differences between switching probabilities remain stable for different threshold values.
Accordingly, the computational analysis substantiates the claim that the reliance on (multiple) small samples, but not necessarily the consideration of a small number of sampled outcomes, causes an as-if underweighting of rare outcomes in DfE.
In other words, because the switching probabilities change the size of the samples underlying each mean comparison, the model predicts them to affect whether a low-probability outcome contributes to the majority of required mean comparisons about as much as would be warranted based on its objective probabilities, or whether they contribute rather less.
Yet, since each comparison's contribution to the accumulated evidence is weighted equally in the form of round wins, the model does not predict the thresholds to have a similar effect.
Rather, whatever systematic differences in the latent properties of prospects may be represented in a mean comparison, should be more accurately assessed as the thresholds---and therefore also the number of sampled outcomes that are taken into account during the evaluation of prospects---increase.
To this end, this paper argues that the mind may rely on different sampling strategies or switching probabilities, respectively, to change for which differences in the latent properties of prospects evidence is accumulated.
Before presenting the model and the computational analysis, the following section provides a formal definition of prospects and reviews how CPT can be used to describe choices between prospects in terms of deviations from the principle of EV maximization.

# Prospects and the Description of Choices in CPT

Since there is an uncountable number of choices people confront in the natural world, behavioral decision research routinely abstracts from choices between particular choice alternatives, e.g., the choice between job offers, political parties to vote for, investment plans, and whatnot.
Rather, with the choice between at least two prospects, it studies a case which is counterfactual in that it omits the many particularities of each choice situation, but retains in the form of prospects the fundamental properties that almost all choice alternatives are assumed to possess.
These fundamental properties are considered the possible outcomes of a choice alternative and the probabilities with which these outcomes occur following the choice of the alternative.
Roughly speaking, then, all choice alternatives that can be fully described by their outcome-probability pairs are prospects (or: gambles).
As such, prospects can be understood as the distribution of a discrete random variable, for which the following section briefly provides an axiomatic basis. 

## Formal Definition of Prospects

Following @kolmogorovFoundationsTheoryProbability1950 and adopting the symbolic notation from @georgiiStochastikEinfuhrungWahrscheinlichkeitstheorie2015, let the tuple $(\Omega, \mathcal{F}, P)$ be a probability space.
Accordingly, $\Omega$ is a finite sample space with each element $\omega$ in $\Omega$ denoting a possible consequence of choosing the respective prospect.
$\mathcal{F}$ is the power set $\mathcal{P}(\Omega)$ with each element $A$ in $\mathcal{F}$ being a subset of $\Omega$, also denoted a random event.
$P$ is the probability measure
$$
P: \mathcal{F} \to [0,1]
\tag{1}
$$
assigning the elements of $\mathcal{F}$ a probability $0 < p(A) \leq 1$ with $P(\Omega) = 1$.
Now, the random variable is a function
$$
\begin{aligned}
  X : (\Omega, \mathcal{F}) &\to (\Omega', \mathcal{F'})  \\
  \omega &\mapsto x
  \; ,
\end{aligned}
\tag{2}
$$
where $(\Omega', \mathcal{F'})$ is a measurable space with each element $x \in \Omega'$ denoting a possible outcome of a prospect and each element $A' \in \mathcal{F'}$ being a subset of $\Omega'$.
An outcome's probability $p_X(x)$ is then provided by the pushforward measure
$$
P' : \mathcal{F'} \to [0,1]
\tag{3}
$$
with 
$$
P'(A' \in \mathcal{F'}) := P(\{\omega: X(\omega) = x \in A'\})
\; .
\tag{4}
$$
Note that the measure in (3) and (4), respectively, is a random variable's distribution and therefore a prospect. 
Accordingly, given a choice between $1, ..., k$ prospects, the inputs from the environment or memory should provide information about the set
$$
\{P'^1, ... , P'^k\}
\; ,
\tag{5}
$$
where each element $P'$ can itself be considered a set of outcome-probability pairs $\{(x_i, p_X(x_i)\}$.

## CPT and the Concept of Probability Weighting

The choices people actually make between prospects are often described in terms of deviations from the principle of EV maximization, according to which the prospect with the largest EV
$$
EV = \sum_i^n x_i \times p_X(x_i)
\tag{6}
$$ 
should be chosen (EV maximization).
To *describe* how people's actual choices deviate from this principle [see @erevAnomaliesForecastsDescriptive2017, for a recent replication of classical demonstrations],^[<!--Open footnote-->@tverskyAdvancesProspectTheory1992 addressed the *fourfold pattern of risk attitudes*---i.e., choices between prospects indicate risk aversion for gains of high probability and losses of low probability, and risk seeking for gains of low probability and losses of high probability---and related effects, e.g., the *certainty effect* and the *reflection effect*, which were already addressed by CPT's predecessor, prospect theory [@kahnemanProspectTheoryAnalysis1979]. Together, these empirical choice patterns indicate a violation of the principle of EV and expected utility (EU) maximization [see @bernoulliExpositionNewTheory1954\/1738].<!--Close footnote-->]
CPT and similar rank-dependent models [see @stottCumulativeProspectTheory2006, for an overview of models] fit choice data by maximizing^[<!--Open footnote-->Note that CPT may be amended by a stochastic choice rule to account for the probabilistic nature of choices [@rieskampProbabilisticNaturePreferential2008]. Stochastic choice rules [@stottCumulativeProspectTheory2006, for an overview] make the choice of a prospect more probable the better its valuation relative to those of other prospects.<!--Close footnote-->]
the value
$$
V = \sum_i^n v(x_i) \times \pi_i
\; ,
\tag{7}
$$ 
with objective outcomes $x_i$ being transformed by a value function $v$, and *cumulative decision weights* $\pi_i$ being determined by the difference between transformed cumulative probabilities of the distribution $P'$.
More specifically, following @tverskyAdvancesProspectTheory1992, the objective outcomes are transformed by a value function
$$
\begin{aligned}
  v : \Omega' &\to \mathbb{R} \\
  x &\mapsto 
  \begin{cases}
     x_i^\alpha &\forall x_i \geq 0, \\
     -\lambda |x_i|^\alpha &\text{else}
     \; ,
  \end{cases}
\end{aligned}
\tag{8}
$$
with $\alpha \in [0,1]$ determining the degree of the function's concavity (convexity) over the positive (negative) outcome interval, and $\lambda > 1$ increasing the function's slope over the negative outcome interval only.
Each subjective value $v(x)$ is then multiplied (or: weighted) with a cumulative decision weight that takes the form
$$
\pi_i =
  \begin{cases}
     w^+(P(X(\omega) \geq x_i)) - w^+(P(X(\omega) > x_i)) \quad \forall x_i \geq 0, \\
     w^-(P(X(\omega) \leq x_i)) - w^-(P(X(\omega) < x_i)) \quad \text{else}
     \; , 
  \end{cases}
\tag{9}
$$
where $w$ is a monotonic increasing, nonlinear weighting function $w: [0,1] \to [0,1]$ satisfying $w^+(0) = w^-(0) = 0$ and $w^+(1) = w^-(1) = 1$.

Essentially, the proposed model is such that the cumulative decision weights derived for the outcomes may be greater or smaller than their objective probabilities, causing the transformed outcomes---i.e., the subjective values---to be over- or underweighted *in* CPT, respectively.
<!--Accordingly, one may adopt an as-if weighting terminology, stating that people choose *as if* they maximized the value in (10) and applied the weighting pattern that was estimated in CPT.-->
Now, several consequences for the weighting pattern that can be estimated in CPT follow from (9), which are reviewed next:

* From the transformation of cumulative probabilities it necessarily follows that the value of a cumulative decision weight $\pi$ depends on the rank of the outcome $x$ for which $\pi$ is determined.

* The value of a cumulative decision weight depends on the estimated shape of the graph of $w$ or the weighting function's parameters, respectively.
That is, the shape of the graph of $w$ displays over which interval on the cumulative probability scale $[0,1]$ the images of $w$ take values that are greater or smaller than the respective cumulative probabilities and how much greater or smaller these images are.
Hence, assuming a nonlinear graphical shape of $w$, the same numerical difference between objective cumulative probabilities may translate to cumulative decision weights of different value.

* For prospects containing either only positive or only negative outcomes, $\sum_i^n\pi_i = 1$ is satisfied.
Accordingly, for such prospects, the weighting of subjective values with cumulative decision weights rather than with objective probabilities may be roughly understood as a redistribution of the entire probability mass of $P(\Omega') = 1$ across outcomes [@zilkerNonlinearProbabilityWeighting2021].

\noindent
Given that the value of each cumulative decision weight depends on the two transformed cumulative probabilities in (9)---i.e., the probability of obtaining a positive (negative) outcome equal to or greater (smaller) than a respective outcome $x$, and the probability of obtaining a strictly greater (smaller) outcome---the remaining consequences for the weighting pattern are reviewed by considering the actual weighting function.
Therefore, Figure\ \@ref(fig:weighting-function) illustrates some of the possible graphical shapes of the two-parameter weighting function of @goldsteinExpressionTheoryPreference1987, which, however, is just one of several parameterizations that have been proposed [e.g., @prelecProbabilityWeightingFunction1998; @tverskyAdvancesProspectTheory1992; see @stottCumulativeProspectTheory2006, for an overview].
Now, let each $p$ on the abscissa be one of the four cumulative probabilities from (9). 
Then each graph in Figure\ \@ref(fig:weighting-function) displays the graphical shape of the weighting function
$$
\begin{aligned}
  w : [0,1] &\to [0,1] \\
  p &\mapsto \frac{\delta \times p^{\gamma}}
  {\delta \times p^{\gamma} + (1-p)^{\gamma}}
  \; ,
\end{aligned}
\tag{10}
$$
for the respective values of the parameters $\gamma \in [0,2]$ and $\delta > 0$.
Evidently, both parameters have distinct effects on the graphs' shape, with $\gamma$ affecting the curvature and $\delta$ the elevation.
As a consequence from (9) then, each combination of parameters implies a particular weighting pattern, with some of them being similar and others being rather distinct.

(ref:weighting-function) Possible Graphical Shapes of Goldstein and Einhorn's -@goldsteinExpressionTheoryPreference1987 Weighting Function

```{r weighting-function, fig.cap="(ref:weighting-function)", fig.height=7, fig.width=10}
#compute images of weighting function (wf)
wf <- tibble(p = seq(0, 1, .01)) %>%  #cumulative probabilities
  expand_grid(gamma = seq(.1, 2, .1), #gamma values
              delta = c(.1, .5, 1, 2, 5, 10)) %>% #delta values
  mutate(wp = (delta*(p^gamma))/((delta*p^gamma)+(1-p)^gamma)) #images of wf

#labeller function for facet labels with LateX math expressions 
label_delta <- function(string) {
  TeX(paste("$\\delta=$", string, sep = ""))  
}

#plot shapes of weighting function
wf %>% 
  ggplot(aes(p, wp, group = gamma)) +
  facet_wrap(~delta, labeller = as_labeller(label_delta, default = label_parsed)) + 
  scale_x_continuous(breaks = seq(0, 1, .2)) +
  scale_y_continuous(breaks = seq(0, 1, .2)) + 
  labs(x = expression(p),
       y = expression(w(p)), 
       color = expression(gamma)) + 
  theme_apa() + 
  geom_line(aes(color = gamma)) +
  scale_color_viridis(option = "viridis") + 
  geom_abline(intercept = 0, slope = 1, color = "gray", linetype = "dashed")
```

In some more detail, the gray-dashed identity lines in Figure\ \@ref(fig:weighting-function) imply a linear weighting pattern.
That is, since the images of $w$, $w(p)$, are equal to the respective cumulative probabilities $p$, all cumulative decision weights $\pi$ that are derived according to (9) would be equal to the differences between the respective objective cumulative probabilities and therefore also to the objective probabilities of the outcomes for which the weights are determined.
Such a linear weighting pattern is implied by $\gamma = 1$ and $\delta = 1$. 
Now, both deviations from $\gamma = 1$ and $\delta = 1$ produce a nonlinear graphical shape.
That is, for $\gamma > 1$, the graphs take a S-shape---running below (above) the identity line for small (large) cumulative probabilities---which is more accentuated for larger deviations from $\gamma = 1$.
Importantly, the same *small* numerical difference between two cumulative probabilities $p$ then translates to varying differences between the respective images $w(p)$ depending on the interval on the cumulative probability scale. 
Specifically, because of the S-shaped curvature, the differences between the images $w(p)$ may be smaller than those between the respective cumulative probabilities in the lower and upper part of the cumulative probability scale, but may be considerably greater in the middle part.
As a consequence, the cumulative decisions weights are smaller than the respective objective probabilities for small-probability outcomes of either a low rank or a high rank, but they are larger for small-probability outcomes of a middle rank.^[<!--Open footnote-->Note that the distinction between outcomes of either low/high rank or middle rank is unnecessary for two-outcome prospects [see @tverskyAdvancesProspectTheory1992]. However, for finite many outcomes, the distinction is critical. More so, the fact that CPT may weigh outcomes of the same small probability differently, adds another reasons why the description-experience gap should not be equated with a reversed weighting pattern in CPT [see @hertwigConstructBehaviorGap2018]. I.e., neither could the gap then be interpreted in terms of an as-if underweighting (overweighting) of rare outcomes in DfE (DfD), nor would any of the potential contributors to the gap considered so far [see, e.g., @hertwigDescriptionexperienceGapRisky2009; @wulffMetaanalyticReviewTwo2018] treat some small-probability outcomes differently than other small-probability outcomes.<!--Close footnote-->]
Note that for all $\gamma < 1$ the graph of $w$ takes an inverse S-shape and the entire weighting pattern is reversed.
Moreover, the shape of the weighting function, and thus the weighting pattern, depends strongly on $\delta$, which affects the overall elevation of the graph.
That is, for $\delta < 1$, the interval on the cumulative probability scale over which the weighting function runs below the identity line is greater than the interval over which the weighting function runs above the identity line; and this pattern reverses for $\delta > 1$.
More specifically, $\delta$ shifts across the entire cumulative probability scale the intervals within which small differences between objective cumulative probabilities translate to either smaller or greater cumulative decision weights.
In other words then, for any decrease (increase) in $\delta$, the underweighting of small probability outcomes extends to more outcomes at the lower (higher) end of the outcome range.

To summarize the preceding paragraphs, CPT is data model that can be used to describe choices between prospects by capturing systematic deviations from the principle of EV maximization in the parameter estimates of its value and weighting function.
Importantly, each estimated weighting function implies a weighting pattern.
Although the possible weighting patterns depend on the exact parameterization of the weighting function [e.g., @goldsteinExpressionTheoryPreference1987], the latter's graph usually takes a more or less accentuated (inverse) S-shape, which implies a weighting pattern where small-probability outcomes of low and high rank are underweighted (overweighted) in CPT's overall evaluation of a prospect according to (7).
Notably, for the case of choices between at most two-outcome prospects, the weighting pattern reduces to either an underweighting or overweighting of rare outcomes.
For the description of such choices, one may then adopt the as-if weighting terminology from the introduction, by stating that people decide as if they were either underweighting or overweighting rare outcomes.

To be clear, however, the as-if prefix indicates that it cannot be concluded from the estimated weighting pattern that the mind does indeed perform any of the computations associated with the weighting of subjective values in CPT; rather, the mind processes the information about the choice alternatives in a way that the resulting choices translate to the estimated CPT model.
The as-if weighting terminology is therefore reminiscent of the distinction between different levels of explanation [@marrVisionComputationalInvestigation1982].
That is, CPT may be considered a computational-level theory that specifies the computational problem the mind faces when being confronted with the choice between prospects, and how this problem may be solved by the abstract calculus reviewed above.
However, as such, CPT cannot substitute algorithmic-level theories that make claims about how exactly the mind transforms the inputs to the decision-making process into outputs which approximate the solutions of CPT's calculus [see @zilkerMeasuringModelingConstruction2020; see also @griffithsProbabilisticModelsCognition2010].
Therefore, this paper uses CPT merely as a data model and continues to inquire how on the algorithmic level the mind may integrate sampling strategies into the information-processing chain upfront choices in DfE.

# Sampling Strategies in DfE

The need to distinguish between the computational and the algorithmic level in the domain of decision making is exemplified by the fact that information about prospects may come in the form of descriptions or sampled outcomes.
Specifically, only rarely in peoples' daily life, the inputs to the decision-making process take the form of explicit descriptions of all outcome-probability pairs.
In such cases, people would make DfD, since the information from which the mind learns about the properties of prospects are complete descriptions thereof.
Yet, rather often, people make DfE, where the mind can learn about the latent properties of prospects only by experiental sampling over time [@hertwigDescriptionexperienceGapRisky2009, p. 517].
Accordingly, for the choice between $1, ..., k$ prospects, the inputs to the decision-making process are generated by a set of prospect-specific stochastic processes
$$
\{\{X^1_t\}, ... , \{X^k_t\}\}
\; ,
\tag{11}
$$
where each $\{X_{t}\}_{t \in \mathbb N}$ is a collection of independent and identically distributed (i.i.d.) random variables.^[Note that it has been emphasized that in the natural world samples are "virtually never random" [@fiedlerBewareSamplesCognitiveecological2000, p. 660]. However, in the sampling paradigm, i.i.d. random variables are commonly assumed.]
The inputs to DfE are then the realizations of the random variables in (11), which require different algorithms to be processed than do descriptions of the outcomes and their probabilities.

## Accumulating Evidence About Differences in Latent Properties

Since it is only through their fundamental properties, or combinations thereof, that any distinction between the prospects can be made, systematic choices in DfE must be based on the assessment of differences in the latent properties of prospects.
Accordingly, the mind may use the sampled outcomes to first infer the latent objective probabilities of all possible outcomes, and then proceed by selecting a property, or combinations thereof, to assess differences among the prospects and make a respective choice.
However, the mind may also approximate such a process by using an algorithm that omits explicit inferences and directly accumulates evidence about the differences in latent properties from the sampled outcomes.
To this end, sequential sampling models of decision making assume that the outcomes of prospects are sequentially sampled and integrated into dynamic decision variables and that a choice is made once a decision variable exceeds a threshold in favor for one of the prospects.
This class of process models thereby provides an algorithmic link between the actual sampling process and the choices in DfE and has shown to explain some of the robust deviations from the EV or EU principle, respectively [@busemeyerDecisionFieldTheory1993; @bhatiaSequentialSamplingParadoxes2014], including the underweighting of rare outcomes in the sampling paradigm [@markantModelingChoiceSearch2015].
Moreover, these models implement speed-accuracy tradeoffs, with the amount of evidence that must be sampled in favor for a prospect increasing with thresholds, leading to more (less) accurate but slower (faster) choices for high (low) thresholds.  

Yet, there are distinct latent properties that can be used to assess differences between prospects.
Respectively, this paper presents a computational model which implements the summary and the round-wise decision strategy---for which either the prospect is chosen that produced the greater mean across all sampled outcomes or the prospect that won more comparisons between single sampled outcomes, respectively [see @hillsInformationSearchDecisions2010]---into a common evidence accumulation model.
That is, the model accumulates the evidence obtained from a sequence of comparisons between sample means, with the size of the samples underlying each mean comparison decreasing with increasing switching probabilities.
Accordingly, whereas the decision variables of the model capture how much (more) comparisons between sample means a prospect has won, the evidence accumulation process more closely resembles the round-wise strategy for high switching probabilities and small sample sizes, and the summary strategy for low switching probabilities and large sample sizes.
Now, since for a large overall number of sampled outcomes the summary strategy is more indicative for differences in the EV and the round-wise strategy is more indicative for differences in the likelihood to produce a higher outcome than the respective other prospect, the proposed model shows how the mind may rely on different sampling strategies to change for which differences in the latent properties of prospects evidence is accumulated.

## Model 

We consider the sampling paradigm and a choice between two prospects denoting the distributions $P'^X$ and $P'^Y$ of the discrete random variables $X$ and $Y$, respectively.
The model assumes that the information-processing chain starts at random with a stochastic process on one of the two random variables.
Specifically, it is assumed that an agent starts with equal probability to sample from one of the prospects, say $P'^X$, and generates a sequence of random variables $X_1, X_2,...$, where the mean over the sequence is computed or updated with each new outcome that is sampled.
The respective stochastic process $\{S^X_n\}_{n \in \mathbb{N}}$ is defined by
$$
S^X_n = \frac{1}{n} \sum_{t = 1}^n X_t, \quad n \in \mathbb{N}
\; ,
\tag{12}
$$
and terminates as soon as an outcome is sampled from the other prospect.
The switching probability $\psi \in (0,1]$, which is adopted prior the start of the information-processing chain and is fixed throughout, controls the probability with which outcomes from different prospects are sampled in direct succession.
In other words, the switching probability is the probability with which the stochastic process on the current random variable stops and a new stochastic process on the other random variable starts with the subsequently sampled outcome.
To implement an evidence accumulation process, the model assumes that each time after a new stochastic process was started and terminated on both random variables, their respective values---i.e., the means over the sequences---are compared, with the prospect underlying the stochastic process with the greater final value receiving a round win.
Accordingly, for the prospect $P'^X$, this stage is modeled as the mapping
$$
\begin{aligned}
  Z^X : \mathbb{R} &\to \mathbb{N} \\
  S^X_n - S^Y_n &\mapsto 
  \begin{cases}
     1 &\text{if} \quad  S^X_n > S^Y_n, \\
     0 &\text{else}
     \; ,
  \end{cases}
\end{aligned}
\tag{13}
$$
where $Z^X$ is a random variable that takes only the two values $0$ and $1$, denoting a lost or won mean comparison, respectively. 
The model assumes that a round consisting of a pair of stochastic processes---i.e., one for each prospect---and the subsequent comparison of their final values is repeated until the sum of round wins for one of the two prospects reaches a threshold $\theta \in \mathbb{N}$.
Similar to the switching probability, it is assumed that the threshold is adopted prior the start of the information-processing chain and is fixed throughout.
For $P'^X$, then, we obtain a sequence of independent comparisons $Z^X_1, Z^X_2, ...$ and model the respective evidence accumulation process as a random walk $\{D^X_m\}_{m \in \mathbb{N}}$ defined by 
$$
D^X_m = \sum_{i = 1}^m Z^X_i, \quad m \in \mathbb{N}
\;,
\tag{14}
$$
resembling a dynamic decision variable.
Note that since (13) describes a Bernoulli trial, the random walk in (14) is based on a Bernoulli process.

Finally, note that the described model assumes two random walks---i.e., one for each prospect---approaching the same threshold, which is the absolute number of mean comparisons a prospect must win in order to be chosen.
However, one may also define a single random walk, resembling a common dynamic decision variable, which approaches a positive threshold $\theta^+$ and a negative threshold $\theta^-$.
Therefore, (13) may be adapted such that $Z$ takes the value $-1$ instead of $0$, such that the threshold determines how many more mean comparisons a prospect must win in order to be chosen.

### Model Predictions

Below, the key predictions concerning the effects of the model parameters---i.e., the switching probability $\psi$ and the threshold $\theta$---on the length of the information-processing chain and the final choice are outlined.
In addition, previous research has shown that differences as to how information about the properties of prospects is processed can translate to characteristic shapes of CPT's value and weighting function [see @pachurHowTwainCan2017; @zilkerNonlinearProbabilityWeighting2021].
Respectively, this section also contains predictions about the potential relations between $\psi$ and $\theta$ and the CPT parameters.

#### Length of Information-Processing Chain

The model predicts the overall number of sampled outcomes to increase with thresholds.
Specifically, the required number of independent mean comparisons increases with thresholds.
Then, since each comparison is based on its own pair of stochastic processes, an increase in thresholds translates to a larger number of required stochastic processes and, ceteris paribus, therefore also to a larger overall number of sampled outcomes.
In turn, the model predicts the overall number of sampled outcomes to decrease with increasing switching probabilities.
Specifically, each stochastic process is predicted to terminate faster, the higher the probability that the subsequent outcome is sampled from the other prospect---i.e., the switching probability.
Accordingly, an increase in the switching probability leads to shorter stochastic processes which translates, ceteris paribus, to a smaller overall number of sampled outcomes.

#### Final Choice

First note that the threshold and switching probability can be used to alter the degree to which the evidence accumulation process resembles the summary or the round-wise strategy.
That is, for low thresholds and switching probabilities, only a few mean comparisons based on pairs of long stochastic processes---i.e., large samples---are carried out, thereby resembling the summary strategy.
Accordingly, for such a parameter combination, the model tends to choose the prospect that possesses the larger EV.
In turn, for high thresholds and switching probabilities, many mean comparisons based on pairs of short stochastic process---i.e., small samples---are carried out, thereby resembling the round-wise strategy.
Accordingly, for such a parameter combination, the model tends to choose the prospect that is more likely to return a higher outcome.
Yet, if the prospect that is more likely to return a higher outcome does not also possess the higher EV, the parameter combination resembling the round-wise strategy is predicted to cause a deviation form the principle of EV maximization.

Now, more specifically, because the switching probability is predicted to alter the length of the stochastic processes underlying each mean comparison, the switching probability should affect whether a low-probability outcome contributes to the majority of the required mean comparisons about as much as would be warranted based on its objective probabilities, or whether it contributes rather less.
Specifically, for short stochastic processes, the binomial distribution associated with small-probability outcomes is positive skewed, therefore causing the relative frequencies with which these outcomes occur over the course of such a process to be smaller rather than larger than their latent objective probabilities.
Respectively, if the small-probability outcome is smaller (larger) than the expected value, high switching probabilities---and therefore pairs of short stochastic processes---cause the mean to be an inflated (deflated) estimate of the expected value.
Thus, high switching probabilities are predicted to cause an as-if underweighting of rare outcomes that can translate to choices that deviate from the principle of EV maximization.
In turn, low switching probabilities---and therefore pairs of long stochastic processes---cause the mean to closely correspond to the expected value, leading to choices that tend to conform the principle of EV maximization.

Because each comparison‚Äôs contribution to the accumulated evidence is weighted equally, the predictions made for a given switching probability should become more robust as the thresholds and therefore the number of independent comparisons increases.
The thresholds therefore serve to implement 


#### Cumulative Prospect Theory

```{r eval=FALSE}
predictions <- tibble("Simulation Pararameter" = c("s", "$\\theta$"),
                      "$\\gamma$" = c(0,0),
                      "$\\delta$" = c(0,0),
                      "$\\alpha$" = c(0,0))
apa_table(predictions, caption = "Model Predictions", escape = FALSE)
```

# Simulation Study

## Method

### Choice Problems

The test set of $60$ choice problems, each consisting of a safe prospect and a two-outcome prospect, was obtained by stratified sampling from an initial set of $10,000$ choice problems.
The stratification was used to assure that some of the choice problems contain a small (high) outcome of a probability smaller than $.20$.
The exact procedure was as follows:
For each of the $10.000$ problems in the initial set, three outcomes were randomly drawn from a uniform distribution over the interval $[0, 20]$, with the smallest and highest of these three outcomes being assigned to the risky prospect in order to omit the case of dominant prospects.
The probability for the smaller outcome of the risky prospect, $p_{small}$, was drawn from a uniform distribution over the interval $[.01, .99]$, with the probability of the higher outcome being set to $p_{high} = 1-p_{small}$.
To obtain the $60$ choice problems of the test set, the initial set was divided into three subsets, where each subset contained either all problems with $p_{high} \in (0,.2)$ or $p_{high} \in [.2,.8]$ or $p_{high} \in (.8,1)$.
From each of these subset, then, $20$ choice problems were randomly sampled.

### Data Generation

The evidence accumulation model presented in this paper was implemented as a computational model to simulate the sampling process and the final choice for each of the $60$ choice problems in the test set.
All of the following parameter values were combined with each other:

* **Switching probability**: 
The switching probability $s$, defined as the probability of sampling outcomes from different prospects in direct succession, was varied in the interval $[.1, 1]$ in increments of $.1$.

* **Decision variable**: The decision variable of a prospect either counts the absolute number of won comparisons or how much more comparisons were won relative to the other prospect.

* **Threshold**: 
The value of the threshold $\theta$, determining how much (more) comparisons a prospect must win, was varied in the interval $[1,5]$ in increments of 1. 

\noindent
In sum, 100 (parameter combinations) x 60 (choices) x 100 (agents) = 600,000 sampling processes and choices were simulated. 

### Modeling

## Results

```{r data, include=FALSE}
#load choice data
cols_choices <- list(.default = col_double(),
                     boundary = col_factor(),
                     gamble = col_factor(),
                     rare = col_factor(),
                     agent = col_factor(),
                     choice = col_factor())
choices <- read_csv("data/choices/choices.csv", col_types = cols_choices)

#load CPT estimates
cols_cpt <- list(.default = col_double(),
                 boundary = col_factor(),
                 a = col_factor(),
                 parameter = col_factor())
cpt <- read_csv("data/estimates/estimates_cpt.csv", col_types = cols_cpt)
```

<!-- Plots choice rates -->

(ref:choice-rates) Effect of Switching Probability on Rates of False Risky and False Safe Choices for 2-Outcome Prospects with High Rank Outcomes of Different Probability

```{r choice-rates, fig.cap="(ref:choice-rates)", fig.height=7, fig.width=10}
#prepare data

##determine normative choice according to latent and sampled EV
fr_rates <- choices %>%
  mutate(norm = case_when(ev_ratio > 1 ~ "A", ev_ratio < 1 ~ "B"), #normative choice latent EV
         ev_ratio_exp = round(a_ev_exp/b_ev_exp, 2),
         norm_exp = case_when(ev_ratio_exp > 1 ~ "A", ev_ratio_exp < 1 ~ "B")) #normative choice sampled EV

##compute false response rates  

###latent EV
fr_rates_l <- fr_rates %>%
  filter(!is.na(norm)) %>% #exclude trials with equal EV
  group_by(s, boundary, a, rare, norm, choice) %>% #separate by model parameter and type of rare event
  summarise(n = n()) %>%
  mutate(rate = round(n/sum(n), 2), #compute response rates
         type = case_when(norm == "A" & choice == "B" ~ "Safe", #determine false response type
                          norm == "B" & choice == "A" ~ "Risky")) %>%
  ungroup() %>%
  filter(!is.na(type)) %>% #remove correct responses
  select(-c(norm, choice, n)) %>%
  mutate(norm_ev = "Latent") #assign facet label

###sampled EV
fr_rates_s <- fr_rates %>%
  filter(!is.na(norm_exp)) %>%
  group_by(s, boundary, a, rare, norm_exp, choice) %>%
  summarise(n = n()) %>%
  mutate(rate = round(n/sum(n), 2),
         type = case_when(norm_exp == "A" & choice == "B" ~ "Safe",
                          norm_exp == "B" & choice == "A" ~ "Risky")) %>%
  ungroup() %>%
  filter(!is.na(type)) %>%
  select(-c(norm_exp, choice, n)) %>%
  mutate(norm_ev = "Sample")

### merge latent and sampled data
fr_rates <- bind_rows(fr_rates_l, fr_rates_s) %>%
  mutate(rare = case_when(rare == "none" ~ "\\in \\[.2,.8\\]", #change facet labels for rare events
                          rare == "attractive" ~ "\\in (0,.2)",
                          rare == "unattractive" ~ "\\in (.8,1)"))

# labeller functions
label_theta <- function(string) {
  TeX(paste("$\\theta=$", string, sep = "")) #threshold parameter theta
}

label_rare <- function(string) {
  TeX(paste("$\\p_{High}$", string, sep = "")) #type of rare event
}

#plot
fr_rates %>%
  ggplot(aes(s, rate)) +
  facet_grid(rare~a, labeller = labeller(rare = as_labeller(label_rare, default = label_parsed), 
                                         a = as_labeller(label_theta, default = label_parsed))) +
  labs(x = "Switching Probability",
       y = "False Response Rate",
       color = "Normative\nExpected\nValue",
       shape = "False\nResponse\nType") +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, .5)) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, .5)) +
  geom_point(aes(color = norm_ev, shape = type)) +
  scale_shape_manual(values = c(4, 16)) +
  scale_color_viridis_d(option = "plasma", end = .7) +
  theme_apa()
```

<!-- Plots CPT -->

(ref:weighting-parameters) Parameter Estimates of the Weighting Function of @goldsteinExpressionTheoryPreference1987 for Different Simulation Parameters

```{r weighting-parameters, fig.cap="(ref:weighting-parameters)", fig.height=7, fig.width=10}
#prepare data
cpt <- cpt %>% mutate(boundary = if_else(boundary == "absolute", "Absolute", "Relative"))

#plot estimates

##gamma
gamma <- cpt %>%
  filter(parameter == "gamma") %>%
  ggplot(aes(s, mean, color = s, shape = boundary)) +
  facet_wrap(~a, nrow = 1, labeller = labeller(a = as_labeller(label_theta, default = label_parsed))) +
  labs(x = element_blank(), #omit axis title
       y = expression(gamma),
       color = "Switching\nProbability",
       shape = "Boundary") +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0,1,.5)) +
  geom_point() +
  geom_pointrange(aes(ymin=`2.5%`, ymax=`97.5%`)) + 
  scale_shape_manual(values = c(4, 16)) +
  scale_color_viridis(option = "plasma") +
  theme_apa() 

##delta
delta <- cpt %>%
  filter(parameter == "delta") %>% 
  ggplot(aes(s, mean, color = s, shape = boundary)) +
  facet_wrap(~a, nrow = 1) + 
  labs(x = "Switching Probability",
       y = expression(delta),
       color = "Switching\nProbability",
       shape = "Boundary") +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0,1,.5)) +
  geom_point() +
  geom_pointrange(aes(ymin=`2.5%`, ymax=`97.5%`)) + 
  scale_color_viridis(option = "plasma") +
  scale_shape_manual(values = c(4, 16)) +
  theme_apa() + 
  theme(strip.text.x = element_blank()) #omit facet labels

##merge figures
ggarrange(gamma, delta, ncol = 1, nrow = 2, common.legend = TRUE, legend = "right", labels = "AUTO") 
```

(ref:value-parameters) Parameter Estimates of the Value Function of @tverskyAdvancesProspectTheory1992 and of the Logit Choice Rule for Different Simulation Parameters

```{r value-parameters, fig.cap="(ref:value-parameters)", fig.height=7, fig.width=10}
#plot estimates

##alpha
alpha <- cpt %>%
  filter(parameter == "alpha") %>% 
  ggplot(aes(s, mean, color = s, shape = boundary)) +
  facet_wrap(~a, nrow = 1, labeller = labeller(a = as_labeller(label_theta, default = label_parsed))) +
  labs(x = element_blank(),
       y = expression(alpha),
       color = "Switching\nProbability",
       shape = "Boundary") +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0,1,.5)) +
  geom_point() +
  geom_pointrange(aes(ymin=`2.5%`, ymax=`97.5%`)) + 
  scale_color_viridis(option = "plasma") +
  scale_shape_manual(values = c(4, 16)) +
  theme_apa()

##rho
rho <- cpt %>%
  filter(parameter == "rho") %>% 
  ggplot(aes(s, mean, color = s, shape = boundary)) +
  facet_wrap(~a, nrow = 1) + 
  labs(x = "Switching Probability",
       y = expression(rho),
       color = "Switching\nProbability",
       shape = "Boundary") +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1,.5)) +
  geom_point() +
  geom_pointrange(aes(ymin=`2.5%`, ymax=`97.5%`)) + 
  scale_color_viridis(option = "plasma") +
  scale_shape_manual(values = c(4, 16)) +
  theme_apa() + 
  theme(strip.text.x = element_blank())

##merge figure
ggarrange(alpha, rho, ncol = 1, nrow = 2, common.legend = TRUE, legend = "right", labels = "AUTO") 
```

(ref:cpt-graphs) Estimated Graphical Shapes of the Weighting and Value Function for Different Simulation Parameters

```{r cpt-graphs, fig.cap="(ref:cpt-graphs)", fig.height=7, fig.width=10}
#prepare data

##compute images of weighting and value function

###weighting function
wf <- cpt %>%
  select(s, boundary, a, parameter, mean) %>%
  pivot_wider(names_from = parameter, values_from = mean) %>%
  select(-c(alpha, rho)) %>%
  expand_grid(p = seq(0, 1, .05)) %>%
  mutate(w = round(  (delta * p^gamma)/ ((delta * p^gamma)+(1-p)^gamma), 2))

###value function
vf <- cpt %>%
  select(s, boundary, a, parameter, mean) %>%
  pivot_wider(names_from = parameter, values_from = mean) %>%
  select(-c(gamma, delta, rho)) %>%
  expand_grid(x = seq(0, 20, .5)) %>%  
  mutate(v = round(x^alpha, 2)) 

##split data to avoid passing two y-values to the same x-value in geom_line()
wf_a <- wf %>% filter(boundary == "Absolute")
wf_r <- wf %>% filter(boundary == "Relative")
vf_a <- vf %>% filter(boundary == "Absolute")
vf_r <- vf %>% filter(boundary == "Relative")

#plots 

##weighting function
weight <- wf_a %>% #graphs for absolute boundary
  ggplot(aes(p, w, group = s, color = s)) +
  facet_wrap(~a, nrow = 1, labeller = labeller(a = as_labeller(label_theta, default = label_parsed))) + 
  labs(x = "Experienced Probability",
       y = "Decision Weight",
       color = "Switching\nProbability") +
  scale_x_continuous(breaks = seq(0, 1, .5)) +
  scale_y_continuous(breaks = seq(0, 1, .5)) +
  geom_line(size = .5) +
  geom_line(data = wf_r, size = .5) +  #add curves for relative boundary
  scale_color_viridis(option = "plasma") +
  theme_apa()

##value function
value <- vf_a %>% 
  ggplot(aes(x, v, group = s, color = s)) +
  facet_wrap(~a, nrow = 1)+ 
  labs(x = "Objective Outcome",
       y = "Subjective Value",
       color="Switching\nProbability") +
  scale_x_continuous(breaks = seq(0, 20, 10)) +
  scale_y_continuous(breaks = seq(0, 20, 10)) +
  geom_line(size = .5) +
  geom_line(data = vf_r, size = .5) + 
  scale_color_viridis(option = "plasma") +
  theme_apa() + 
  theme(strip.text.x = element_blank())

##merge figures
ggarrange(weight, value, ncol = 1, nrow = 2, common.legend = TRUE, legend = "right", labels = "AUTO") 
```

For each distinct parameter combination, we ran 20 chains of 40,000 iterations each, after a warm-up period of 1000 samples.
To reduce potential autocorrelation during the sampling process, we only kept every 20th sample (thinning).
The potential scale reduction factor $\hat{R}$ [@gelmanInferenceIterativeSimulation1992] was $\leq$ `r round(max(cpt$Rhat), 3)` for all parameters, indicating good convergence.
The minimum effective sample size was `r min(cpt$n.eff)`.

As expected, the scatter plot below shows an inverse relationship between switching probability and trial length.
I.e., the lower the switching probability, the larger become the sample sequences on which each comparison between prospects is based, which in turn leads to longer trials.  
This effect is particularly pronounced for low probabilities such that the increase in trial length accelerates as switching probability decreases.


The figures below display the estimates of the $\gamma$ and $\delta$ parameter of the probability weighting function [@prelecProbabilityWeightingFunction1998] fitted to DfE simulated for different parameter values of the generating model.
The estimates are plotted against the switching probability, where each panel represents a distinct boundary type and level, i.e., number of comparisons. 
Grey dots represent agent level estimates, colored dots represent the mean across all agent level estimates. 

Most significantly, there is a strong relationship between the switching probability in the generating model and the $\gamma$ parameter.
I.e., large switching probabilities, which are indicative for small sample sequences, lead to larger estimates for $\gamma$. 
The resulting strong curvature leads to a compression of probabilities in the lower and upper range, reflecting underweighting of small probabilities and overweighting of large probabilities. 
This pattern is robust for varying degrees of the boundary level, except for $a = 1$. 
The latter deviation may be explained by the potential scale reduction factors for the respective parameter estimates, which indicate that the MCMC chains did not converge (see [Appendix 3][Appendix 3: Convergence Diagnostics for CPT Parameters]).   

<!-- Plots Appendix -->

```{r A-data, include=FALSE}
#load choice data
choices_c <- read_csv("data/choices/choices_comprehensive.csv", col_types = cols_choices)
cpt_c <- read_csv("data/estimates/estimates_cpt_comprehensive.csv", col_types = cols_cpt)
```

```{r A-choice-rates, fig.height=7, fig.width=10, include=FALSE}
#prepare data
fr_rates_c <- choices_c %>%
  mutate(norm = case_when(ev_ratio > 1 ~ "A", ev_ratio < 1 ~ "B"),
         ev_ratio_exp = round(a_ev_exp/b_ev_exp, 2),
         norm_exp = case_when(ev_ratio_exp > 1 ~ "A", ev_ratio_exp < 1 ~ "B"))

fr_rates_c_l <- fr_rates_c %>%
  filter(!is.na(norm)) %>%
  group_by(s, boundary, a, rare, norm, choice) %>% 
  summarise(n = n()) %>%
  mutate(rate = round(n/sum(n), 2),
         type = case_when(norm == "A" & choice == "B" ~ "Safe",
                          norm == "B" & choice == "A" ~ "Risky")) %>%
  ungroup() %>%
  filter(!is.na(type)) %>%
  select(-c(norm, choice, n)) %>%
  mutate(norm_ev = "Latent")

fr_rates_c_s <- fr_rates_c %>%
  filter(!is.na(norm_exp)) %>%
  group_by(s, boundary, a, rare, norm_exp, choice) %>%
  summarise(n = n()) %>%
  mutate(rate = round(n/sum(n), 2),
         type = case_when(norm_exp == "A" & choice == "B" ~ "Safe",
                          norm_exp == "B" & choice == "A" ~ "Risky")) %>%
  ungroup() %>%
  filter(!is.na(type)) %>%
  select(-c(norm_exp, choice, n)) %>%
  mutate(norm_ev = "Sample")

fr_rates_c <- bind_rows(fr_rates_c_l, fr_rates_c_s) %>%
  mutate(rare = case_when(rare == "none" ~ "\\in \\[.2,.8\\]",
                          rare == "attractive" ~ "\\in (0,.2)",
                          rare == "unattractive" ~ "\\in (.8,1)"))

fr_rates_c %>%
  ggplot(aes(s, rate)) +
  facet_grid(rare~a, labeller = labeller(rare = as_labeller(label_rare, default = label_parsed), 
                                         a = as_labeller(label_theta, default = label_parsed))) +
  labs(x = "Switching Probability",
       y = "False Response Rate",
       color = "Normative\nExpected\nValue",
       shape = "False\nResponse\nType") +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, .5)) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, .5)) +
  geom_point(aes(color = norm_ev, shape = type)) +
  scale_shape_manual(values = c(4, 16)) +
  scale_color_viridis_d(option = "plasma", end = .7) +
  theme_apa()
```

```{r A-weighting-parameters, fig.height=7, fig.width=10, include=FALSE}
#prepare data
cpt_c <- cpt_c %>% mutate(boundary = if_else(boundary == "absolute", "Absolute", "Relative"))

#plot estimates

##gamma
gamma_c <- cpt_c %>%
  filter(parameter == "gamma") %>%
  ggplot(aes(s, mean, color = s, shape = boundary)) +
  facet_wrap(~a, nrow = 1, labeller = labeller(a = as_labeller(label_theta, default = label_parsed))) +
  labs(x = element_blank(),
       y = expression(gamma),
       color = "Switching\nProbability",
       shape = "Boundary") +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0,1,.5)) +
  geom_point() +
  geom_pointrange(aes(ymin=`2.5%`, ymax=`97.5%`)) + 
  scale_shape_manual(values = c(4, 16)) +
  scale_color_viridis(option = "plasma") +
  theme_apa() 

##delta
delta_c <- cpt_c %>%
  filter(parameter == "delta") %>% 
  ggplot(aes(s, mean, color = s, shape = boundary)) +
  facet_wrap(~a, nrow = 1) + 
  labs(x = "Switching Probability",
       y = expression(delta),
       color = "Switching\nProbability",
       shape = "Boundary") +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0,1,.5)) +
  geom_point() +
  geom_pointrange(aes(ymin=`2.5%`, ymax=`97.5%`)) + 
  scale_color_viridis(option = "plasma") +
  scale_shape_manual(values = c(4, 16)) +
  theme_apa() + 
  theme(strip.text.x = element_blank()) #omit facet labels

##merge figures
ggarrange(gamma_c, delta_c, ncol = 1, nrow = 2, common.legend = TRUE, legend = "right", labels = "AUTO") 
```

```{r A-value-parameters, fig.height=7, fig.width=10, include=FALSE}
#plot estimates

##alpha
alpha_c <- cpt_c %>%
  filter(parameter == "alpha") %>% 
  ggplot(aes(s, mean, color = s, shape = boundary)) +
  facet_wrap(~a, nrow = 1, labeller = labeller(a = as_labeller(label_theta, default = label_parsed))) +
  labs(x = element_blank(),
       y = expression(alpha),
       color = "Switching\nProbability",
       shape = "Boundary") +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0,1,.5)) +
  geom_point() +
  geom_pointrange(aes(ymin=`2.5%`, ymax=`97.5%`)) + 
  scale_color_viridis(option = "plasma") +
  scale_shape_manual(values = c(4, 16)) +
  theme_apa()

##rho
rho_c <- cpt_c %>%
  filter(parameter == "rho") %>% 
  ggplot(aes(s, mean, color = s, shape = boundary)) +
  facet_wrap(~a, nrow = 1) + 
  labs(x = "Switching Probability",
       y = expression(rho),
       color = "Switching\nProbability",
       shape = "Boundary") +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1,.5)) +
  geom_point() +
  geom_pointrange(aes(ymin=`2.5%`, ymax=`97.5%`)) + 
  scale_color_viridis(option = "plasma") +
  scale_shape_manual(values = c(4, 16)) +
  theme_apa() + 
  theme(strip.text.x = element_blank())

##merge figure
ggarrange(alpha_c, rho_c, ncol = 1, nrow = 2, common.legend = TRUE, legend = "right", labels = "AUTO") 
```

```{r A-cpt-graphs, fig.height=7, fig.width=10, include=FALSE}
#prepare data

##compute images of weighting and value function

###weighting function
wf_c <- cpt_c %>%
  select(s, boundary, a, parameter, mean) %>%
  pivot_wider(names_from = parameter, values_from = mean) %>%
  select(-c(alpha, rho)) %>%
  expand_grid(p = seq(0, 1, .05)) %>%
  mutate(w = round(  (delta * p^gamma)/ ((delta * p^gamma)+(1-p)^gamma), 2))

###value function
vf_c <- cpt_c %>%
  select(s, boundary, a, parameter, mean) %>%
  pivot_wider(names_from = parameter, values_from = mean) %>%
  select(-c(gamma, delta, rho)) %>%
  expand_grid(x = seq(0, 20, .5)) %>%  
  mutate(v = round(x^alpha, 2)) 

##split data to avoid passing two y-values to the same x-value in geom_line()
wf_c_a <- wf_c %>% filter(boundary == "Absolute")
wf_c_r <- wf_c %>% filter(boundary == "Relative")
vf_c_a <- vf_c %>% filter(boundary == "Absolute")
vf_c_r <- vf_c %>% filter(boundary == "Relative")

#plots 

##weighting function
weight_c <- wf_c_a %>% 
  ggplot(aes(p, w, group = s, color = s)) +
  facet_wrap(~a, nrow = 1, labeller = labeller(a = as_labeller(label_theta, default = label_parsed))) + 
  labs(x = "Experienced Probability",
       y = "Decision Weight",
       color = "Switching\nProbability") +
  scale_x_continuous(breaks = seq(0, 1, .5)) +
  scale_y_continuous(breaks = seq(0, 1, .5)) +
  geom_line(size = .5) +
  geom_line(data = wf_c_r, size = .5) + 
  scale_color_viridis(option = "plasma") +
  theme_apa()

##value function
value_c <- vf_c_a %>% 
  ggplot(aes(x, v, group = s, color = s)) +
  facet_wrap(~a, nrow = 1)+ 
  labs(x = "Objective Outcome",
       y = "Subjective Value",
       color="Switching\nProbability") +
  scale_x_continuous(breaks = seq(0, 20, 10)) +
  scale_y_continuous(breaks = seq(0, 20, 10)) +
  geom_line(size = .5) +
  geom_line(data = vf_c_r, size = .5) + 
  scale_color_viridis(option = "plasma") +
  theme_apa() + 
  theme(strip.text.x = element_blank())

##merge figures
ggarrange(weight_c, value_c, ncol = 1, nrow = 2, common.legend = TRUE, legend = "right", labels = "AUTO") 
```

# Summary and Conclusion

\newpage

# References
