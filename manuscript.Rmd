---
title             : "Sampling Strategies in Decisions from Experience: Property Representation and Probability Weighting Approximation"
shorttitle        : "Sampling strategies"

author: 
  - name          : "Linus Hof"
    affiliation   : "1"
    corresponding : yes    
    address       : "Department of Psychology, Heidelberg University, Hauptstra√üe 47-51, 69117 Heidelberg, Germany"
    email         : "linushof@posteo.de"
    
affiliation:
  - id            : "1"
    institution   : "Heidelberg University"

note: | 
  **Unsubmitted draft created from the commit with the hash**: ``r repro::current_hash()``
  
authornote: |
 This is a dynamic document which can be reproduced from the accompanying GitHub repository: https://github.com/linushof/sampling-in-dfe. 
 
 The current version of this manuscript is prepared for submission as a master's thesis.
 Supervisors: Thorsten Pachur & Veronika Zilker.   
 
abstract: |
  Add short abstract.
  
keywords          : ""
wordcount         : ""

bibliography      : ["references_sampling-in-dfe.bib"]
csl               : apa.csl

figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
linkcolor         : "blue"
mask              : no
draft             : no

classoption       : "man"
documentclass     : "apa7"
output            : papaja::apa6_pdf

header-includes:
   - \usepackage[capposition=top]{floatrow}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = F, fig.pos = "t", fig.align = "l")
```

```{r packages}
# load required packages
pacman::p_load(papaja,
               readr,
               tidyr, 
               dplyr,
               ggplot2,
               ggbeeswarm,
               scico)
```

<!-- Introduction -->

The human mind is a cognitive system that operates on inputs from an environment or memory, and in the domain of decision making, such inputs provide information about the *properties* of the choice alternatives between which people can choose.
In fact, irrespective of their exact mathematical specification or computational implementation, decision models generally take as inputs information about a set of choice alternatives to return a subset thereof as outputs [@heOntologyDecisionModels2020].

Now, more formally,^[<!--Open footnote-->Paragraphs with this beginning usually formalize the ideas of the preceding paragraphs or elaborate on them by using mathematical expressions and concepts. Although I consider these paragraphs important, the major propositions of this paper should be captured despite skipping them.<!--Close footnote-->]
let us treat the choice alternatives as elements $c$ of the set $C$.
To each $c \in C$ is then assigned some information from the inputs and this information may be used by the mind to learn about the properties of the respective alternative.
Accordingly, decision making can be considered an information-processing problem which starts from the set $C$, containing all choice alternatives, and stops at a proper subset $C' \subset C$ with the inclusion map
$$
\begin{aligned}
  \iota : C' &\to C \\
  c &\mapsto c  \quad \forall c \in C'
\end{aligned}
\tag{1}
$$
[@jostMathematicalConcepts2015, p. 15], containing only the chosen alternatives.

Importantly, the information that is provided about the choice alternatives can come in many forms.
In *decisions from description* (DfD), inputs take the form of explicit and complete descriptions of all relevant properties of the choice alternatives.
Yet, in *decisions from experience* [DfE, @hertwigDecisionsExperienceEffect2004], inputs take the form of *samples*.
In this latter case, the properties of the choice alternatives are not described and therefore not known with certainty, but they must be inferred from the outcomes that occurred when the alternatives were chosen in similar past decisions.
Although these distinct input forms may in principle carry the same information about a given set of choice alternatives, behavioral decision research has so far produced a large body of papers indicating that---if a quite general perspective is taken---there are robust differences between the choices that are made in DfD and DfE.
That is, ceteris paribus, empirical choice patterns across studies indicate that in DfD people choose *as if* a rare outcome is given more weight in the evaluation of a choice alternative than would be objectively warranted by the principle of mathematical expectation (see below); in turn, in DfE, people choose *as if* the same rare outcome is given less weight than would be warranted [e.g., @barronSmallFeedbackbasedDecisions2003; @hertwigDecisionsExperienceEffect2004; @weberPredictingRiskSensitivity2004; @hauDescriptionexperienceGapRisky2008; @rakowBiasedSamplesNot2008; @ungemachAreProbabilitiesOverweighted2009; @camilleriWhenWhyRare2011; @kellenHowVariantAre2016; @erevAnomaliesForecastsDescriptive2017; @regenwetterConstructBehaviorGap2017].
Hereafter, we refer to these choice patterns as the as-if overweighting and underweighting of rare outcomes, respectively.

This paper uses the as-if underweighting of rare outcomes as a starting point for an inquiry into the sampling process upfront choices in DfE.
Essentially, it discusses and demonstrates in a simulation how different strategies of sequentially generating outcomes and integrating them into samples (hereafter: sampling strategies) can produce distinct choice patterns in DfE---among them, the as-if underweighting of rare outcomes [see @hillsInformationSearchDecisions2010, for the original formulation of this claim].
<!-- It thereby proposes that ... -->

The rest of the paper is therefore structured as follows.
In the first brief section, a formal definition of DfE is provided.
In the subsequent section, cumulative prospect theory [CPT, @tverskyAdvancesProspectTheory1992; @wakkerAxiomatizationCumulativeProspect1993] is reviewed as a data model that can be used to describe choices in DfE in terms of deviations from the principle of mathematical expectation.
<!-- Outline of the section -->
In the section that follows, sampling strategies are treated in quite some detail.
<!-- Outline of the section -->
Thereafter, a simulation is presented in which the theoretical claims that are made in the preceding section are implemented in a computational model to simulate choice data in DfE.
The predictions of the model are summarized by fitting the choice data in CPT.
The paper closes with a summary and a conclusion.
Note that this paper, including the simulations and data analyses, is reproducible---materials and instructions can be found on the GitHub repository.

# Prospects, Decisions from Description, and Decisions from Experience

Since there is an uncountable number of decisions people confront in the natural world, behavioral decision research routinely abstracts from decisions between particular choice alternatives, e.g., the choice between job offers, political parties to vote for, investment plans, and whatnot.
Rather, with the choice between at least two *prospects* (see below), it studies a case which is counterfactual in that it omits the many particularities of each decision, but retains in the form of prospects the fundamental properties that almost all choice alternatives are assumed to possess [@lopesThoughtsPsychologicalConcept1983].^[<!--Open footnote-->See @knightRiskUncertaintyProfit1921, Chapter 1, for a discussion of the reasonableness of such abstractions.<!--Close footnote-->]
These fundamental properties are considered the possible outcomes of a choice alternative (hereafter: $x_i$) and the probabilities with which these outcomes occur following the choice of the alternative (hereafter: $p_X(x_i)$).
Roughly speaking, then, all choice alternatives that can be fully described by their outcome-probability pairs $(x_i, p_X(x_i))$ are prospects (or: gambles), which are exclusively considered below. 

Now, more formally, let the axiomatic definition of a prospect be that of a discrete random variable's distribution.
Let therefore the tuple $(\Omega, \mathcal{F}, P)$ be a probability space. 
Following the @kolmogorovFoundationsTheoryProbability1950 axioms, $\Omega$ is the sample space containing a finite set of elements
$$
\omega_i \in \{\omega_1, ..., \omega_n\} = \Omega
\; .
\tag{2}
$$ 
$\mathcal{F}$ is the set of all possible subsets of $\Omega$, i.e., the power set $\mathcal{P}(\Omega)$ with each element $A \in P(\Omega) = \mathcal{F}$ being a subset of $\Omega$.^[<!--Open footnote-->Note that this paper considers only the case of finite sample spaces. The exact axiomatic definition of $\mathcal{F}$, which can be found in Kolmogorov [-@kolmogorovFoundationsTheoryProbability1950, pp. 2-3 and 14-15], also covers infinite sample spaces and is that of a $\sigma$-algebra. For the finite case, however, $\mathcal{P}(\Omega)$ can be considered as such [@georgiiStochastikEinfuhrungWahrscheinlichkeitstheorie2015, Chapter 1.1].<!--Close footnote-->]
$P$ is a probability measure
$$
P: \mathcal{F} \to [0,1]
\; ,
\tag{3}
$$
which assigns each subset $A \in \mathcal{F}$ a probability $0 < p(A) \leq 1$ with $P(\Omega) = 1$.<!--^[The probabilities $p(\omega)$ are determined by the probability mass function $p : \Omega \to [0,1]$, which is used to construct the probability measures $P(A) = \sum_{\omega \in A} p(\omega)$ for all $A \in \mathcal{F}$.]-->
The random variable $X$ is the measurable function
$$
\begin{aligned}
  X : (\Omega, \mathcal{F}) &\to (\Omega', \mathcal{F'})  \\
  \omega &\mapsto x
  \; ,
\end{aligned}
\tag{4}
$$
where $(\Omega', \mathcal{F'})$ is a measurable space with each element $x \in \Omega'$ being a possible outcome of a prospect and each element $A' \in \mathcal{F'}$ being a subset of $\Omega'$.
An outcome's probability $p_X(x)$ is then provided by the probability measure
$$
P_X : \mathcal{F'} \to [0,1]
\tag{5}
$$
with 
$$
P_X(A' \in \mathcal{F'}) := P(X^{-1} \ A') = P(\{\omega: X(\omega) = x \in A'\})
\tag{6}
$$
[see @kolmogorovFoundationsTheoryProbability1950, p. 21-22].^[<!--Open footnote-->See also @georgiiStochastikEinfuhrungWahrscheinlichkeitstheorie2015, Chapter 1, for a comprehensive yet accessible introduction to random variables.<!--Close footnote-->]
That is, the set consisting of a single outcome $x$ can be treated as a subset $\{x\} \in \mathcal{F'}$ and we therefore obtain $p_X(x) = P(X(\omega) = x)$.
Note that the probability measure in (5) and (6), respectively, is a discrete random variable's distribution and therefore a prospect.
To summarize and simplify, given a choice between $1, ..., k$ prospects, the inputs from the environment or memory should provide information about the set
$$
C = \{P_{X_1}, ... , P_{X_k}\}
\tag{7}
$$
with each element $c \in C$ being a distribution $P_X$ of a random variable defined on a different probability space.
Accordingly, each $c$ can itself be considered a set of outcome-probability pairs $(x_i, p_X(x_i)) \in \{(x_1, p_X(x_1)), ..., (x_n, p_X(x_n))\}$.

However, rarely in peoples' daily life, inputs take the form of explicit descriptions of all outcomes-probability pairs.
In such cases, people would make decisions from description (DfD), since the information from which the mind learns about the prospects are complete descriptions thereof.
Yet, rather often, people make decisions from experience (DfE), where the mind can learn about the prospects only from a record of past experiences with them.
That is, people may have experienced the possible outcomes of a prospect when they themselves or others have made the same decision repeatedly in the past, and the record of experienced outcomes for this particular prospect is a rough description of a sample.
The mind may then infer from the relative frequency with which an outcome occurred in such a sample of past decisions its probability of occurrence following a future decision.

Now, more formally, let the sample of a prospect be a set of realizations of the respective random variable $X$ according to its distribution $P_X$.^[<!--Open footnote-->Note that this paper considers the counterfactual case where samples are random, although it has been emphasized that in the natural world samples are "virtually never random" [@fiedlerBewareSamplesCognitiveecological2000, p. 660]. To account for non-random sampling, e.g., in the form of sampling biases [e.g., @fiedlerBewareSamplesCognitiveecological2000] or dynamic information structures [e.g., @cohenEffectPerceivedPatterns2021; @plonskyRelianceSmallSamples2015], the i.i.d-assumption would need to be dropped, which is out of the scope of this paper.<!--Close footnote-->]
So, for the choice between $1, ..., k$ prospects, the inputs to DfE are generated by a set $S$ of stochastic processes
$$
S = \{X_{1t}, ... , X_{kt}\}
\tag{8}
$$
with $t \in \{1, ..., N\}$ denoting a sequence of independent and identically distributed (i.i.d.) random variables and $N$ denoting the length of the respective sequence.
The sets of realizations that result from these stochastic processes may then be used to make an uncertain inference about the respective sets of outcome-probability pairs in (7).

Of course, the laws of large numbers [LLN, see, e.g., @kolmogorovFoundationsTheoryProbability1950] apply to DfE and it follows that the accuracy of inferences from a sample to a prospect depends, ceteris paribus, on the size of the samples.
Yet, when describing the choices that people actually make between prospects in DfD and in DfE, the differences outlined in the introduction are robustly found---i.e., as-if overweighting (underweighting) of rare outcomes in DfD (DfE)---and the size of samples in DfE provides just one and only a partial explanation for this so-called *description-experience gap* [@hertwigDescriptionexperienceGapRisky2009; see also @wulffMetaanalyticReviewTwo2018, for a comprehensive meta-analytic review].
More so, despite the LLN, the description-experience gap need not decrease at all, even for ever increasing sample sizes in DfE that are fully taken into account.
Below, this paper discusses---and demonstrates in a simulation---how different strategies of sequentially generating outcomes and integrating them into samples interact with the LLN to produce distinct as-if weighting patterns, including the underweighting of rare outcomes, which may stabilize as the sample size increases.
Before so doing, however, the following section reviews in general how choices between prospects are described in cumulative prospect theory (CPT), and in particular how CPT's *weighting function* captures as-if weighting patterns.

# The Description of Choices in CPT and the As-If Underweighting in DfE

The choices people actually make between prospects---be that in DfD or DfE---are often described in terms of deviations from the principle of mathematical expectation, according to which the prospect with the largest expected value (EV)
$$
EV = \sum_i^n x_i \times p_X(x_i)
\; .
\tag{9}
$$ 
should be chosen (EV maximization).
To *describe* how people's actual choices deviate from this principle [see @erevAnomaliesForecastsDescriptive2017, for a recent replication of classical demonstrations], the models of CPT [@tverskyAdvancesProspectTheory1992] and similar rank-dependent models [see @stottCumulativeProspectTheory2006, for an overview of models] fit choice data by maximizing^[<!--Open footnote-->Note that CPT may be amended by a stochastic choice rule to account for the probabilistic nature of choices [@rieskampProbabilisticNaturePreferential2008]. Most choice rules [@stottCumulativeProspectTheory2006, for an overview] are functions of the prospects' values $V$ and make a choice of a prospect with a greater $V$ only probable but not deterministic.<!--Close footnote-->]
the value
$$
V = \sum_i^n v(x_i) \times \pi_i
\; ,
\tag{10}
$$ 
with objective outcomes $x_i$ being transformed by a *value function* $v$, and *cumulative decision weights* $\pi_i$ being determined by the difference between transformed cumulative probabilities of the distribution $P_X$.
Importantly, the transformation of cumulative probabilities is carried out by a nonlinear weighting function $w$ (see (12) and (13) below).

To break down (9) and (10) and give some further background on the as-if weighting terminology in the description of choices, leaving out mathematical detail and much of the modeling jargon for now:^[<!--Open footnote-->See, @farrellComputationalModelingCognition2018, for an accessible introduction to fundamental modeling concepts.<!--Close footnote-->] 
Following the principle of mathematical expectation, one would calculate the prospects' EVs according to (9) to subsequently choose the prospect with the greatest EV (EV maximization).
However, in quite many occasions, people do not choose the prospect with the greatest EV [e.g., @erevAnomaliesForecastsDescriptive2017; @kahnemanProspectTheoryAnalysis1979].
To describe people's actual choices in a way that effectively captures the deviations from the principle of mathematical expectation,^[<!--Open footnote-->@tverskyAdvancesProspectTheory1992 addressed the *fourfold pattern of risk attitudes*---i.e., choices between prospects indicate risk aversion for gains of high probability and losses of low probability, and risk seeking for gains of low probability and losses of high probability---and related effects, e.g., the *certainty effect* and the *reflection effect*, that were already addressed by CPT's predecessor, prospect theory [@kahnemanProspectTheoryAnalysis1979]. Together, these empirical choice patterns indicate a violation of the principle of mathematical expectation and the expected utility (EU) principle [see @bernoulliExpositionNewTheory1954\/1738].<!--Close footnote-->]
CPT uses a value function $v$ and a weighting function $w$ to estimate how the objective outcomes and probabilities in (9) can be adjusted, i.e., replaced by subjective values $v(x_i)$ and cumulative decision weights $\pi_i$, respectively, so that the maximization of the value in (10) would produce model-implied choice data that is, roughly speaking, similar to the actual choice data.
Essentially, the proposed model is such that the estimated cumulative decision weights in (10) may be greater or smaller than the respective objective probabilities in (9); and they may therefore give more or less weight to the subjective values than would be warranted by the objective probability.
In other words, for a cumulative decision weight that is greater (smaller) than the objective probability it replaces, the respective outcome---which is transformed into a subjective value---is overweighted (underweighted) *in* CPT.
Accordingly, to describe an empirical choice pattern, one may adopt the as-if weighting terminology which was already used in the introduction by stating that people decide *as if* they maximized the value in (10) and applied the over- and underweighting pattern that was estimated in CPT [see @pachurWeightUncertainEvents2019].
<!--Notably, the as-if prefix indicates that it cannot be concluded from the estimated cumulative decision weights that the mind does indeed perform any of the computations associated with the weighting of subjective values in CPT; rather, it processes the inputs in a way that the outputs, i.e., the actual choice patterns, approximate the solutions from the estimated CPT model [see, e.g., @griffithsRationalUseCognitive2015].
Therefore, this paper uses CPT merely as a data model to describe choices between prospects and to demonstrate how different sampling strategies can produce weighting patterns in CPT resembling those that were estimated for empirical choice data from DfE.-->

Now, more formally, following @tverskyAdvancesProspectTheory1992, the objective outcomes are transformed by a value function
$$
\begin{aligned}
  v : \Omega' &\to \mathbb{R} \\
  x &\mapsto 
  \begin{cases}
     x_i^\alpha &\forall x_i \geq 0, \\
     -\lambda \times |x_i|^\alpha &\text{else}
     \; ,
  \end{cases}
\end{aligned}
\tag{11}
$$
with $\alpha \in [0,1]$ determining the degree of the function's concavity (convexity) over the positive (negative) outcome interval, and $\lambda > 1$ increasing the function's slope over the negative outcome interval only.
Each subjective value $v(x)$ is then multiplied (or: weighted) with a cumulative decision weight that takes the form
$$
\pi_i =
  \begin{cases}
     w^+(P(X \geq x_i)) - w^+(P(X > x_i)) \quad \forall x_i \geq 0, \\
     w^-(P(X \leq x_i)) - w^-(P(X < x_i)) \quad \text{else}
     \; , 
  \end{cases}
\tag{12}
$$
where $w$ is a monotonic increasing, nonlinear function $w: [0,1] \to [0,1]$ satisfying $w^+(0) = w^-(0) = 0$ and $w^+(1) = w^-(1) = 1$.
Several consequences for the weighting pattern that can be estimated in CPT follow from (12) and are reviewed one by one next: 

* From the transformation of cumulative probabilities it necessarily follows that the value of a cumulative decision weight $\pi$ depends on the rank of the outcome $x$ for which $\pi$ is determined.

* The value of a cumulative decision weight depends on the estimated shape of $w$.
That is, the shape of $w$ determines over which interval on the cumulative probability scale $[0,1]$ the images of $w$ take values that are greater or smaller than the respective cumulative probabilities---i.e., the preimages---and how much greater or smaller these image values are.
Hence, assuming a nonlinear shape of $w$, the same numerical difference between objective cumulative probabilities may translate to cumulative decision weights of different value.

* For prospects containing either only positive or only negative outcomes, $\sum_i^n\pi_i = 1$ is satisfied.
Accordingly, for such prospects, the replacement of objective probabilities as in (9) by cumulative decision weights as in (10) may be roughly understood as a redistribution of the entire probability mass of $P(\Omega') = 1$ across outcomes [@zilkerNonlinearProbabilityWeighting2021].

\noindent
Given that the value of each cumulative decision weight depends on the two respective cumulative probabilities in (12)---i.e., the probability of obtaining a positive (negative) outcome equal to or greater (smaller) than a respective outcome $x$, and the probability of obtaining a strictly greater (smaller) outcome---the remaining consequences for the weighting pattern in CPT may be reviewed by considering the actual shape of the weighting function.
Therefore, Figure 1 illustrates some of the possible shapes of the two-parameter weighting function of @goldsteinExpressionTheoryPreference1987, which, however, is just one of several parameterizations that have been proposed [e.g., @prelecProbabilityWeightingFunction1998; @tverskyAdvancesProspectTheory1992; see @stottCumulativeProspectTheory2006, for an overview].
Now, let each $p$ on the abscissa be one of the four cumulative probabilities from (12). 
Then each graph in Figure 1 illustrates the shape of the weighting function
$$
\begin{aligned}
  w : [0,1] &\to [0,1] \\
  p &\mapsto \frac{\delta \times p^{\gamma}}
  {\delta \times p^{\gamma} + (1-p)^{\gamma}}
  \; ,
\end{aligned}
\tag{13}
$$
for the respective values of $\gamma \in [0,2]$ and $\delta > 0$.


In essence, if the estimate of a cumulative decision weight is larger (smaller) than the objective probability from which it is derived, the weighting function runs above (below) the dashed identity line.
The choice data are then described *as if* people maximized the value in (8) and applied the over- and underweighting pattern indicated by the estimated shape of the overall probability weighting function.


(ref:weighting) Possible Shapes of Goldstein and Einhorn's -@goldsteinExpressionTheoryPreference1987 Weighting Function

```{r fig.cap="(ref:weighting)"}
wf <- tibble(gamma = seq(.1, 2, .1)) %>%
  expand_grid(delta = c(.1, .5, 1, 2, 5, 10),
              prob = seq(0, 1, .01)) %>%
  mutate(w = (delta*(prob^gamma))/((delta*prob^gamma)+(1-prob)^gamma))

ggplot(wf) +
  geom_line(aes(prob, w, group = gamma, color = gamma)) +
  geom_segment(x = 0, xend = 1, y =  0, yend = 1, linetype = "dashed", size = .2, color = "gray") +
  scale_color_scico(palette = "cork") + 
  scale_x_continuous(breaks = seq(0, 1, .2)) +
  scale_y_continuous(breaks = seq(0, 1, .2)) + 
  labs(x = "p",
       y = "w(p)", 
       color = expression(gamma)) + 
  facet_wrap(. ~ delta, labeller = label_bquote(cols = delta: .(delta))) + 
  theme_minimal()
```

<!-- Brief review of CPT in DfE: Specifically, when people make DfD, decision weights are typically estimated to be larger than the objective probabilities in the lower probability range.
However, for DfE, this pattern reverses and decisions weights are typically estimated to be smaller than the objective probabilities in the lower probability range [e.g., @barronSmallFeedbackbasedDecisions2003; @hertwigDecisionsExperienceEffect2004; @weberPredictingRiskSensitivity2004; also see @wulffMetaanalyticReviewTwo2018]. -->

# Sampling Strategies

<!-- Old

Given that the choice is perfectly systematic---as opposed to random or noisy---the chosen alternatives in $I'$ should share some property or combination of several properties which the other alternatives in the complementary subset $I \setminus I'$ do not possess.
In the following, I will refer to statements about properties that serve to distinguish between the chosen and non-chosen alternatives as *selection criteria*. 

Often, however, the properties used by the mind to make a choice are not readily described and cannot be directly evaluated [cf. @brunswikRepresentativeDesignProbabilistic1955; see @brunswikPerceptionRepresentativeDesign1956, for a detailed treatment]; rather, proper descriptions must be inferred from whatever information the inputs provide about the properties of choice alternatives, using transformation rules, statistical generalizations, or both.
Since how the inputs are represented determines the information that is made explicit and the costs at which certain operations can be carried out on that information, the feasibility and ease of these inferences depends greatly on the choice of *representation* [@marrVisionComputationalInvestigation1982; see also, e.g., @gigerenzerHowImproveBayesian1995; @griffithsProbabilisticModelsCognition2010; @kempStructuredStatisticalModels2009].
Hence, to explain how the mind makes a selection among choice alternatives, one needs to understand the output controlling properties, i.e., the applied selection criteria, the representation that is used to describe these properties, and the costs of inferring the descriptions in that representation, including their susceptibility to distortions from uncertainty or noise. The process is sketched in Figure 1.  

(ref:figure1) A rough computational level sketch of decision making as an information processing problem. To make a selection among choice alternatives on the basis of particular selection criteria, the mind requires explicit descriptions of the relevant properties before they can be evaluated. Descriptions are either readily available or must be inferred from the inputs.

```{r fig.cap= "(ref:figure1)"}
knitr::include_graphics("images/dm-sketch.png")
```

In what follows, the paper elaborates on the concepts touched upon thus far, certainly only a subset of the factors involved in the decision making process, yet central enough to illustrate a rather important principle:
Given that the mind is indeed sensitive to variations in the property values, the more accurately their descriptions are inferred, the more systematic the choices should become.
Importantly, this principle is not restricted to any one specific property. 
Rather, each selection criterion or combination thereof imposes constraints on the assessment of choice alternatives, leading to characteristic choice patterns that are eventually blurred by inaccurate property descriptions.

For the sake of example, I argue that a choice pattern robustly emerging in *decisions from experience* (DfE), the apparent underweighting of small probability events [e.g., @barronSmallFeedbackbasedDecisions2003; @hertwigDecisionsExperienceEffect2004; @weberPredictingRiskSensitivity2004; see also @wulffMetaanalyticReviewTwo2018, for a meta-analytic review], may be caused by the assessment constraints associated with a round-wise decision strategy.
This particular claim has already been made by @hillsInformationSearchDecisions2010 and follows quite directly from probability theory.
However, the current paper suggests to subordinate decision strategies to the adoption of selection criteria and addresses in some detail two "problems" in the context of DfE, in which descriptions of the properties of choice alternatives must be inferred from sampling data.

For one, there is an *induction problem*, well known from inferential statistics, according to which the sample size determines the margin of error within which any true property of a choice alternative can be inferred, given an otherwise perfect inference process. 
Yet, it follows that if an underweighting pattern is caused by the adoption of an arbitrary selection criterion, it is *a priori* predicted to become more stable with increasing sample size. 
This prediction is in line with the principle already stated above, i.e., that selection-criteria-induced choice patterns should become more systematic as property descriptions become more accurate.
Nevertheless, the prediction contrasts the explanation that underweighting is caused by the reliance on small samples [e.g., @foxDecisionsExperienceSampling2006; @hertwigDecisionsExperienceEffect2004; @hauDescriptionexperienceGapRisky2008; @ungemachAreProbabilitiesOverweighted2009; @rakowBiasedSamplesNot2008], highlighting that outside of the *statistical* models used to describe the choice data, *causal* decision making models of quite different nature may be at play, and that a rather important difference between these causal models resides in the selection criteria they incorporate.
I will elaborate on this matter as I proceed, however, to be clear from the outset, what the mind does in neither case is performing a probability weighting computation in its literal sense, but it processes the inputs in a way that the outputs approximate the solutions of what can be considered probability weighting on an abstract computational level [cf., e.g., @griffithsRationalUseCognitive2015]. 

For another, this paper approaches the problem of organizing the sampling data in such a way that it allows chains of inferences connecting the input set $I$ with the output set $I' \subset I$ that are algorithmically short and simple enough to be carried out by minds whose cognitive capacities are bounded [cf. @simonInvariantsHumanBehavior1990; @simonBehavioralModelRational1955]. 
This may be considered a *representation problem* since the inference chains are constructed around explicit descriptions of the properties of choice alternatives and representations are the "formal schemes" of symbols and rules with which these descriptions are derived [@marrRepresentationRecognitionSpatial1978, p. 270]. 
Specifically, representations differ in how they organize the information used to infer the descriptions of certain properties and these organizations determine the algorithms that can be built into the inference chain [@marrVisionComputationalInvestigation1982].
In this paper, *sampling strategies* in DfE are considered as representation systems used to organize the samples in a way that subserves the property inferences required by the application of particular selection criteria.
Though most of my expositions on the role of sampling strategies remain speculative, the core idea is not new to decision research [see, e.g., @hillsInformationSearchDecisions2010; @wulffHowShortLongrun2015].
By restating this idea and adopting some terms and concepts that have well defined meaning in math and cognitive science, I hope to integrate it into the rich study of the computationally limited mind as an adaptive cognitive system whose processes interact with the information structures in the environment or memory. 

The theoretical arguments sketched in this introduction are substantiated by a simulation study. 
For this purpose, the sampling and decision making processes are implemented in a computational model to simulate choice data from DfE.
The simulated data is modeled in cumulative prospect theory (CPT), a data model commonly used for describing decisions under risk and uncertainty [@goldsteinExpressionTheoryPreference1987; see also @tverskyAdvancesProspectTheory1992]. 
All simulations and analyses are reproducible---materials and instructions can be found on the GitHub repository.

# Inputs to the Decision Making Process and Property Descriptions 

By defining decision making as an information processing problem whose solution requires a selection among choice alternatives of the form described in (1), much weight is given to the properties of choice alternatives---whatever those may be for the moment---since it is only through them that any necessary distinction between the alternatives or collections thereof can be made. 
To stress this point, consider the power set $\mathcal{P}(I)$, which is the set of all possible subsets of $I$ (see example in Figure 2). 

(ref:figure2) A. The power set of the set $I$ including the choice alternatives A, B, and C. Black (white) circles indicate that an alternative does (not) possess a property. Each column displays a possible subset that could result from assessing whether the alternatives posses a relevant property. The properties determine the various relations among the choice alternatives. Note that properties resulting in the empty set $\{\}$ and the subset $\{A,B,C\}$ do not discriminate between choice alternatives. B. A Venn diagram visualizing the inclusion map $\iota : I' \mapsto I$. $\iota$ maps every element $i$ of the subset $I'$ to its image $i$, treated as element of the superset $I$.

```{r fig.cap= "(ref:figure2)"}
knitr::include_graphics("images/power-set.png")
```

Each of the subsets $I' \in \mathcal{P}(I)$ corresponds to a possible distinction between the choice alternatives in $I$ that could result from assessing for each alternative whether it possesses an arbitrary property (collection) or not [cf. @jostMathematicalConcepts2015, p. 16]. 
The concept of the power set thereby illustrates how different selection criteria can alter the consequential choice.
That is, in rather abstract terms, selection criteria are statements regarding the properties of choice alternatives that the mind evaluates as either true or false during the decision making process; accordingly, the subset $I'$ only contains the alternatives for which the statement is evaluated to be true.
It follows that if there is some variance in the properties across choice alternatives and the properties are not perfectly correlated, changing the selection criteria can eventually change the subset $I'$.

## What Are the Choice Alternatives and Their Properties? 

Given (3), it is evident that the prospects can only be distinguished by the elements of their tuples. 
These elements are, generally speaking, symbols, some of which describe properties on their own, e.g., the outcome $\omega$, whereas other properties can only be described by collections of symbols, e.g., the sample space $\Omega$ by a set of outcomes as in (2.1), or more complex combinations thereof, e.g., the weighted average which is obtained by first multiplying all outcomes in $\Omega$ by their respective probabilities in $P$ and then summing the resulting products.^[I do not refer to this quantity as expected value, since the term is reserved for random variables, which were not introduced so far.]
In turn, for choices between prospects, selection criteria are statements about the elements of their tuples that are true or false and these statements imply relations among prospects that are at least nominal, but may also draw on other levels of measurement.  

## Inferring Property Descriptions from Sampling Data

Note that a selection criterion may refer to a property for which the raw input, i.e., the symbols, their organization, and the information they encode, do not yet provide an explicit description.
Even more so, it has been repeatedly stated that for quite many decisions the mind faces, it is simply not possible to draw on direct descriptions of the properties, since they are part of the distal environment  [e.g., @brunswikOrganismicAchievementEnvironmental1943; @fiedlerExplainingSimulatingJudgment1996; @hertwigDescriptionexperienceGapRisky2009; see also @brunswikRepresentativeDesignProbabilistic1955; @brunswikPerceptionRepresentativeDesign1956; @kozyrevaInterpretationUncertaintyEcological2021].
In either case, some form of description must be inferred from whatever information the input provides about the property in question.
Consider first the counterfactual case of the prospect whose tuple is not part of the distal environment but is directly described. 
Since all information about the prospect is given, one may in principle consider the input complete, yet, 
the tuple implies properties of the prospect which are not explicitly described.
For example, to obtain the description of the prospect's weighted average, still a calculus according to the algebraic rules described in the preceding paragraph must be carried out. 
In general, in cases like the one just considered, where the properties are not distal and all information about them is available, property inferences take the form of a mere transformation, where the symbols from the input are ordered and combined according to some transformation rules [cf. @marrVisionComputationalInvestigation1982, p. 20].

In natural world decisions, however, the properties of choice alternatives are often distal and amenable only through their probabilistic relation to proximal cues or samples, which provide the input to the decision making process [cf. @brunswikOrganismicAchievementEnvironmental1943].
Such a proximal information basis is obtained by experience or, more technically, by *sampling*, and is neither consistent nor complete but varies in its degree of representativeness. 
In turn, the mind is not merely required to transform the inputs but to draw inductive inferences to arrive at property descriptions which then, by definition, are only probable. 
Importantly, the probability, or accuracy, of the description depends on both the sampling process that determines the representativeness of the sample and the use of the probabilistic information that is entailed in the sample [e.g., @fiedlerBewareSamplesCognitiveecological2000]. 

To adapt (3) for the case of decisions from experience [@hertwigDecisionsExperienceEffect2004], the tuples $(\Omega, \mathcal{F}, P)$ must be replaced by samples, which in this paper are treated as collections of independent and identically distributed (i.i.d.) random variables $X$.^[Note that this paper considers the counterfactual case where samples are random, although it has been stated that in the natural world samples are "virtually never random" [@fiedlerBewareSamplesCognitiveecological2000, p. 660]. To account for non-random sampling, e.g., in the form of sampling biases [e.g., @fiedlerBewareSamplesCognitiveecological2000] or dynamic information structures [e.g., @cohenEffectPerceivedPatterns2021; @plonskyRelianceSmallSamples2015], the i.i.d-assumption would need to be dropped, which is out of the scope of this paper.]
Specifically, random variables are defined as a measurable function 

$$
X: (\Omega, \mathcal{F})  \mapsto (\Omega', \mathcal{F'}) 
\; ,
\tag{4}
$$

where $\Omega'$ is a set of real numbered values $X$ can take and $\mathcal{F'}$ is a set of subsets of $\Omega'$. 
I.e., $\Omega$ maps into $\Omega'$ such that correspondingly each subset $A' \in \mathcal{F'}$ has a pre-image $X^{-1}A' \in \mathcal{F}$, which is the set $\{\omega \in \Omega: X(\omega) \in A'\}$ [@kolmogorovFoundationsTheoryProbability1950, p. 21].


$$
\begin{aligned}
  & \iota : I' \mapsto I, \ \text{where} \ I = \{X_{1i}, ..., X_{ki}\} \\
  & i \in \{1, ..., N\} 
  \; .
\end{aligned}
\tag{5}
$$


To provide a formal definition of sampling in risky choice, we make use of the mathematical concept of a random variable and start by referring to a prospect as *"risky"* in the case where $p(\omega) \neq 1$ for all $\omega \in \Omega$.
Here, risky describes the fact that if agents would choose a prospect and any of its outcomes in $\Omega$ must occur, none of these outcomes will occur with certainty. 
It is acceptable to speak of the occurrence of $\omega$ as a realization of a random variable $X$ defined on a prospect iff the following conditions (1) and (2) are met: 


(2) The mapping is such that $X(\omega) = x \equiv \omega$. 

In (2), $x \equiv \omega$ means that the realization of a random variable $X(\omega) = x$ is numerically equivalent to its pre-image $\omega$.  
Given conditions (1) and (2), we denote any observation of $\omega$ as a *"single sample"*, or realization, of a random variable defined on a prospect and the act of generating a sequence of single samples in discrete time as *"sequential sampling"*. 
Note that, since random variables defined on the same prospect are independent and identically distributed (iid), the weak law of the large number applies to the relative frequency of occurrence of an outcome $\omega$ in a sequence of single samples originating from the same prospect.
Thus, long sample sequences in principle allow to obtain the same information about a prospect by sampling as by symbolic description.

Consider now a choice between prospects $1, ..., k$.
To construct a stochastic sampling model for DfE, we assume that agents base their decision on the information related to these prospects and define a decision variable as a function of the latter:

$$
D:= f((\Omega_1, \mathcal{F}_1, P_1), ..., (\Omega_k, \mathcal{F}_k, P_k))
\;.
$$

Now, since in DfE no symbolic descriptions of the prospects are provided, the model must be restricted to the case where decisions are based on sequences of single samples originating from the respective prospects:

$$
D := f(X_{i1}, ..., X_{ik}) 
\; ,
$$

where $i \in \{1, ..., N\}$ denotes a sequence of length $N$ of random variables that are iid.  

Concerning the form of $f$ and the measures it utilizes, it is quite proper to say that they reflect our assumptions about the exact kind of information agents process and the way they do and that these choices should be informed by psychological theory and empirical protocols. 
Taking the case of different sampling and decision strategies previously assumed to play a role in DfE, the following section demonstrates how such assumptions can be explicated in a stochastic model that builds on the sampling approach outlined so far.  

-->

# Simulation Study

...

The switching probability $s$ is the probability with which agents draw a single sample from the prospect they did not get their most recent single sample from.
$s$ is varied between .1 to 1 in increments of .1.
The two boundary parameters resemble the concept of a decision threshold, i.e., if a prospect reaches a boundary, it is chosen by the synthetic agent.
The boundary type is either the minimum number of comparisons any prospect must win (absolute boundary) or the minimum difference between the number of won comparisons (relative boundary).
The boundary value $a$ is varied between 1 to 5 in increments of 1.

<!-- Test set-->

For each parameter combination of the generating model, 100 synthetic agents are presented with 60 choices problems.
In sum, 100 (parameter combinations) x 100 (agents) x 60 (choices) = 600,000 choices are simulated.
We test a set of 2-prospect choice problems, where one of the prospects contains a safe outcome, i.e., $p(\omega) = 1$ and the other two outcomes where all $p(\omega) \neq 1$.
Both outcomes and probabilities are drawn from uniform distributions, ranging from 0 to 20 for outcomes and from .01 to .99 for probabilities of the smaller outcome of the risky prospect.
To omit dominant prospects within a choice problem, outcomes of the safe prospect always fall between both outcomes of the risky prospect.
Table A1 in [Appendix 2][Appendix 2: Choice Problems] contains the test set of 60 choice problems, which were sampled from an initial set of 10,000.
Sampling of gambles was stratified, randomly drawing an equal number of 20 gambles with no, an attractive, and an unattractive rare outcome.
Risky outcomes are considered *"rare"* if their probability is $p < .2$ and *"attractive"* (*"unattractive"*) if they are higher (lower) than the safe outcome.

<!-- Results -->

...

```{r}

# read cpt data

cols <- list(.default = col_double(),
             boundary = col_factor(),
             a = col_factor(),
             parameter = col_factor())
cpt_long <- read_csv("data/estimates/estimates_cpt.csv", col_types = cols)

# store convergence diagnostics

gel_92 <- cpt_long %>% select(s, boundary, a, parameter, Rhat, n.eff) 
```

For each distinct parameter combination, we ran 20 chains of 40,000 iterations each, after a warm-up period of 1000 samples.
To reduce potential autocorrelation during the sampling process, we only kept every 20th sample (thinning).
The potential scale reduction factor $\hat{R}$ [@gelmanInferenceIterativeSimulation1992] was $\leq$ `r round(max(gel_92$Rhat), 3)` for all parameters, indicating good convergence.
The minimum effective sample size was `r min(gel_92$n.eff)`.

<!-- Plausibility Check: Relationship between Switching Probability and Trial Length -->

```{r include=FALSE}

# read choice data 

cols <- list(.default = col_double(),
             boundary = col_factor(),
             gamble = col_factor(),
             rare = col_factor(),
             agent = col_factor(),
             choice = col_factor())
choices <- read_csv("data/choices/choices.csv", col_types = cols)

# get median trial length for each parameter combination

trial_length <- choices %>% 
  group_by(s, boundary, a) %>% 
  summarise(med = median(n_sample))
```

The median length of trials, i.e., the number of single samples drawn in a trial, generated by different parameter combinations ranged from `r min(trial_length$med)` to `r max(trial_length$med)`.
As expected, the scatter plot below shows an inverse relationship between switching probability and trial length.
I.e., the lower the switching probability, the larger become the sample sequences on which each comparison between prospects is based, which in turn leads to longer trials.  
This effect is particularly pronounced for low probabilities such that the increase in trial length accelerates as switching probability decreases.


```{r eval=FALSE}

# get median trial length for each switching probability 

trial_length_s <- choices %>% 
  group_by(s) %>%
  summarise(med = median(n_sample))

# plot

trial_length %>%
  ggplot(aes(x = s, y = med)) +
  geom_jitter(color = "#CECECE", size = 3) +
  geom_point(data = trial_length_s, aes(color = s), size = 3) +
  geom_path(data = trial_length_s, aes(color = s)) +
  scale_color_scico(palette = "buda") + 
  scale_x_continuous(breaks = seq(0, 1, .1)) + 
  scale_y_continuous(breaks = seq(0, 120, 10)) +
  labs(title = "Trial Length", 
       x ="Switching Probability",
       y = "Median Trial Length", 
       color="Switching Probability") + 
  theme_apa()
```

<!-- Weighting function -->

```{r eval=FALSE}

# tidy CPT data: parameters as separate columns 

cpt_wide <- cpt_long %>% 
  select(s, boundary, a, parameter, mean) %>% 
  pivot_wider(names_from = parameter, values_from = mean)
```

The figures below display the estimates of the $\gamma$ and $\delta$ parameter of the probability weighting function [@prelecProbabilityWeightingFunction1998] fitted to DfE simulated for different parameter values of the generating model.
The estimates are plotted against the switching probability, where each panel represents a distinct boundary type and level, i.e., number of comparisons. 
Grey dots represent agent level estimates, colored dots represent the mean across all agent level estimates. 

Most significantly, there is a strong relationship between the switching probability in the generating model and the $\gamma$ parameter.
I.e., large switching probabilities, which are indicative for small sample sequences, lead to larger estimates for $\gamma$. 
The resulting strong curvature leads to a compression of probabilities in the lower and upper range, reflecting underweighting of small probabilities and overweighting of large probabilities. 
This pattern is robust for varying degrees of the boundary level, except for $a = 1$. 
The latter deviation may be explained by the potential scale reduction factors for the respective parameter estimates, which indicate that the MCMC chains did not converge (see [Appendix 3][Appendix 3: Convergence Diagnostics for CPT Parameters]).   

```{r eval=FALSE}

# Density Plot 

cpt_long %>% 
  filter(parameter == "gamma" | parameter == "delta") %>% 
  ggplot(aes(x = s, y = mean)) +
  geom_density_2d_filled(contour_var = "ndensity") + 
  scale_fill_scico_d(palette = "acton") +
  scale_color_scico_d(palette = "acton") + 
  scale_x_continuous(breaks = seq(0, 1, .2)) + 
  scale_y_continuous(limits = c(0,2), breaks = seq(0, 2, .2)) +
  labs(x ="Switching Probability",
       y = "Parameter Value",  
       fill="Density") + 
  facet_grid(parameter~a) + 
  theme_apa()
```

```{r eval=FALSE}

# Scatter Plots 

# Gamma 

cpt_wide %>% 
  ggplot(aes(x = s, y = gamma, color = s)) +
  geom_point(size = 2) +
  geom_line(size = 1) +
  scale_color_scico(palette = "buda") + 
  scale_x_continuous(breaks = seq(0, 1, .2)) + 
  scale_y_continuous(breaks = seq(0, 2, .2)) +
  facet_grid(boundary~a) +
  labs(title = expression(paste("Curvature ", gamma)),
       x ="Switching Probability",
       y = expression(gamma), 
       color="Switching Probability") + 
  theme_minimal()

# Delta

cpt_wide %>% 
  ggplot(aes(x = s, y = delta, color = s)) +
  geom_point(size = 2) +
  geom_line(size = 1) +
  scale_color_scico(palette = "buda") + 
  scale_x_continuous(breaks = seq(0, 1, .2)) + 
  scale_y_continuous(breaks = seq(0, 2, .2)) +
  facet_grid(boundary~a, switch = "y") +
  labs(title = expression(paste("Curvature ", delta)),
       x ="Switching Probability",
       y = expression(delta), 
       color="Switching Probability") + 
  theme_minimal()
```

Below, the resulting probability weighting functions are displayed. 

```{r eval=FALSE}

# Weighting Functions 

## compute decision weights 

cpt_w <- cpt_wide %>% 
  select(-c(alpha, rho)) %>% 
  expand_grid(ep = seq(0, 1, .1)) %>% #
  mutate(w = round(  (delta * ep^gamma)/ ((delta * ep^gamma)+(1-ep)^gamma), 2)) 

## plot curves

cpt_w %>% 
  group_by(boundary, a, s) %>% 
  ggplot(aes(x = ep, y = w, color = s)) +
  geom_path() + 
  scale_color_scico(palette = "tokyo") + 
  scale_x_continuous(breaks = seq(0, 1, .2)) +
  scale_y_continuous(breaks = seq(0, 1, .2)) + 
  labs(x ="Experienced Probability",
       y = "Probability Weighting", 
       color="Switching Probability") + 
  theme_minimal()

cpt_w %>% 
  group_by(boundary, a, s) %>% 
  ggplot(aes(x = ep, y = w, color = s)) +
  geom_path(size = 1) + 
  scale_color_scico(palette = "tokyo") + 
  scale_x_continuous(breaks = seq(0, 1, .2)) +
  scale_y_continuous(breaks = seq(0, 1, .2)) + 
  facet_grid(boundary~a) +
  labs(x ="Experienced Probability",
       y = "Probability Weighting", 
       color="Switching Probability") + 
  theme_dark()
```

The false response rates for different parameter values of the generating model reflect the probability weighting patterns from above. 
That is, the strong curvature resulting from large switching probabilities produces an underweighting of small probabilities. This in turn has the effect that the rarity of an attractive (unattractive) outcome leads to higher rates of choosing the safe (risky) prospect although the risky (safe) prospect had a higher experienced expected value.

```{r eval=FALSE}

# compute false response rates

fr_rates <- choices %>% 
  mutate(ev_ratio_exp = round(a_ev_exp/b_ev_exp, 2), # experienced EV (eEV)
         norm = case_when(ev_ratio_exp > 1 ~ "A", ev_ratio_exp < 1 ~ "B")) %>% # normative choice according to eEV
  filter(!is.na(norm)) %>% # exclude trials with normative indifferent prospects
  group_by(s, boundary, a, rare, norm, choice) %>% # group correct and incorrect responses
  summarise(n = n()) %>% # absolute numbers 
  mutate(rate = round(n/sum(n), 2), # response rates 
         type = case_when(norm == "A" & choice == "B" ~ "false safe", norm == "B" & choice == "A" ~ "false risky")) %>% filter(!is.na(type))  # remove correct responses

# violin scatter plot

fr_rates %>% 
  ggplot(aes(x = rare, y = rate, color = s)) +
  geom_quasirandom(aes(shape = type), size = 3) +  
  scale_y_continuous(breaks = seq(0, 1, .2)) +
  scale_color_scico(palette = "buda") + 
  scale_shape_manual(values=c(8, 16))+
  labs(x = "Rare Event", 
       y = "False Response Rate", 
       color = "Switching Probability",
       shape = "False Response") + 
  theme_minimal() 
```

```{r eval=FALSE}
fr_rates %>% 
  ggplot(aes(a, s, fill = rate)) + 
  geom_tile(colour="white", size=0.1) +
  scale_fill_scico(palette = "buda") + 
  facet_grid(type ~ fct_relevel(rare, "attractive", "none", "unattractive"), switch = "y") +
  scale_x_continuous(expand=c(0,0), breaks = seq(1, 5, 1)) +
  scale_y_continuous(expand=c(0,0), breaks = seq(.1, 1, .1)) +
  labs(title = "False Response Rates", 
       x = "a", 
       y= "s", 
       fill = "% False Responses") + 
  theme_minimal() 


fr_rates %>% 
  ggplot(aes(s, rate, color = s)) + 
  facet_grid(type ~ fct_relevel(rare, "attractive", "none", "unattractive"), switch = "y") +
  geom_jitter(size = 3) + 
  scale_x_continuous(breaks = seq(0, 1, .1)) +
  scale_y_continuous(breaks = seq(0, 1, .1)) +
  scale_color_scico(palette = "buda") + 
  labs(title = "False Response Rates", 
       x = "s", 
       y= "% False Responses", 
       color = "a") + 
  theme_minimal()
```

<!-- Weighting function -->

```{r eval=FALSE}

# Density Plot 

cpt_wide %>%
  ggplot(aes(x = s, y = alpha)) +
  geom_density_2d_filled(contour_var = "ndensity") + 
  scale_fill_scico_d(palette = "grayC") +
  scale_x_continuous(breaks = seq(0, 1, .2)) + 
  scale_y_continuous(limits = c(0,2), breaks = seq(0, 2, .2)) +
  labs(x ="Switching Probability",
       y = "Parameter Value",  
       fill="Density") + 
  facet_wrap(~a, nrow = 1) + 
  theme_minimal()
```

```{r eval=FALSE}
# value function 

## compute values 

cpt_v <- cpt_wide %>% 
  select(-c(gamma, delta, rho)) %>% 
  expand_grid(x = seq(0, 20, 2)) %>%  
  mutate(v = round(x^alpha, 2)) 

## plot curves

cpt_v %>% 
  group_by(boundary, a, s) %>% 
  ggplot(aes(x = x, y = v)) +
  geom_path(aes(color = s)) +
  scale_color_scico(palette = "grayC") + 
  scale_x_continuous(breaks = seq(0, 20, 5)) +
  scale_y_continuous(breaks = seq(0, 20, 5)) + 
  labs(title = "Value Function",
       x ="Objective Value",
       y = "Subjective Value", 
       color="Switching Probability") + 
  theme_minimal()

cpt_v %>% 
  group_by(boundary, a, s) %>% 
  ggplot(aes(x = x, y = v)) +
  geom_path(aes(color = s)) +
  scale_color_scico(palette = "buda") + 
  scale_x_continuous(breaks = seq(0, 20, 5)) +
  scale_y_continuous(breaks = seq(0, 20, 5)) + 
  facet_wrap(~a) +
  labs(title = "Value Function",
       x ="Objective Value",
       y = "Subjective Value", 
       color="Switching Probability") + 
  theme_minimal()
```

```{r eval=FALSE}

# alpha
cpt_wide %>% 
  ggplot(aes(x = s, y = alpha, color = s)) +
  geom_point(size = 2) +
  geom_line(size = 1) +
  scale_color_scico(palette = "buda") + 
  scale_x_continuous(breaks = seq(0, 1, .2)) + 
  scale_y_continuous(breaks = seq(0, 2, .2)) +
  facet_grid(boundary~a, switch = "y") +
  labs(title = expression(alpha),
       x ="Switching Probability",
       y = expression(alpha), 
       color="Switching Probability") + 
  theme_minimal()
```

# Summary and Conclusion

# Acknowledgement

# References
