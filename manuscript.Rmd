---
title             : "Sampling Strategies in Decisions from Experience: Property Representation and Probability Weighting Approximation"
shorttitle        : "Sampling strategies"

author: 
  - name          : "Linus Hof"
    affiliation   : "1"
    corresponding : yes    
    address       : "Department of Psychology, Heidelberg University, Hauptstra√üe 47-51, 69117 Heidelberg, Germany"
    email         : "linushof@posteo.de"
    
affiliation:
  - id            : "1"
    institution   : "Heidelberg University"

note: | 
  **Unsubmitted draft created from the commit with the hash**: ``r repro::current_hash()``
  
authornote: |
 This is a dynamic document which can be reproduced from the accompanying GitHub repository: https://github.com/linushof/sampling-in-dfe. 
 
 The current version of this manuscript is prepared for submission as a master's thesis.
 Supervisors: Thorsten Pachur & Veronika Zilker.   
 
abstract: |
  Add short abstract.
  
keywords          : ""
wordcount         : ""

bibliography      : references_sampling-in-dfe.bib
csl               : /Users/Linus Hof/Documents/Psychologie/ARC/sampling-in-dfe/apa.csl
appendix          : manuscript_appendix.Rmd

figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
linkcolor         : "blue"
mask              : no
draft             : no

classoption       : "man"
documentclass     : "apa7"
output: papaja::apa6_pdf

header-includes:
   - \usepackage[capposition=top]{floatrow}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, #omit messages
                      fig.align = "l", #align figures to left margin
                      fig.pos = "t")
```

```{r packages, include=FALSE}
#load required packages
pacman::p_load(papaja,
               readr,
               tidyr, 
               dplyr,
               ggplot2,
               viridis,
               ggpubr,
               latex2exp)
```

<!-- Introduction -->

The human mind is a cognitive system that operates on inputs from an environment or memory, and in the domain of decision making, such inputs provide information about the choice alternatives between which people can choose.
In fact, irrespective of their exact specification, decision models generally take as inputs information about a set of choice alternatives, each possessing some defining *properties*, and returns a subset of chosen alternatives as outputs [@heOntologyDecisionModels2020].

Now, let a *prospect* be a choice alternative that possesses only two kinds of primitive properties: the possible outcomes of the alternative and the probabilities with which these outcomes occur following its choice.
Importantly, the information that inputs provide about the properties of a prospect can come in different forms.
For example, in *decisions from description* (DfD), inputs take the form of explicit and complete descriptions of all outcomes and probabilities.
Yet, in *decisions from experience* [DfE, @hertwigDecisionsExperienceEffect2004], these properties are latent and therefore not known with certainty, but they must be inferred from the relative frequencies with which the outcomes occurred when the prospect was chosen in past decisions.
The inputs to DfE therefore take the form of sampled outcomes.
Although these distinct input forms may in principle carry the same information about a given set of prospects, behavioral decision research has so far produced a large body of papers indicating that---taken a rather general perspective---there are robust differences between the choices that are made in DfD and DfE, leading to the notion of the *description-experience gap* [@hertwigDescriptionexperienceGapRisky2009].
In one reading of the gap, empirical choice patterns indicate that in DfD people choose as if a small-probability outcome is given more weight in the evaluation of a prospect than would be objectively warranted by the principle of mathematical expectation; in turn, in DfE, ceteris paribus, people choose as if the same small-probability outcome is given less weight than would be warranted [e.g., @barronSmallFeedbackbasedDecisions2003; @hertwigDecisionsExperienceEffect2004; @weberPredictingRiskSensitivity2004; @erevAnomaliesForecastsDescriptive2017; see @wulffMetaanalyticReviewTwo2018, for a comprehensive meta-analytic review].
Hereafter, these choice patterns are referred to as the *as-if overweighting* and *as-if underweighting of rare outcomes*, respectively.

Similar to other behavioral phenomena that could be explained by assuming that the mind is sensitive to the composition of the *samples*---i.e., sets of sampled outcomes---on which it operates [see @fiedlerBewareSamplesCognitiveecological2000], the as-if underweighting of rare outcomes in DfE could be explained by assuming that the mind is sensitive to the information provided by the small samples that people tend to rely on [see @hertwigDecisionsExperienceEffect2004; @plonskyRelianceSmallSamples2015; @rakowBiasedSamplesNot2008; @wulffMetaanalyticReviewTwo2018].
That is, following from the laws of large numbers [LLN, see, e.g., @kolmogorovFoundationsTheoryProbability1950], the relative frequencies with which the prospect's outcomes occur in a sample of infinite size converge almost surely to the respective objective probabilities.
In such a case, or in the less strict case of large samples, then, the binomial distributions of the outcomes can be approximated by a symmetric normal distribution [cf. @georgiiStochastikEinfuhrungWahrscheinlichkeitstheorie2015].
However, for small samples, the binomial distribution associated with small-probability outcomes is positive skewed, therefore causing the relative frequencies with which these outcomes occur in the sample to be smaller rather than larger than their latent objective probabilities.
Thus, small-probability outcomes tend to be underrepresented in small samples and might not occur at all.

One class of process models that provide a mechanistic link between the actual sampling process, the reliance on small samples, and the as-if underweighting of rare outcomes are sequential sampling models of decision making [e.g., @bhatiaSequentialSamplingParadoxes2014; @busemeyerDecisionFieldTheory1993; @markantModelingChoiceSearch2015; @ratcliffComparisonSequentialSampling2004].
That is, in one paradigm used to study DfE, namely, the sampling paradigm [@weberPredictingRiskSensitivity2004], before making a final choice between prospects with latent properties, people can, as much as they want, sequentially sample the prospects' outcomes according to their objective probabilities.
To relate the sampling process to the final choice, sequential sampling models assume that the information-processing chain upfront the choice takes the form of an evidence accumulation process over time, with choices becoming more accurate the more evidence must be accumulated.
Specifically, these models assume that the sampled outcomes are sequentially accumulated into one or more dynamic decision variables and that a choice is made once a decision variable exceeds a threshold in favor for one of the prospects.
Now, since the thresholds determine how much (more) evidence a prospect must accumulate in order to be chosen and since the amount of evidence for either prospect can only increase by sampling more outcomes, these models predict, ceteris paribus, an as-if underweighting of rare outcomes for low thresholds [see @markantModelingChoiceSearch2015].
In other words, a decrease in the threshold leads to a decrease in the number of outcomes that must be sampled to reach this threshold.
Because of the skewness of their binomial distribution, then, small-probability outcomes are predicted to contribute less to the accumulated evidence than would be warranted by their objective probabilities.
In turn, the higher a threshold, and thus the higher the number of outcomes that must be sampled to reach this threshold, the more should the amount of evidence contributed by small-probability outcomes correspond to what would be objectively warranted.

Yet, the as-if underweighting of rare outcomes in DfE may appear, even if the overall number of sampled outcomes is large and the relative frequencies across all sampled outcomes closely resemble the latent objective probabilities [@hauDescriptionexperienceGapRisky2008; @hauDecisionsExperienceStatistical2010; @ungemachAreProbabilitiesOverweighted2009; see also @wulffMetaanalyticReviewTwo2018].
While this suggests that the as-if underweighting of rare outcomes in DfE cannot be reduced to a small overall number of sampled outcomes, this paper emphasizes that it may still be the case that the respective deviations from the principle of mathematical expectation were caused by the reliance on small samples.
That is, there may be a discrepancy between the overall number of sampled outcomes and the size of the samples that are actually used to evaluate the prospects or to make an inference about their latent properties, respectively.
Accordingly, one approach to explain the as-if underweighting of rare outcomes in the light of a large overall number of sampled outcomes is to assume that some of the sampled outcomes are discounted or ignored in the evaluation of prospects.
For example, some studies suggest that outcomes that were sampled more recently receive more weight in the evaluation of a prospect [see @hertwigDecisionsExperienceEffect2004] or that only those outcomes are considered right before a decision that followed some sequential pattern [see @cohenEffectPerceivedPatterns2021; @plonskyRelianceSmallSamples2015].


Specifically, a core assumption of this paper is that any number of sampled outcomes from a prospect may either be integrated into the same sample or into multiple respectively smaller samples, and that each of these samples serves an independent evaluation of a prospect.
Accordingly, to explain the as-if underweighting of rare outcomes in the light of a large overall number of sampled outcomes,  

his paper presents a process model that can explain an as-if underweighting of rare outcomes, this approach differs in two respects from other approaches:
It does not assume that the overall number of sampled outcomes is small and it does also not assume that some of the sampled outcomes are discounted or ignored.


Rather, in another approach, @hillsInformationSearchDecisions2010 propose a dependency between sampling patterns and decision strategies that provides a link between the reliance on small samples and the as-if underweighting of rare outcomes that does not assume a discounting or omission of sampled outcomes. 
Considering the choice between two prospects, this approach distinguishes sampling patterns along the frequency of switches between the prospects during the sampling phase and assumes that only outcomes that were sampled from the same prospect between switches are integrated into the same sample.
Then, it is assumed that each time after a new sample for both prospects was generated, their means are compared, and the choice is determined by the number of won comparisons.^[<!--Open footnote-->Note that @hillsInformationSearchDecisions2010 describe only the two paradigmatic cases, where prospects are either switched just once (comprehensive sampling) or after each sampled outcome (piecewise sampling). Accordingly, in the former case choices depend on a single comparison between the means of samples that contain all outcomes sampled for the respective prospect (summary strategy); in the latter case, however, choices depend on multiple comparisons between samples that contain only one sampled outcome (round-wise strategy). Nevertheless, Hills and Hertwig [-@hillsInformationSearchDecisions2010, p. 1788] acknowledge that "many [sampling] strategies will fall on the continuum between [these cases]", which is why the general description is used above.<!--Close footnote-->]
Since each comparison is weighted equally, such a link between sampling patterns and decision strategies can explain an as-if underweighting of rare outcomes despite a large overall number of sampled outcomes being considered.
More specifically, while the probability of a rare outcome to be sampled increases with the overall number of sampled outcomes, an increasing switching frequency causes the rare outcome not to be considered in an increasing number of comparisons.

To substantiate this claim, this paper defines a *sampling strategy* as the probability of sampling outcomes from different prospects in direct succession (hereafter: switching probability) and presents a sequential sampling model that accumulates the evidence obtained from the comparisons between sample means.
More specifically, similar to @hillsInformationSearchDecisions2010, the model only integrates the outcomes that are sampled from the same prospect between switches into the same sample and each time after a new sample for both prospects was generated, their respective means are compared.
The information about the outcomes of these comparisons are integrated into either one dynamic decision variable capturing the difference in the number of won comparisons or into two dynamic decision variables capturing the absolute number of won comparisons for both prospects separately.
Accordingly, the thresholds determine how much (more) comparisons a prospect must win in order to be chosen. 
The model demonstrates that sampling strategies, through their probabilistic relation to the size of the samples, determine which differences in the latent properties of the prospects are represented in the outcomes of the comparisons.
That is, for a decreasing switching probability, the number of outcomes that are successively sampled from a given prospect increases such that the size of the sample grows.
Accordingly, the model shows how with decreasing switching probability a mean comparison becomes more indicative for differences of the prospects in their expected values.
In turn, for an increasing switching probability, the number of outcomes that are successively sampled from a given prospect decreases such that the size of the sample decreases.
Accordingly, the growing sequence of sample comparisons becomes more indicative for which prospect produces the higher outcomes most of the time.

Now, this paper integrates both approaches to discuss in some more detail how the effect of small samples may exert its influence on the choices between two prospects in DfE.
The main argument here is that whereas the reliance on small samples generally explains the as-if underweighting of rare outcomes in DfE, this explanation cannot be equated with inaccurate inferences about the latent properties of prospects that carry over to the subsequent choices.
Rather the opposite can be the case.
That is, the latent properties may imply different relations among the prospects in the form that choices according to the differences in one latent property may not coincide with the choices according to the differences in another latent property.
Hence, although technically the reliance on small samples caused deviations from the principle of mathematical expectation, these deviations may reflect an accurate assessment of differences in a latent property other than the expected values.



# The Description of Choices in CPT

Following @kolmogorovFoundationsTheoryProbability1950,^[The symbolic notation from @georgiiStochastikEinfuhrungWahrscheinlichkeitstheorie2015 is adopted.] let the axiomatic definition of a prospect be that of a discrete random variable's distribution.
Let therefore the tuple $(\Omega, \mathcal{F}, P)$ be a probability space.
Accordingly, $\Omega$ is a finite sample space with each element $\omega \in \Omega$ denoting a possible consequence of choosing the respective prospect.
$\mathcal{F}$ is the power set $\mathcal{P}(\Omega)$ with each element $A \in \mathcal{F}$ being a subset of $\Omega$.
$P$ is the probability measure
$$
P: \mathcal{F} \to [0,1]
\tag{1}
$$
assigning the elements of $\mathcal{F}$ a probability $0 < p(A) \leq 1$ with $P(\Omega) = 1$.
Now, the random variable is a function
$$
\begin{aligned}
  X : (\Omega, \mathcal{F}) &\to (\Omega', \mathcal{F'})  \\
  \omega &\mapsto x
  \; ,
\end{aligned}
\tag{2}
$$
where $(\Omega', \mathcal{F'})$ is a measurable space with each element $x \in \Omega'$ denoting a possible outcome of a prospect and each element $A' \in \mathcal{F'}$ being a subset of $\Omega'$.
An outcome's probability $p_X(x)$ is then provided by the pushforward measure
$$
P_X : \mathcal{F'} \to [0,1]
\tag{3}
$$
with 
$$
P_X(A' \in \mathcal{F'}) := P(\{\omega: X(\omega) = x \in A'\})
\; .
\tag{4}
$$
To summarize and simplify, given a choice between $1, ..., k$ prospects, the inputs from the environment or memory should provide information about the set
$$
\{P_{X_1}, ... , P_{X_k}\}
\; ,
\tag{5}
$$
where each element $P_X$ can itself be considered a set of outcome-probability pairs $\{(x_, p_X(x)\}$.

The choices people actually make between prospects are often described in terms of deviations from the principle of mathematical expectation, according to which the prospect with the largest expected value (EV)
$$
EV = \sum_i^n x_i \times p_X(x_i)
\tag{7}
$$ 
should be chosen (EV maximization).
To *describe* how people's actual choices deviate from this principle [see @erevAnomaliesForecastsDescriptive2017, for a recent replication of classical demonstrations],^[<!--Open footnote-->@tverskyAdvancesProspectTheory1992 addressed the *fourfold pattern of risk attitudes*---i.e., choices between prospects indicate risk aversion for gains of high probability and losses of low probability, and risk seeking for gains of low probability and losses of high probability---and related effects, e.g., the *certainty effect* and the *reflection effect*, which were already addressed by CPT's predecessor, prospect theory [@kahnemanProspectTheoryAnalysis1979]. Together, these empirical choice patterns indicate a violation of the principle of mathematical expectation and the expected utility (EU) principle [see @bernoulliExpositionNewTheory1954\/1738].<!--Close footnote-->]
CPT and similar rank-dependent models [see @stottCumulativeProspectTheory2006, for an overview of models] fit choice data by maximizing^[<!--Open footnote-->Note that CPT may be amended by a stochastic choice rule to account for the probabilistic nature of choices [@rieskampProbabilisticNaturePreferential2008]. Stochastic choice rules [@stottCumulativeProspectTheory2006, for an overview] make the choice of a prospect more probable the better its valuation relative to those of other prospects.<!--Close footnote-->]
the value
$$
V = \sum_i^n v(x_i) \times \pi_i
\; ,
\tag{8}
$$ 
with objective outcomes $x_i$ being transformed by a value function $v$, and *cumulative decision weights* $\pi_i$ being determined by the difference between transformed cumulative probabilities of the distribution $P_X$.
More specifically, following @tverskyAdvancesProspectTheory1992, the objective outcomes are transformed by a value function
$$
\begin{aligned}
  v : \Omega' &\to \mathbb{R} \\
  x &\mapsto 
  \begin{cases}
     x_i^\alpha &\forall x_i \geq 0, \\
     -\lambda |x_i|^\alpha &\text{else}
     \; ,
  \end{cases}
\end{aligned}
\tag{9}
$$
with $\alpha \in [0,1]$ determining the degree of the function's concavity (convexity) over the positive (negative) outcome interval, and $\lambda > 1$ increasing the function's slope over the negative outcome interval only.
Each subjective value $v(x)$ is then multiplied (or: weighted) with a cumulative decision weight that takes the form
$$
\pi_i =
  \begin{cases}
     w^+(P(X \geq x_i)) - w^+(P(X > x_i)) \quad \forall x_i \geq 0, \\
     w^-(P(X \leq x_i)) - w^-(P(X < x_i)) \quad \text{else}
     \; , 
  \end{cases}
\tag{10}
$$
where $w$ is a monotonic increasing, nonlinear weighting function $w: [0,1] \to [0,1]$ satisfying $w^+(0) = w^-(0) = 0$ and $w^+(1) = w^-(1) = 1$.

Essentially, the proposed model is such that the cumulative decision weights derived for the outcomes may be greater or smaller than their objective probabilities, causing the transformed outcomes---i.e., the subjective values---to be over- or underweighted *in* CPT, respectively.
<!--Accordingly, one may adopt an as-if weighting terminology, stating that people choose *as if* they maximized the value in (10) and applied the weighting pattern that was estimated in CPT.-->
Now, several consequences for the weighting pattern that can be estimated in CPT follow from (12), which are reviewed next:

* From the transformation of cumulative probabilities it necessarily follows that the value of a cumulative decision weight $\pi$ depends on the rank of the outcome $x$ for which $\pi$ is determined.

* The value of a cumulative decision weight depends on the estimated shape of the graph of $w$, the weighting function's parameters respectively.
That is, the shape of the graph of $w$ displays in which interval on the cumulative probability scale $[0,1]$ the images of $w$, $w(p)$, take values that are greater or smaller than the respective cumulative probabilities and how much greater or smaller these images are.
Hence, assuming a nonlinear graphical shape of $w$, the same numerical difference between objective cumulative probabilities may translate to cumulative decision weights of different value.

* For prospects containing either only positive or only negative outcomes, $\sum_i^n\pi_i = 1$ is satisfied.
Accordingly, for such prospects, the weighting of subjective values with cumulative decision weights rather than with objective probabilities may be roughly understood as a redistribution of the entire probability mass of $P(\Omega') = 1$ across outcomes [@zilkerNonlinearProbabilityWeighting2021].

\noindent
Given that the value of each cumulative decision weight depends on the two transformed cumulative probabilities in (12)---i.e., the probability of obtaining a positive (negative) outcome equal to or greater (smaller) than a respective outcome $x$, and the probability of obtaining a strictly greater (smaller) outcome---the remaining consequences for the weighting pattern are reviewed by considering the actual weighting function.
Therefore, Figure\ \@ref(fig:weighting-function) illustrates some of the possible graphical shapes of the two-parameter weighting function of @goldsteinExpressionTheoryPreference1987, which, however, is just one of several parameterizations that have been proposed [e.g., @prelecProbabilityWeightingFunction1998; @tverskyAdvancesProspectTheory1992; see @stottCumulativeProspectTheory2006, for an overview].
Now, let each $p$ on the abscissa be one of the four cumulative probabilities from (12). 
Then each graph in Figure\ \@ref(fig:weighting-function) displays the graphical shape of the weighting function
$$
\begin{aligned}
  w : [0,1] &\to [0,1] \\
  p &\mapsto \frac{\delta \times p^{\gamma}}
  {\delta \times p^{\gamma} + (1-p)^{\gamma}}
  \; ,
\end{aligned}
\tag{11}
$$
for the respective values of the parameters $\gamma \in [0,2]$ and $\delta > 0$.
Evidently, both parameters have distinct effects on the graphs' shape, with $\gamma$ affecting the curvature and $\delta$ the elevation.
As a consequence from (12) then, each combination of parameters implies a particular weighting pattern, with some of them being similar and others being rather distinct.

(ref:weighting-function) Possible Graphical Shapes of Goldstein and Einhorn's -@goldsteinExpressionTheoryPreference1987 Weighting Function

```{r weighting-function, fig.cap="(ref:weighting-function)", fig.height=7, fig.width=10}
#compute images of weighting function (wf)
wf <- tibble(p = seq(0, 1, .01)) %>%  #cumulative probabilities
  expand_grid(gamma = seq(.1, 2, .1), #gamma values
              delta = c(.1, .5, 1, 2, 5, 10)) %>% #delta values
  mutate(wp = (delta*(p^gamma))/((delta*p^gamma)+(1-p)^gamma)) #images of wf

#labeller function for facet labels with LateX math expressions 
label_delta <- function(string) {
  TeX(paste("$\\delta=$", string, sep = ""))  
}

#plot shapes of weighting function
wf %>% 
  ggplot(aes(p, wp, group = gamma)) +
  facet_wrap(~delta, labeller = as_labeller(label_delta, default = label_parsed)) + 
  scale_x_continuous(breaks = seq(0, 1, .2)) +
  scale_y_continuous(breaks = seq(0, 1, .2)) + 
  labs(x = expression(p),
       y = expression(w(p)), 
       color = expression(gamma)) + 
  theme_apa() + 
  geom_line(aes(color = gamma)) +
  scale_color_viridis(option = "plasma") + 
  geom_abline(intercept = 0, slope = 1, color = "gray", linetype = "dashed")
```

In some more detail, the gray-dashed identity lines in Figure\ \@ref(fig:weighting-function) imply a linear weighting pattern.
That is, since the images of $w$, $w(p)$, are equal to the respective cumulative probabilities $p$, all cumulative decision weights $\pi$ that are derived according to (12) would be equal to the differences between the respective objective cumulative probabilities and therefore also to the objective probabilities of the outcomes for which the weights are determined.
Such a linear weighting pattern is implied by $\gamma = 1$ and $\delta = 1$. 
Now, both deviations from $\gamma = 1$ and $\delta = 1$ produce a nonlinear graphical shape.
That is, for $\gamma > 1$, the graphs take a S-shape---running below (above) the identity line for small (large) cumulative probabilities---which is more accentuated for larger deviations from $\gamma = 1$.
Importantly, the same *small* numerical difference between two cumulative probabilities $p$ then translates to varying differences between the respective images $w(p)$ depending on the interval on the cumulative probability scale. 
Specifically, because of the S-shaped curvature, the differences between the images $w(p)$ may be smaller than those between the respective cumulative probabilities in the lower and upper part of the cumulative probability scale, but may be considerably greater in the middle part.
As a consequence, the cumulative decisions weights are smaller than the respective objective probabilities for small-probability outcomes of either a low rank or a high rank, but they are larger for small-probability outcomes of a middle rank.^[<!--Open footnote-->Note that the distinction between outcomes of either low/high rank or middle rank is unnecessary for two-outcome prospects [see @tverskyAdvancesProspectTheory1992]. However, for finite many outcomes, the distinction is critical. More so, the fact that CPT may weigh outcomes of the same small probability differently, adds another reasons why the description-experience gap should not be equated with a reversed weighting pattern in CPT [see @hertwigConstructBehaviorGap2018]. I.e., neither could the gap then be interpreted in terms of an as-if underweighting (overweighting) of rare outcomes in DfE (DfD), nor would any of the potential contributors to the gap considered so far [see, e.g., @hertwigDescriptionexperienceGapRisky2009; @wulffMetaanalyticReviewTwo2018] treat some small-probability outcomes differently than other small-probability outcomes.<!--Close footnote-->]
Note that for all $\gamma < 1$ the graph of $w$ takes an inverse S-shape and the entire weighting pattern is reversed.
Moreover, the shape of the weighting function, and thus the weighting pattern, depends strongly on $\delta$, which affects the overall elevation of the graph.
That is, for $\delta < 1$, the interval on the cumulative probability scale over which the weighting function runs below the identity line is greater than the interval over which the weighting function runs above the identity line; and this pattern reverses for $\delta > 1$.
More specifically, $\delta$ shifts across the entire cumulative probability scale the intervals within which small differences between objective cumulative probabilities translate to either smaller or greater cumulative decision weights.
In other words then, for any decrease (increase) in $\delta$, the underweighting of small probability outcomes extends to more outcomes at the lower (higher) end of the outcome range.

To summarize the preceding paragraphs, CPT is data model that can be used to describe choices between prospects by capturing systematic deviations from the principle of mathematical expectation in the parameter estimates of its value and weighting function.
Importantly, each estimated weighting function implies a weighting pattern.
Although the possible weighting patterns depend on the exact parameterization of the weighting function [e.g., @goldsteinExpressionTheoryPreference1987], the latter's graph usually takes a more or less accentuated (inverse) S-shape, which implies a weighting pattern where small-probability outcomes of low and high rank are underweighted (overweighted) in CPT's overall evaluation of a prospect according to (10).
Notably, for the case of choices between at most two-outcome prospects, the weighting pattern reduces to either an underweighting or overweighting of rare outcomes.
For the description of such choices, one may then adopt the as-if weighting terminology from above, by stating that people decide as if they were either underweighting or overweighting rare outcomes.
<!-- Add paragraph about CPT analysis of DfE-->

To be clear, however, the as-if prefix indicates that it cannot be concluded from the estimated weighting pattern that the mind does indeed perform any of the computations associated with the weighting of subjective values in CPT; rather, the mind processes the information about the choice alternatives in a way that the resulting choices translate to the estimated CPT model.
The as-if weighting terminology is therefore reminiscent of the distinction between different levels of explanation [@marrVisionComputationalInvestigation1982].
That is, CPT may be considered a computational-level theory that specifies the computational problem the mind faces during decisions between prospects, and how this problem may be solved by the abstract calculus reviewed above.
However, as such, CPT cannot substitute algorithmic-level theories that make claims about how exactly the mind transforms the inputs to the decision making process into outputs which approximate the solutions of CPT's calculus [see @zilkerMeasuringModelingConstruction2020; see also @griffithsProbabilisticModelsCognition2010].
Therefore, this paper uses CPT merely as a data model and continues to inquire how on the algorithmic level the mind may integrate sampling strategies into the information-processing chain upfront choices in DfE.
More specifically, the following two sections discuss and demonstrate how different strategies of generating outcomes and integrating them into samples produce distinct choice patterns in DfE, and how these choice patterns translate to the weighting pattern in CPT.^[<!--Open footnote-->CPT's value function will also be considered but emphasis is put on the weighting function.<!--Close footnote-->]

# Sampling Strategies in DfE

The need to distinguish between the computational and the algorithmic level in the domain of decision making is exemplified by the fact that information about prospects may come in the form of descriptions or sampled outcomes.
Whereas CPT can be used to model choices in DfD and DfE, the actual information-processing chains that the mind may execute upfront choices in DfE cannot be identical to those in DfD.
That is, inputs in the form of sampled outcomes require different algorithms to be processed than do descriptions of outcomes and their probabilities.
More specifically, the core difference between DfD and DfE is that in the latter case, the properties must be inferred. 
However, as has been outlined in the introduction, for such an inference rather many sampled outcomes must be integrated. 
 
However, only rarely in peoples' daily life, inputs take the form of explicit descriptions of all outcome-probability pairs.
In such cases, people would make DfD, since the information from which the mind learns about the properties of prospects are complete descriptions thereof.
Yet, rather often, people make DfE, where the mind can learn about the latent properties of prospects only by experiental sampling over time [@hertwigDescriptionexperienceGapRisky2009, p. 517].
That is, for the choice between $1, ..., k$ prospects, the inputs to DfE are generated by a set of prospect-specific stochastic processes
$$
\{\{X_{1t}\}, ... , \{X_{kt}\}\}
\; ,
\tag{6}
$$
where each $\{X_{t}\}_{t \in \mathbb N}$ is a collection of independent and identically distributed (i.i.d.) random variables.
Then, in other words, each stochastic process produces a sequence sampled outcomes. 
Now, one core assumption of this paper is that, in order to infer the latent properties of a prospect, the mind sequentially integrates the sampled outcomes into either one sample or into multiple smaller samples.
Assuming that the mind uses all samples so obtained during a process of evidence accumulation, this paper discusses and demonstrates how different sampling strategies that alter the number and size of these samples can produce distinct choice patterns in DfE.
Before so doing, however, the following section reviews how CPT can be used as a data model that captures systematic deviations from the principle of mathematical expectation in the parameter estimates of a *value* and a *weighting function*. 

## The model 

To model the information-processing chain in DfE, the current paper integrates two approaches that have been proposed to link the sampling process and the decision-making process. 
Specifically,  

One such approach is captured by sequential sampling models, which approach the information-processing chain as an evidence accumulation process. 
One core assumption of these models applied to choices between prospects [see, e.g., @bhatiaSequentialSamplingParadoxes2014; @busemeyerDecisionFieldTheory1993; @markantModelingChoiceSearch2015; @ratcliffComparisonSequentialSampling2004] is that the sampled outcomes are used to sequentially accumulate evidence for or against the respective prospect as the sequences in (8) grow.
More specifically, sequential sampling models assume that the values of the sampled outcomes are sequentially integrated into one or more dynamic decision variables.
Once the value of a decision variable reaches a threshold in favor of a prospect, a respective choice is made. 
 
In contrast, @hillsInformationSearchDecisions2010, proposed another potential link between the sampling and the decision process in DfE.
Specifically, the authors suppose that the sampling pattern may be linked to the choice strategy that people apply.
That is, if single samples originating from different prospects are generated in direct succession (piecewise sampling), the evaluation of prospects is based on multiple ordinal comparisons of single samples (round-wise decisions).
In contrast, if single samples originating from the same prospect are generated in direct succession (comprehensive sampling), it is supposed that the evaluation of prospects is based on a single ordinal comparison of long sequences of single samples (summary decisions).
Indeed, they found some evidence that people who switch more often, make more choices in line with a piecewise strategy and people who switch rarely make more choices in line with a comprehensive strategy [but see @wulffHowShortLongrun2015].
However, in this approach no causal claims are made.

Therefore, model assumes that evidence is accumulated over a sequence of mean-based comparisons between the samples.
The results of each comparison is defined as a random variable
$$
\begin{aligned}
  Z : \mathbb{R} &\to \mathbb{N} \\
  (\overline{X}, \overline{Y}) &\mapsto 
  \begin{cases}
     1 &\text{if} \quad \overline{X} > \overline{Y}, \\
     -1 &\text{if} \quad \overline{X} < \overline{Y}, \\
     0 &\text{else}
     \; . 
  \end{cases}
\end{aligned}
\tag{}
$$
The evidence accumulation process can therefore be defined as a random walk 
$$
D_n = \sum_t^n Z_t \quad n \in \mathbb N
$$

# Simulation Study

## Choice Problems

The test set of $60$ choice problems, each consisting of a safe prospect and a two-outcome prospect, was obtained by stratified sampling from an initial set of $10,000$ choice problems.
The stratification was used to assure that some of the choice problems contain a small (high) outcome of a probability smaller than $.20$.
The exact procedure was as follows:
For each of the $10.000$ problems in the initial set, three outcomes were randomly drawn from a uniform distribution over the interval $[0, 20]$, with the smallest and highest of these three outcomes being assigned to the risky prospect in order to omit the case of dominant prospects.
The probability for the smaller outcome of the risky prospect, $p_{small}$, was drawn from a uniform distribution over the interval $[.01, .99]$, with the probability of the higher outcome being set to $p_{high} = 1-p_{small}$.
To obtain the $60$ choice problems of the test set, the initial set was divided into three subsets, where each subset contained either all problems with $p_{high} \in (0,.2)$ or $p_{high} \in [.2,.8]$ or $p_{high} \in (.8,1)$.
From each of these subset, then, $20$ choice problems were randomly sampled.

## Data Generation

The evidence accumulation model presented in this paper was implemented as a computational model to simulate the sampling process and the final choice for each of the $60$ choice problems in the test set.
All of the following parameter values were combined with each other:

* **Switching probability**: 
The switching probability $s$, defined as the probability of sampling outcomes from different prospects in direct succession, was varied in the interval $[.1, 1]$ in increments of $.1$.

* **Decision variable**: The decision variable of a prospect either counts the absolute number of won comparisons or how much more comparisons were won relative to the other prospect.

* **Threshold**: 
The value of the threshold $\theta$, determining how much (more) comparisons a prospect must win, was varied in the interval $[1,5]$ in increments of 1. 

\noindent
In sum, 100 (parameter combinations) x 60 (choices) x 100 (agents) = 600,000 sampling processes and choices were simulated. 

## Results

```{r data, include=FALSE}
#load choice data
cols_choices <- list(.default = col_double(),
                     boundary = col_factor(),
                     gamble = col_factor(),
                     rare = col_factor(),
                     agent = col_factor(),
                     choice = col_factor())
choices <- read_csv("data/choices/choices.csv", col_types = cols_choices)

#load CPT estimates
cols_cpt <- list(.default = col_double(),
                 boundary = col_factor(),
                 a = col_factor(),
                 parameter = col_factor())
cpt <- read_csv("data/estimates/estimates_cpt.csv", col_types = cols_cpt)
```

<!-- Plots choice rates -->

(ref:choice-rates) Effect of Switching Probability on Rates of False Risky and False Safe Choices for 2-Outcome Prospects with High Rank Outcomes of Different Probability

```{r choice-rates, fig.cap="(ref:choice-rates)", fig.height=7, fig.width=10}
#prepare data

##determine normative choice according to latent and sampled EV
fr_rates <- choices %>%
  mutate(norm = case_when(ev_ratio > 1 ~ "A", ev_ratio < 1 ~ "B"), #normative choice latent EV
         ev_ratio_exp = round(a_ev_exp/b_ev_exp, 2),
         norm_exp = case_when(ev_ratio_exp > 1 ~ "A", ev_ratio_exp < 1 ~ "B")) #normative choice sampled EV

##compute false response rates  

###latent EV
fr_rates_l <- fr_rates %>%
  filter(!is.na(norm)) %>% #exclude trials with equal EV
  group_by(s, boundary, a, rare, norm, choice) %>% #separate by model parameter and type of rare event
  summarise(n = n()) %>%
  mutate(rate = round(n/sum(n), 2), #compute response rates
         type = case_when(norm == "A" & choice == "B" ~ "Safe", #determine false response type
                          norm == "B" & choice == "A" ~ "Risky")) %>%
  ungroup() %>%
  filter(!is.na(type)) %>% #remove correct responses
  select(-c(norm, choice, n)) %>%
  mutate(norm_ev = "Latent") #assign facet label

###sampled EV
fr_rates_s <- fr_rates %>%
  filter(!is.na(norm_exp)) %>%
  group_by(s, boundary, a, rare, norm_exp, choice) %>%
  summarise(n = n()) %>%
  mutate(rate = round(n/sum(n), 2),
         type = case_when(norm_exp == "A" & choice == "B" ~ "Safe",
                          norm_exp == "B" & choice == "A" ~ "Risky")) %>%
  ungroup() %>%
  filter(!is.na(type)) %>%
  select(-c(norm_exp, choice, n)) %>%
  mutate(norm_ev = "Sample")

### merge latent and sampled data
fr_rates <- bind_rows(fr_rates_l, fr_rates_s) %>%
  mutate(rare = case_when(rare == "none" ~ "\\in \\[.2,.8\\]", #change facet labels for rare events
                          rare == "attractive" ~ "\\in (0,.2)",
                          rare == "unattractive" ~ "\\in (.8,1)"))

# labeller functions
label_theta <- function(string) {
  TeX(paste("$\\theta=$", string, sep = "")) #threshold parameter theta
}

label_rare <- function(string) {
  TeX(paste("$\\p_{High}$", string, sep = "")) #type of rare event
}

#plot
fr_rates %>%
  ggplot(aes(s, rate)) +
  facet_grid(rare~a, labeller = labeller(rare = as_labeller(label_rare, default = label_parsed), 
                                         a = as_labeller(label_theta, default = label_parsed))) +
  labs(x = "Switching Probability",
       y = "False Response Rate",
       color = "Normative\nExpected\nValue",
       shape = "False\nResponse\nType") +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, .5)) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, .5)) +
  geom_point(aes(color = norm_ev, shape = type)) +
  scale_shape_manual(values = c(4, 16)) +
  scale_color_viridis_d(option = "plasma", end = .7) +
  theme_apa()
```

<!-- Plots CPT -->

(ref:weighting-parameters) Parameter Estimates of the Weighting Function of @goldsteinExpressionTheoryPreference1987 for Different Simulation Parameters

```{r weighting-parameters, fig.cap="(ref:weighting-parameters)", fig.height=7, fig.width=10}
#prepare data
cpt <- cpt %>% mutate(boundary = if_else(boundary == "absolute", "Absolute", "Relative"))

#plot estimates

##gamma
gamma <- cpt %>%
  filter(parameter == "gamma") %>%
  ggplot(aes(s, mean, color = s, shape = boundary)) +
  facet_wrap(~a, nrow = 1, labeller = labeller(a = as_labeller(label_theta, default = label_parsed))) +
  labs(x = element_blank(), #omit axis title
       y = expression(gamma),
       color = "Switching\nProbability",
       shape = "Boundary") +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0,1,.5)) +
  geom_point() +
  geom_pointrange(aes(ymin=`2.5%`, ymax=`97.5%`)) + 
  scale_shape_manual(values = c(4, 16)) +
  scale_color_viridis(option = "plasma") +
  theme_apa() 

##delta
delta <- cpt %>%
  filter(parameter == "delta") %>% 
  ggplot(aes(s, mean, color = s, shape = boundary)) +
  facet_wrap(~a, nrow = 1) + 
  labs(x = "Switching Probability",
       y = expression(delta),
       color = "Switching\nProbability",
       shape = "Boundary") +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0,1,.5)) +
  geom_point() +
  geom_pointrange(aes(ymin=`2.5%`, ymax=`97.5%`)) + 
  scale_color_viridis(option = "plasma") +
  scale_shape_manual(values = c(4, 16)) +
  theme_apa() + 
  theme(strip.text.x = element_blank()) #omit facet labels

##merge figures
ggarrange(gamma, delta, ncol = 1, nrow = 2, common.legend = TRUE, legend = "right", labels = "AUTO") 
```

(ref:value-parameters) Parameter Estimates of the Value Function of @tverskyAdvancesProspectTheory1992 and of the Logit Choice Rule for Different Simulation Parameters

```{r value-parameters, fig.cap="(ref:value-parameters)", fig.height=7, fig.width=10}
#plot estimates

##alpha
alpha <- cpt %>%
  filter(parameter == "alpha") %>% 
  ggplot(aes(s, mean, color = s, shape = boundary)) +
  facet_wrap(~a, nrow = 1, labeller = labeller(a = as_labeller(label_theta, default = label_parsed))) +
  labs(x = element_blank(),
       y = expression(alpha),
       color = "Switching\nProbability",
       shape = "Boundary") +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0,1,.5)) +
  geom_point() +
  geom_pointrange(aes(ymin=`2.5%`, ymax=`97.5%`)) + 
  scale_color_viridis(option = "plasma") +
  scale_shape_manual(values = c(4, 16)) +
  theme_apa()

##rho
rho <- cpt %>%
  filter(parameter == "rho") %>% 
  ggplot(aes(s, mean, color = s, shape = boundary)) +
  facet_wrap(~a, nrow = 1) + 
  labs(x = "Switching Probability",
       y = expression(rho),
       color = "Switching\nProbability",
       shape = "Boundary") +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1,.5)) +
  geom_point() +
  geom_pointrange(aes(ymin=`2.5%`, ymax=`97.5%`)) + 
  scale_color_viridis(option = "plasma") +
  scale_shape_manual(values = c(4, 16)) +
  theme_apa() + 
  theme(strip.text.x = element_blank())

##merge figure
ggarrange(alpha, rho, ncol = 1, nrow = 2, common.legend = TRUE, legend = "right", labels = "AUTO") 
```

(ref:cpt-graphs) Estimated Graphical Shapes of the Weighting and Value Function for Different Simulation Parameters

```{r cpt-graphs, fig.cap="(ref:cpt-graphs)", fig.height=7, fig.width=10}
#prepare data

##compute images of weighting and value function

###weighting function
wf <- cpt %>%
  select(s, boundary, a, parameter, mean) %>%
  pivot_wider(names_from = parameter, values_from = mean) %>%
  select(-c(alpha, rho)) %>%
  expand_grid(p = seq(0, 1, .05)) %>%
  mutate(w = round(  (delta * p^gamma)/ ((delta * p^gamma)+(1-p)^gamma), 2))

###value function
vf <- cpt %>%
  select(s, boundary, a, parameter, mean) %>%
  pivot_wider(names_from = parameter, values_from = mean) %>%
  select(-c(gamma, delta, rho)) %>%
  expand_grid(x = seq(0, 20, .5)) %>%  
  mutate(v = round(x^alpha, 2)) 

##split data to avoid passing two y-values to the same x-value in geom_line()
wf_a <- wf %>% filter(boundary == "Absolute")
wf_r <- wf %>% filter(boundary == "Relative")
vf_a <- vf %>% filter(boundary == "Absolute")
vf_r <- vf %>% filter(boundary == "Relative")

#plots 

##weighting function
weight <- wf_a %>% #graphs for absolute boundary
  ggplot(aes(p, w, group = s, color = s)) +
  facet_wrap(~a, nrow = 1, labeller = labeller(a = as_labeller(label_theta, default = label_parsed))) + 
  labs(x = "Experienced Probability",
       y = "Decision Weight",
       color = "Switching\nProbability") +
  scale_x_continuous(breaks = seq(0, 1, .5)) +
  scale_y_continuous(breaks = seq(0, 1, .5)) +
  geom_line(size = .5) +
  geom_line(data = wf_r, size = .5) +  #add curves for relative boundary
  scale_color_viridis(option = "plasma") +
  theme_apa()

##value function
value <- vf_a %>% 
  ggplot(aes(x, v, group = s, color = s)) +
  facet_wrap(~a, nrow = 1)+ 
  labs(x = "Objective Outcome",
       y = "Subjective Value",
       color="Switching\nProbability") +
  scale_x_continuous(breaks = seq(0, 20, 10)) +
  scale_y_continuous(breaks = seq(0, 20, 10)) +
  geom_line(size = .5) +
  geom_line(data = vf_r, size = .5) + 
  scale_color_viridis(option = "plasma") +
  theme_apa() + 
  theme(strip.text.x = element_blank())

##merge figures
ggarrange(weight, value, ncol = 1, nrow = 2, common.legend = TRUE, legend = "right", labels = "AUTO") 
```

For each distinct parameter combination, we ran 20 chains of 40,000 iterations each, after a warm-up period of 1000 samples.
To reduce potential autocorrelation during the sampling process, we only kept every 20th sample (thinning).
The potential scale reduction factor $\hat{R}$ [@gelmanInferenceIterativeSimulation1992] was $\leq$ `r round(max(cpt$Rhat), 3)` for all parameters, indicating good convergence.
The minimum effective sample size was `r min(cpt$n.eff)`.

As expected, the scatter plot below shows an inverse relationship between switching probability and trial length.
I.e., the lower the switching probability, the larger become the sample sequences on which each comparison between prospects is based, which in turn leads to longer trials.  
This effect is particularly pronounced for low probabilities such that the increase in trial length accelerates as switching probability decreases.


The figures below display the estimates of the $\gamma$ and $\delta$ parameter of the probability weighting function [@prelecProbabilityWeightingFunction1998] fitted to DfE simulated for different parameter values of the generating model.
The estimates are plotted against the switching probability, where each panel represents a distinct boundary type and level, i.e., number of comparisons. 
Grey dots represent agent level estimates, colored dots represent the mean across all agent level estimates. 

Most significantly, there is a strong relationship between the switching probability in the generating model and the $\gamma$ parameter.
I.e., large switching probabilities, which are indicative for small sample sequences, lead to larger estimates for $\gamma$. 
The resulting strong curvature leads to a compression of probabilities in the lower and upper range, reflecting underweighting of small probabilities and overweighting of large probabilities. 
This pattern is robust for varying degrees of the boundary level, except for $a = 1$. 
The latter deviation may be explained by the potential scale reduction factors for the respective parameter estimates, which indicate that the MCMC chains did not converge (see [Appendix 3][Appendix 3: Convergence Diagnostics for CPT Parameters]).   

<!-- Plots Appendix -->

```{r A-data, include=FALSE}
#load choice data
choices_c <- read_csv("data/choices/choices_comprehensive.csv", col_types = cols_choices)
cpt_c <- read_csv("data/estimates/estimates_cpt_comprehensive.csv", col_types = cols_cpt)
```

```{r A-choice-rates, fig.height=7, fig.width=10, include=FALSE}
#prepare data
fr_rates_c <- choices_c %>%
  mutate(norm = case_when(ev_ratio > 1 ~ "A", ev_ratio < 1 ~ "B"),
         ev_ratio_exp = round(a_ev_exp/b_ev_exp, 2),
         norm_exp = case_when(ev_ratio_exp > 1 ~ "A", ev_ratio_exp < 1 ~ "B"))

fr_rates_c_l <- fr_rates_c %>%
  filter(!is.na(norm)) %>%
  group_by(s, boundary, a, rare, norm, choice) %>% 
  summarise(n = n()) %>%
  mutate(rate = round(n/sum(n), 2),
         type = case_when(norm == "A" & choice == "B" ~ "Safe",
                          norm == "B" & choice == "A" ~ "Risky")) %>%
  ungroup() %>%
  filter(!is.na(type)) %>%
  select(-c(norm, choice, n)) %>%
  mutate(norm_ev = "Latent")

fr_rates_c_s <- fr_rates_c %>%
  filter(!is.na(norm_exp)) %>%
  group_by(s, boundary, a, rare, norm_exp, choice) %>%
  summarise(n = n()) %>%
  mutate(rate = round(n/sum(n), 2),
         type = case_when(norm_exp == "A" & choice == "B" ~ "Safe",
                          norm_exp == "B" & choice == "A" ~ "Risky")) %>%
  ungroup() %>%
  filter(!is.na(type)) %>%
  select(-c(norm_exp, choice, n)) %>%
  mutate(norm_ev = "Sample")

fr_rates_c <- bind_rows(fr_rates_c_l, fr_rates_c_s) %>%
  mutate(rare = case_when(rare == "none" ~ "\\in \\[.2,.8\\]",
                          rare == "attractive" ~ "\\in (0,.2)",
                          rare == "unattractive" ~ "\\in (.8,1)"))

fr_rates_c %>%
  ggplot(aes(s, rate)) +
  facet_grid(rare~a, labeller = labeller(rare = as_labeller(label_rare, default = label_parsed), 
                                         a = as_labeller(label_theta, default = label_parsed))) +
  labs(x = "Switching Probability",
       y = "False Response Rate",
       color = "Normative\nExpected\nValue",
       shape = "False\nResponse\nType") +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, .5)) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, .5)) +
  geom_point(aes(color = norm_ev, shape = type)) +
  scale_shape_manual(values = c(4, 16)) +
  scale_color_viridis_d(option = "plasma", end = .7) +
  theme_apa()
```

```{r A-weighting-parameters, fig.height=7, fig.width=10, include=FALSE}
#prepare data
cpt_c <- cpt_c %>% mutate(boundary = if_else(boundary == "absolute", "Absolute", "Relative"))

#plot estimates

##gamma
gamma_c <- cpt_c %>%
  filter(parameter == "gamma") %>%
  ggplot(aes(s, mean, color = s, shape = boundary)) +
  facet_wrap(~a, nrow = 1, labeller = labeller(a = as_labeller(label_theta, default = label_parsed))) +
  labs(x = element_blank(),
       y = expression(gamma),
       color = "Switching\nProbability",
       shape = "Boundary") +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0,1,.5)) +
  geom_point() +
  geom_pointrange(aes(ymin=`2.5%`, ymax=`97.5%`)) + 
  scale_shape_manual(values = c(4, 16)) +
  scale_color_viridis(option = "plasma") +
  theme_apa() 

##delta
delta_c <- cpt_c %>%
  filter(parameter == "delta") %>% 
  ggplot(aes(s, mean, color = s, shape = boundary)) +
  facet_wrap(~a, nrow = 1) + 
  labs(x = "Switching Probability",
       y = expression(delta),
       color = "Switching\nProbability",
       shape = "Boundary") +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0,1,.5)) +
  geom_point() +
  geom_pointrange(aes(ymin=`2.5%`, ymax=`97.5%`)) + 
  scale_color_viridis(option = "plasma") +
  scale_shape_manual(values = c(4, 16)) +
  theme_apa() + 
  theme(strip.text.x = element_blank()) #omit facet labels

##merge figures
ggarrange(gamma_c, delta_c, ncol = 1, nrow = 2, common.legend = TRUE, legend = "right", labels = "AUTO") 
```

```{r A-value-parameters, fig.height=7, fig.width=10, include=FALSE}
#plot estimates

##alpha
alpha_c <- cpt_c %>%
  filter(parameter == "alpha") %>% 
  ggplot(aes(s, mean, color = s, shape = boundary)) +
  facet_wrap(~a, nrow = 1, labeller = labeller(a = as_labeller(label_theta, default = label_parsed))) +
  labs(x = element_blank(),
       y = expression(alpha),
       color = "Switching\nProbability",
       shape = "Boundary") +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0,1,.5)) +
  geom_point() +
  geom_pointrange(aes(ymin=`2.5%`, ymax=`97.5%`)) + 
  scale_color_viridis(option = "plasma") +
  scale_shape_manual(values = c(4, 16)) +
  theme_apa()

##rho
rho_c <- cpt_c %>%
  filter(parameter == "rho") %>% 
  ggplot(aes(s, mean, color = s, shape = boundary)) +
  facet_wrap(~a, nrow = 1) + 
  labs(x = "Switching Probability",
       y = expression(rho),
       color = "Switching\nProbability",
       shape = "Boundary") +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1,.5)) +
  geom_point() +
  geom_pointrange(aes(ymin=`2.5%`, ymax=`97.5%`)) + 
  scale_color_viridis(option = "plasma") +
  scale_shape_manual(values = c(4, 16)) +
  theme_apa() + 
  theme(strip.text.x = element_blank())

##merge figure
ggarrange(alpha_c, rho_c, ncol = 1, nrow = 2, common.legend = TRUE, legend = "right", labels = "AUTO") 
```

```{r A-cpt-graphs, fig.height=7, fig.width=10, include=FALSE}
#prepare data

##compute images of weighting and value function

###weighting function
wf_c <- cpt_c %>%
  select(s, boundary, a, parameter, mean) %>%
  pivot_wider(names_from = parameter, values_from = mean) %>%
  select(-c(alpha, rho)) %>%
  expand_grid(p = seq(0, 1, .05)) %>%
  mutate(w = round(  (delta * p^gamma)/ ((delta * p^gamma)+(1-p)^gamma), 2))

###value function
vf_c <- cpt_c %>%
  select(s, boundary, a, parameter, mean) %>%
  pivot_wider(names_from = parameter, values_from = mean) %>%
  select(-c(gamma, delta, rho)) %>%
  expand_grid(x = seq(0, 20, .5)) %>%  
  mutate(v = round(x^alpha, 2)) 

##split data to avoid passing two y-values to the same x-value in geom_line()
wf_c_a <- wf_c %>% filter(boundary == "Absolute")
wf_c_r <- wf_c %>% filter(boundary == "Relative")
vf_c_a <- vf_c %>% filter(boundary == "Absolute")
vf_c_r <- vf_c %>% filter(boundary == "Relative")

#plots 

##weighting function
weight_c <- wf_c_a %>% 
  ggplot(aes(p, w, group = s, color = s)) +
  facet_wrap(~a, nrow = 1, labeller = labeller(a = as_labeller(label_theta, default = label_parsed))) + 
  labs(x = "Experienced Probability",
       y = "Decision Weight",
       color = "Switching\nProbability") +
  scale_x_continuous(breaks = seq(0, 1, .5)) +
  scale_y_continuous(breaks = seq(0, 1, .5)) +
  geom_line(size = .5) +
  geom_line(data = wf_c_r, size = .5) + 
  scale_color_viridis(option = "plasma") +
  theme_apa()

##value function
value_c <- vf_c_a %>% 
  ggplot(aes(x, v, group = s, color = s)) +
  facet_wrap(~a, nrow = 1)+ 
  labs(x = "Objective Outcome",
       y = "Subjective Value",
       color="Switching\nProbability") +
  scale_x_continuous(breaks = seq(0, 20, 10)) +
  scale_y_continuous(breaks = seq(0, 20, 10)) +
  geom_line(size = .5) +
  geom_line(data = vf_c_r, size = .5) + 
  scale_color_viridis(option = "plasma") +
  theme_apa() + 
  theme(strip.text.x = element_blank())

##merge figures
ggarrange(weight_c, value_c, ncol = 1, nrow = 2, common.legend = TRUE, legend = "right", labels = "AUTO") 
```

# Summary and Conclusion

\newpage

# References

```{r echo = FALSE, results = 'asis', cache = FALSE}
papaja::render_appendix('manuscript_appendix.Rmd')
```


```{r echo = FALSE, results = 'asis', cache = FALSE}
papaja::render_appendix('manuscript_appendix.Rmd')
```


```{r echo = FALSE, results = 'asis', cache = FALSE}
papaja::render_appendix('manuscript_appendix.Rmd')
```

