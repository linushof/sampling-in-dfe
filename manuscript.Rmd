---
title             : "Sampling Strategies in Decisions from Experience: Property Representation and Probability Weighting Approximation"
shorttitle        : "Sampling strategies"

author: 
  - name          : "Linus Hof"
    affiliation   : "1"
    corresponding : yes    
    address       : "Department of Psychology, Heidelberg University, Hauptstra√üe 47-51, 69117 Heidelberg, Germany"
    email         : "linushof@posteo.de"
    
affiliation:
  - id            : "1"
    institution   : "Heidelberg University"

note: | 
  **Unsubmitted draft created from the commit with the hash**: ``r repro::current_hash()``
  
authornote: |
 This is a dynamic document which can be reproduced from the accompanying GitHub repository: https://github.com/linushof/sampling-in-dfe. 
 
 The current version of this manuscript is prepared for submission as a master's thesis.  
 
abstract: |
  Add short abstract.
  
keywords          : ""
wordcount         : ""

bibliography      : ["references_sampling-in-dfe.bib"]
csl               : apa.csl

figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
linkcolor         : "blue"
mask              : no
draft             : no

classoption       : "man"
documentclass     : "apa7"
output            : papaja::apa6_pdf

header-includes:
   - \usepackage[capposition=top]{floatrow}
---

```{r}
# load packages
pacman::p_load(tidyverse, 
               knitr, 
               ggbeeswarm,
               scico, # for scientific color palettes
               papaja)

knitr::opts_chunk$set(fig.pos = "tb")
```

<!-- Introduction -->

The mind is a cognitive system that operates on inputs from an environment or memory.
In the domain of decision making, such inputs can be organized in a set $I$ of choice alternatives $i \in I$, the latter being distinguishable from each other on the basis of the *properties* they possess.
The basic structure of decision making may then be modeled by the inclusion map

$$
\begin{aligned}
  & \iota : I' \mapsto I \\
  & \iota(i) = i \ \text{for all} \ i \in I' 
\end{aligned}
\tag{1}
$$

[cf. @jostMathematicalConcepts2015, pp. 14-15], indicating that decision making is a *process* that starts from an input set $I$, including all choice alternatives, and stops at a proper subset $I' \subset I$, including only the chosen alternatives. 
Given that the choice is perfectly systematic---as opposed to random or noisy---the chosen alternatives in $I'$ should share some property or combination of several properties which the other alternatives in the complementary subset $I \setminus I'$ do not possess.
In the following, I will refer to statements about properties that serve to distinguish between the chosen and non-chosen alternatives as *selection criteria*. 

Often, however, the properties used by the mind to make a choice are not readily described and cannot be directly evaluated [cf. @brunswikRepresentativeDesignProbabilistic1955; see @brunswikPerceptionRepresentativeDesign1956, for a detailed treatment]; rather, proper descriptions must be inferred from whatever information the inputs provide about the properties of choice alternatives, using transformation rules, statistical generalizations, or both.
Since how the inputs are represented determines the information that is made explicit and the costs at which certain operations can be carried out on that information, the feasibility and ease of these inferences depends greatly on the choice of *representation* [@marrVisionComputationalInvestigation1982; see also, e.g., @gigerenzerHowImproveBayesian1995; @griffithsProbabilisticModelsCognition2010; @kempStructuredStatisticalModels2009].
Hence, to explain how the mind makes a selection among choice alternatives, one needs to understand the output controlling properties, i.e., the applied selection criteria, the representation that is used to describe these properties, and the costs of inferring the descriptions in that representation, including their susceptibility to distortions from uncertainty or noise. The process is schematically shown in Figure 1.  

(ref:figure1) A rough computational level sketch of decision making as an information processing problem. To make a selection among choice alternatives on the basis of particular selection criteria, the mind requires explicit descriptions of the relevant properties before they can be evaluated. Descriptions are either readily available or must be inferred from the inputs.

```{r fig.cap="(ref:figure1)", fig.align='left'}
knitr::include_graphics("manuscript_files/figure-png/figure1.png")
```

In what follows, this paper will elaborate on the concepts touched upon thus far, certainly only a subset of the factors involved in the decision making process, yet central enough to illustrate a rather important principle:
Given that the mind is indeed sensitive to variations in the property values, the more accurately their descriptions are inferred, the more systematic the choices should become.
Importantly, this principle is not restricted to any one specific property. 
Rather, each selection criterion or combination thereof imposes constraints on the assessment of choice alternatives, leading to characteristic choice patterns that are eventually blurred by inaccurate property descriptions.

For the sake of example, I argue that a choice pattern robustly emerging in *decisions from experience* (DfE), the apparent underweighting of small probability events [e.g., @barronSmallFeedbackbasedDecisions2003; @hertwigDecisionsExperienceEffect2004; @weberPredictingRiskSensitivity2004; see also @wulffMetaanalyticReviewTwo2018, for a meta-analytic review], may be caused by the assessment constraints associated with a round-wise decision strategy.
This particular claim has already been made by @hillsInformationSearchDecisions2010 and follows quite directly from probability theory.
However, the current paper suggests to subordinate decision strategies to the adoption of selection criteria and addresses in some detail two "problems" in the context of DfE, in which descriptions of the properties of choice alternatives must be inferred from sampling data.

For one, there is an *induction problem*, well known from inferential statistics, according to which the sample size determines the margin of error within which any true property of a choice alternative can be inferred, given an otherwise perfect inference process. 
Yet, it follows that if an underweighting pattern is caused by the adoption of an arbitrary selection criterion, it is *a priori* predicted to become more stable with increasing sample size. 
This prediction is in line with the principle already stated above, i.e., that selection-criteria-induced choice patterns should become more systematic as property descriptions become more accurate.
Nevertheless, the prediction contrasts the explanation that underweighting is caused by the reliance on small samples [e.g., @foxDecisionsExperienceSampling2006; @hertwigDecisionsExperienceEffect2004; @hauDescriptionexperienceGapRisky2008; @ungemachAreProbabilitiesOverweighted2009; @rakowBiasedSamplesNot2008], highlighting that outside of the *statistical* models used to describe the choice data, *causal* decision making models of quite different nature may be at play, and that a rather important difference between these causal models resides in the selection criteria they incorporate.
I will elaborate on this matter as I proceed, however, to be clear from the outset, what the mind does in neither case is performing a probability weighting computation in its literal sense, but it processes the inputs in a way that the outputs approximate the solutions of what can be considered probability weighting on an abstract computational level [cf., e.g., @griffithsRationalUseCognitive2015]. 

For another, this paper approaches the problem of organizing the sampling data in such a way that it allows chains of inferences connecting the input set $I$ with the output set $I' \subset I$ that are algorithmically short and simple enough to be carried out by minds whose cognitive capacities are bounded [cf. @simonInvariantsHumanBehavior1990; @simonBehavioralModelRational1955]. 
This may be considered a *representation problem* since the inference chains are constructed around explicit descriptions of the properties of choice alternatives and representations are the "formal schemes" of symbols and rules with which these descriptions are derived [@marrRepresentationRecognitionSpatial1978, p. 270]. 
Specifically, representations differ in how they organize the information used to infer the descriptions of certain properties and these organizations determine the algorithms that can be built into the inference chain [@marrVisionComputationalInvestigation1982].
In this paper, *sampling strategies* in DfE are considered as representation systems used to organize the samples in a way that subserves the property inferences required by the application of particular selection criteria.
Though most of my expositions on the role of sampling strategies remain speculative, the core idea is not new to decision research [see, e.g., @hillsInformationSearchDecisions2010; @wulffHowShortLongrun2015].
By restating this idea and adopting some terms and concepts that have well defined meaning in math and cognitive science, I hope to integrate it into the rich study of the computationally limited mind as an adaptive cognitive system whose processes interact with the information structures in the environment or memory. 

The theoretical arguments sketched in this introduction are substantiated by a simulation study. 
For this purpose, the sampling and decision making processes are implemented in a computational model to simulate choice data from DfE.
The simulated data is modeled in cumulative prospect theory (CPT), a data model commonly used for describing decisions under risk and uncertainty [@goldsteinExpressionTheoryPreference1987; see also @tverskyAdvancesProspectTheory1992]. 
All simulations and analyses are reproducible---materials and instructions can be found on the GitHub repository.

# Inputs to the Decision Making Process and Property Descriptions 

By defining decision making as an information processing problem whose solution requires a selection among choice alternatives of the form described in (1), much weight is given to the properties of choice alternatives---whatever those may be for the moment---since it is only through them that any necessary distinction between the alternatives or collections thereof can be made. 
To stress this point, consider the power set $\mathcal{P}(I)$, which is the set of all possible subsets of $I$ (see example in Figure 2). 
Each of the subsets $I' \in \mathcal{P}(I)$ corresponds to a possible distinction between the choice alternatives in $I$ that could result from assessing for each alternative whether it possesses an arbitrary property (collection) or not [cf. @jostMathematicalConcepts2015, p. 16]. 

(ref:figure2) A. The power set of the set $I$ including the choice alternatives A, B, and C. Black (white) circles indicate that an alternative does (not) possess a property. Each column displays a possible subset that could result from assessing whether the alternatives posses a relevant property. The properties determine the various relations among the choice alternatives. Note that properties resulting in the empty set $\{\}$ and the subset $\{A,B,C\}$ do not discriminate between choice alternatives. B. A Venn diagram visualizing the inclusion map $\iota : I' \mapsto I$. $\iota$ maps every element $i$ of the subset $I'$ to its image $i$, treated as element of the superset $I$.

```{r fig.cap="(ref:figure2)", fig.align='left'}
knitr::include_graphics("manuscript_files/figure-png/figure2.png")
```

The concept of the power set thereby illustrates how different selection criteria can alter the consequential choice.
That is, in rather abstract terms, selection criteria are statements regarding the properties of choice alternatives that the mind evaluates as either true or false during the decision making process; accordingly, the subset $I'$ only contains the alternatives for which the statement is evaluated to be true.
It follows that if there is some variance in the properties across choice alternatives and the properties are not perfectly correlated, changing the selection criteria can eventually change the subset $I'$.

## What Are the Choice Alternatives and Their Properties? 

The argument so far has been quite general and abstract, since there is no complete answer to the question raised above this paragraph---there is, of course, an uncountable number of decisions and so are there choice alternatives and properties.
Therefore, in the present paper, *prospects* are used as a proxy for the variety of choice alternatives the mind is required to evaluate in natural world decisions. 
Consequently, the argument continues to proceed at an abstract level: with choices between at least two prospects, the current paper considers a case which is counterfactual, since it is freed from many varieties and complexities that the mind is normally confronted with. 
The case is, however, quite paradigmatic in research on decision theory and there are good reasons for that [cf. @knightRiskUncertaintyProfit1921, especially Chapter 1 for a brief discussion closely related to the current subject]. 
Moreover, the case suffices to answer the above question and to subsequently address the induction and representation problem, at least within the stated limitations.

Now, let a prospect be a *probability space* denoted by the tuple $(\Omega, \mathcal{F}, P)$. 
Following the @kolmogorovFoundationsTheoryProbability1950 axioms, $\Omega$ is the sample space 

$$
\Omega = \{\omega_1, ..., \omega_n\}
\; ,
\tag{2.1}
$$ 

containing a finite set of possible outcomes $\omega$. 
$\mathcal{F}$ is the set of all possible subsets of $\Omega$, i.e., the power set $\mathcal{P}(\Omega)$.^[Note that this paper considers only the case of finite sample spaces containing of a small number of outcomes. The exact axiomatic definition of $\mathcal{F}$, which can be found in Kolmogorov [-@kolmogorovFoundationsTheoryProbability1950, pp. 2-3 and 14-15], also covers infinite sample spaces and is that of a $\sigma$-algebra. For the finite case, however, $\mathcal{P}(\Omega)$ can be considered as such. See Georgii [-@georgiiStochastikEinfuhrungWahrscheinlichkeitstheorie2015, Chapter 1.1] for a comprehensive mathematical treatment.] 
$P$ is a probability measure  

$$
P: \mathcal{F} \mapsto [0,1]
\; ,
\tag{2.2}
$$

which assigns each subset $A \in \mathcal{F}$, also referred to as *events*, a probability $0 < p(A) \leq 1$ with $P(\Omega) = 1$.^[The probability measure formally assigns probabilities to sets of outcomes $A \in F$ rather than the elementary outcomes $\omega \in \Omega$. The probabilities of $\omega$ are determined by the probability mass function $p : \Omega \mapsto [0,1]$, which is used to construct the probability measure according to the equation $P(A) = \sum_{\omega \in A} p(\omega)$ for all $A \in \mathcal{F}$ [cf. @georgiiStochastikEinfuhrungWahrscheinlichkeitstheorie2015, Chapter 1.2].]
In less technical terms, following the choice of a prospect, one of the outcomes associated with this particular prospect is obtained, a gain or loss respectively.
If there is just one outcome associated with a prospect, it is obtained with certainty.
However, if there is a collection of outcomes, just one outcome out of this collection is obtained according to some probability, where the probabilities can vary between outcomes and are given by the measure of the form described in (2.2) (see also Footnote 2). 

Since it was stated that the inputs to the decision making process can be organized in a set of choice alternatives, we can adapt (1) for the case of a choice between $1, ..., n$ prospects to obtain: 

$$
\iota : I' \mapsto I, \ \text{where} \ I = \{(\Omega_1,  \mathcal{F}_1, P_1), ..., (\Omega_n,  \mathcal{F}_n, P_n)\}
\; .
\tag{3}
$$

Given (3), it is evident that the prospects can only be distinguished by the elements of their tuples, and these elements are properties---be that a smallest single element, e.g., an outcome, collections thereof, e.g., the sample space $\Omega$, or more complex combinations of elements, e.g., the weighted average which is obtained by first multiplying all outcomes in $\Omega$ by their respective probabilities in $P$ and then summing the resulting products.^[I do not refer to this quantity as expected value, since the term is reserved for random variables, which were not introduced so far.]
In turn, for choices between prospects, selection criteria are statements about the elements of their tuples that are true or false and these statements imply relations among prospects that are at least nominal, e.g., *"One of the prospect's outcome is positive in value"*, but may also draw on other levels of measurement, e.g., *"The prospect's average mean is the largest among the prospects"* refers to an ordinal level.  

## Inferring Property Descriptions from Sampling Data

Using Marr's [-@marrVisionComputationalInvestigation1982] framework, the sensitivity aspect addresses the algorithmic level and refers to the achievement that the true properties of the choice alternatives are accurately described and the resulting relations among the alternatives are properly evaluated [cf. @brunswikOrganismicAchievementEnvironmental1943].
Note that if the descriptions of the relevant properties were available, the process sketched out in Figure 1 would be substantially simplified since the two intermediate steps of representation and property description would be omitted.
For quite many decisions the mind faces, however, it is simply not the case that the properties of interest, e.g., outcome probabilities, are readily described and amenable to direct assessment, as has been repeatedly stated [e.g., @busemeyerChoiceBehaviorSequential1982; @fiedlerBewareSamplesCognitiveecological2000; @hertwigDecisionsExperienceEffect2004; @weberPredictingRiskSensitivity2004; see also @brunswikRepresentativeDesignProbabilistic1955; @brunswikPerceptionRepresentativeDesign1956, for a detailed treatment].
In such cases, to obtain the respective descriptions from the inputs, the mind is required to make an inference from the available information.

In such a choice paradigm, agents are asked to evaluate the prospects and build a preference for either one of them. 
It is common to make a distinction between two variants of this evaluation process [cf. @hertwigDescriptionexperienceGapRisky2009]. 
For decisions from description (DfD), agents are provided a full symbolic description of the prospects.
For decisions from experience [DfE; e.g., @hertwigDecisionsExperienceEffect2004], prospects are not described but must be explored by the means of sampling. 

To provide a formal definition of sampling in risky choice, we make use of the mathematical concept of a random variable and start by referring to a prospect as *"risky"* in the case where $p(\omega) \neq 1$ for all $\omega \in \Omega$.
Here, risky describes the fact that if agents would choose a prospect and any of its outcomes in $\Omega$ must occur, none of these outcomes will occur with certainty. 
It is acceptable to speak of the occurrence of $\omega$ as a realization of a random variable $X$ defined on a prospect iff the following conditions (1) and (2) are met: 

(1) $X$ is a measurable function $$X: (\Omega, \mathcal{F})  \mapsto (\Omega', \mathcal{F'}) \; ,$$ where $\Omega'$ is a set of real numbered values $X$ can take and $\mathcal{F'}$ is a set of subsets of $\Omega'$. I.e., $\Omega$ maps into $\Omega'$ such that correspondingly each subset $A' \in \mathcal{F'}$ has a pre-image $X^{-1}A' \in \mathcal{F}$, which is the set $\{\omega \in \Omega: X(\omega) \in A'\}$ [@kolmogorovFoundationsTheoryProbability1950, p. 21].

(2) The mapping is such that $X(\omega) = x \equiv \omega$. 

In (2), $x \equiv \omega$ means that the realization of a random variable $X(\omega) = x$ is numerically equivalent to its pre-image $\omega$.  
Given conditions (1) and (2), we denote any observation of $\omega$ as a *"single sample"*, or realization, of a random variable defined on a prospect and the act of generating a sequence of single samples in discrete time as *"sequential sampling"*. 
Note that, since random variables defined on the same prospect are independent and identically distributed (iid), the weak law of the large number applies to the relative frequency of occurrence of an outcome $\omega$ in a sequence of single samples originating from the same prospect [cf. @bernoulliArsConjectandiOpus1713].
Thus, long sample sequences in principle allow to obtain the same information about a prospect by sampling as by symbolic description.

Consider now a choice between prospects $1, ..., k$.
To construct a stochastic sampling model for DfE, we assume that agents base their decision on the information related to these prospects and define a decision variable as a function of the latter:

$$
D:= f((\Omega_1, \mathcal{F}_1, P_1), ..., (\Omega_k, \mathcal{F}_k, P_k))
\;.
$$

Now, since in DfE no symbolic descriptions of the prospects are provided, the model must be restricted to the case where decisions are based on sequences of single samples originating from the respective prospects:

$$
D := f(X_{i1}, ..., X_{ik}) 
\; ,
$$

where $i \in \{1, ..., N\}$ denotes a sequence of length $N$ of random variables that are iid.  

Concerning the form of $f$ and the measures it utilizes, it is quite proper to say that they reflect our assumptions about the exact kind of information agents process and the way they do and that these choices should be informed by psychological theory and empirical protocols. 
Taking the case of different sampling and decision strategies previously assumed to play a role in DfE, the following section demonstrates how such assumptions can be explicated in a stochastic model that builds on the sampling approach outlined so far.  

# Evaluation of Choice Alternatives and Outputs of the Decision Making Process

...

# Sampling Strategies as Representation Systems

...

# Simulation Study

...

The switching probability $s$ is the probability with which agents draw a single sample from the prospect they did not get their most recent single sample from.
$s$ is varied between .1 to 1 in increments of .1.
The two boundary parameters resemble the concept of a decision threshold, i.e., if a prospect reaches a boundary, it is chosen by the synthetic agent.
The boundary type is either the minimum number of comparisons any prospect must win (absolute boundary) or the minimum difference between the number of won comparisons (relative boundary).
The boundary value $a$ is varied between 1 to 5 in increments of 1.

## Test set

For each parameter combination of the generating model, 100 synthetic agents are presented with 60 choices problems.
In sum, 100 (parameter combinations) x 100 (agents) x 60 (choices) = 600,000 choices are simulated.
We test a set of 2-prospect choice problems, where one of the prospects contains a safe outcome, i.e., $p(\omega) = 1$ and the other two outcomes where all $p(\omega) \neq 1$.
Both outcomes and probabilities are drawn from uniform distributions, ranging from 0 to 20 for outcomes and from .01 to .99 for probabilities of the smaller outcome of the risky prospect.
To omit dominant prospects within a choice problem, outcomes of the safe prospect always fall between both outcomes of the risky prospect.
Table A1 in [Appendix 2][Appendix 2: Choice Problems] contains the test set of 60 choice problems, which were sampled from an initial set of 10,000.
Sampling of gambles was stratified, randomly drawing an equal number of 20 gambles with no, an attractive, and an unattractive rare outcome.
Risky outcomes are considered *"rare"* if their probability is $p < .2$ and *"attractive"* (*"unattractive"*) if they are higher (lower) than the safe outcome.

## Results

...

```{r}

# read cpt data

cols <- list(.default = col_double(),
             boundary = col_factor(),
             a = col_factor(),
             parameter = col_factor())
cpt_long <- read_csv("data/estimates/estimates_cpt.csv", col_types = cols)

# store convergence diagnostics

gel_92 <- cpt_long %>% select(s, boundary, a, parameter, Rhat, n.eff) 
```

For each distinct parameter combination, we ran 20 chains of 40,000 iterations each, after a warm-up period of 1000 samples.
To reduce potential autocorrelation during the sampling process, we only kept every 20th sample (thinning).
The potential scale reduction factor $\hat{R}$ [@gelmanInferenceIterativeSimulation1992] was $\leq$ `r round(max(gel_92$Rhat), 3)` for all parameters, indicating good convergence.
The minimum effective sample size was `r min(gel_92$n.eff)`.


### Plausibility Check: Relationship between Switching Probability and Trial Length

```{r message=FALSE}

# read choice data 

cols <- list(.default = col_double(),
             boundary = col_factor(),
             gamble = col_factor(),
             rare = col_factor(),
             agent = col_factor(),
             choice = col_factor())
choices <- read_csv("data/choices/choices.csv", col_types = cols)

# get median trial length for each parameter combination

trial_length <- choices %>% 
  group_by(s, boundary, a) %>% 
  summarise(med = median(n_sample))
```

The median length of trials, i.e., the number of single samples drawn in a trial, generated by different parameter combinations ranged from `r min(trial_length$med)` to `r max(trial_length$med)`.
As expected, the scatter plot below shows an inverse relationship between switching probability and trial length.
I.e., the lower the switching probability, the larger become the sample sequences on which each comparison between prospects is based, which in turn leads to longer trials.  
This effect is particularly pronounced for low probabilities such that the increase in trial length accelerates as switching probability decreases.


```{r fig.cap="Test"}

# get median trial length for each switching probability 

trial_length_s <- choices %>% 
  group_by(s) %>%
  summarise(med = median(n_sample))

# plot

trial_length %>%
  ggplot(aes(x = s, y = med)) +
  geom_jitter(color = "#CECECE", size = 3) +
  geom_point(data = trial_length_s, aes(color = s), size = 3) +
  geom_path(data = trial_length_s, aes(color = s)) +
  scale_color_scico(palette = "buda") + 
  scale_x_continuous(breaks = seq(0, 1, .1)) + 
  scale_y_continuous(breaks = seq(0, 120, 10)) +
  labs(title = "Trial Length", 
       x ="Switching Probability",
       y = "Median Trial Length", 
       color="Switching Probability") + 
  theme_apa()
```

### Probability Weighting Function

```{r}

# tidy CPT data: parameters as separate columns 

cpt_wide <- cpt_long %>% 
  select(s, boundary, a, parameter, mean) %>% 
  pivot_wider(names_from = parameter, values_from = mean)
```

The figures below display the estimates of the $\gamma$ and $\delta$ parameter of the probability weighting function [@prelecProbabilityWeightingFunction1998] fitted to DfE simulated for different parameter values of the generating model.
The estimates are plotted against the switching probability, where each panel represents a distinct boundary type and level, i.e., number of comparisons. 
Grey dots represent agent level estimates, colored dots represent the mean across all agent level estimates. 

Most significantly, there is a strong relationship between the switching probability in the generating model and the $\gamma$ parameter.
I.e., large switching probabilities, which are indicative for small sample sequences, lead to larger estimates for $\gamma$. 
The resulting strong curvature leads to a compression of probabilities in the lower and upper range, reflecting underweighting of small probabilities and overweighting of large probabilities. 
This pattern is robust for varying degrees of the boundary level, except for $a = 1$. 
The latter deviation may be explained by the potential scale reduction factors for the respective parameter estimates, which indicate that the MCMC chains did not converge (see [Appendix 3][Appendix 3: Convergence Diagnostics for CPT Parameters]).   

```{r fig.cap="test 2"}

# Density Plot 

cpt_long %>% 
  filter(parameter == "gamma" | parameter == "delta") %>% 
  ggplot(aes(x = s, y = mean)) +
  geom_density_2d_filled(contour_var = "ndensity") + 
  scale_fill_scico_d(palette = "grayC") +
  scale_color_scico_d(palette = "grayC") + 
  scale_x_continuous(breaks = seq(0, 1, .2)) + 
  scale_y_continuous(limits = c(0,2), breaks = seq(0, 2, .2)) +
  labs(x ="Switching Probability",
       y = "Parameter Value",  
       fill="Density") + 
  facet_grid(parameter~a) + 
  theme_apa()
```

```{r eval=FALSE, include=FALSE}

# Scatter Plots 

# Gamma 

cpt_wide %>% 
  ggplot(aes(x = s, y = gamma, color = s)) +
  geom_point(size = 2) +
  geom_line(size = 1) +
  scale_color_scico(palette = "buda") + 
  scale_x_continuous(breaks = seq(0, 1, .2)) + 
  scale_y_continuous(breaks = seq(0, 2, .2)) +
  facet_grid(boundary~a) +
  labs(title = expression(paste("Curvature ", gamma)),
       x ="Switching Probability",
       y = expression(gamma), 
       color="Switching Probability") + 
  theme_minimal()

# Delta

cpt_wide %>% 
  ggplot(aes(x = s, y = delta, color = s)) +
  geom_point(size = 2) +
  geom_line(size = 1) +
  scale_color_scico(palette = "buda") + 
  scale_x_continuous(breaks = seq(0, 1, .2)) + 
  scale_y_continuous(breaks = seq(0, 2, .2)) +
  facet_grid(boundary~a, switch = "y") +
  labs(title = expression(paste("Curvature ", delta)),
       x ="Switching Probability",
       y = expression(delta), 
       color="Switching Probability") + 
  theme_minimal()
```

Below, the resulting probability weighting functions are displayed. 

```{r}

# Weighting Functions 

## compute decision weights 

cpt_w <- cpt_wide %>% 
  select(-c(alpha, rho)) %>% 
  expand_grid(ep = seq(0, 1, .1)) %>% #
  mutate(w = round(  (delta * ep^gamma)/ ((delta * ep^gamma)+(1-ep)^gamma), 2)) 

## plot curves

cpt_w %>% 
  group_by(boundary, a, s) %>% 
  ggplot(aes(x = ep, y = w, color = s)) +
  geom_path() + 
  scale_color_scico(palette = "grayC") + 
  scale_x_continuous(breaks = seq(0, 1, .2)) +
  scale_y_continuous(breaks = seq(0, 1, .2)) + 
  labs(x ="Experienced Probability",
       y = "Probability Weighting", 
       color="Switching Probability") + 
  theme_minimal()

cpt_w %>% 
  group_by(boundary, a, s) %>% 
  ggplot(aes(x = ep, y = w, color = s)) +
  geom_path() + 
  scale_color_scico(palette = "buda") + 
  scale_x_continuous(breaks = seq(0, 1, .2)) +
  scale_y_continuous(breaks = seq(0, 1, .2)) + 
  facet_wrap(~a) +
  labs(x ="Experienced Probability",
       y = "Probability Weighting", 
       color="Switching Probability") + 
  theme_minimal()
```

The false response rates for different parameter values of the generating model reflect the probability weighting patterns from above. 
That is, the strong curvature resulting from large switching probabilities produces an underweighting of small probabilities. This in turn has the effect that the rarity of an attractive (unattractive) outcome leads to higher rates of choosing the safe (risky) prospect although the risky (safe) prospect had a higher experienced expected value.

```{r}

# compute false response rates

fr_rates <- choices %>% 
  mutate(ev_ratio_exp = round(a_ev_exp/b_ev_exp, 2), # experienced EV (eEV)
         norm = case_when(ev_ratio_exp > 1 ~ "A", ev_ratio_exp < 1 ~ "B")) %>% # normative choice according to eEV
  filter(!is.na(norm)) %>% # exclude trials with normative indifferent prospects
  group_by(s, boundary, a, rare, norm, choice) %>% # group correct and incorrect responses
  summarise(n = n()) %>% # absolute numbers 
  mutate(rate = round(n/sum(n), 2), # response rates 
         type = case_when(norm == "A" & choice == "B" ~ "false safe", norm == "B" & choice == "A" ~ "false risky")) %>% filter(!is.na(type))  # remove correct responses

# violin scatter plot

fr_rates %>% 
  ggplot(aes(x = rare, y = rate, color = s)) +
  geom_quasirandom(aes(shape = type), size = 3) +  
  scale_y_continuous(breaks = seq(0, 1, .2)) +
  scale_color_scico(palette = "buda") + 
  scale_shape_manual(values=c(8, 16))+
  labs(x = "Rare Event", 
       y = "False Response Rate", 
       color = "Switching Probability",
       shape = "False Response") + 
  theme_minimal() 
```

```{r eval=FALSE, include=FALSE}
fr_rates %>% 
  ggplot(aes(a, s, fill = rate)) + 
  geom_tile(colour="white", size=0.1) +
  scale_fill_scico(palette = "buda") + 
  facet_grid(type ~ fct_relevel(rare, "attractive", "none", "unattractive"), switch = "y") +
  scale_x_continuous(expand=c(0,0), breaks = seq(1, 5, 1)) +
  scale_y_continuous(expand=c(0,0), breaks = seq(.1, 1, .1)) +
  labs(title = "False Response Rates", 
       x = "a", 
       y= "s", 
       fill = "% False Responses") + 
  theme_minimal() 


fr_rates %>% 
  ggplot(aes(s, rate, color = s)) + 
  facet_grid(type ~ fct_relevel(rare, "attractive", "none", "unattractive"), switch = "y") +
  geom_jitter(size = 3) + 
  scale_x_continuous(breaks = seq(0, 1, .1)) +
  scale_y_continuous(breaks = seq(0, 1, .1)) +
  scale_color_scico(palette = "buda") + 
  labs(title = "False Response Rates", 
       x = "s", 
       y= "% False Responses", 
       color = "a") + 
  theme_minimal()
```

### Value Function 

```{r}

# Density Plot 

cpt_wide %>%
  ggplot(aes(x = s, y = alpha)) +
  geom_density_2d_filled(contour_var = "ndensity") + 
  scale_fill_scico_d(palette = "grayC") +
  scale_x_continuous(breaks = seq(0, 1, .2)) + 
  scale_y_continuous(limits = c(0,2), breaks = seq(0, 2, .2)) +
  labs(x ="Switching Probability",
       y = "Parameter Value",  
       fill="Density") + 
  facet_wrap(~a, nrow = 1) + 
  theme_minimal()
```

```{r}
# value function 

## compute values 

cpt_v <- cpt_wide %>% 
  select(-c(gamma, delta, rho)) %>% 
  expand_grid(x = seq(0, 20, 2)) %>%  
  mutate(v = round(x^alpha, 2)) 

## plot curves

cpt_v %>% 
  group_by(boundary, a, s) %>% 
  ggplot(aes(x = x, y = v)) +
  geom_path(aes(color = s)) +
  scale_color_scico(palette = "grayC") + 
  scale_x_continuous(breaks = seq(0, 20, 5)) +
  scale_y_continuous(breaks = seq(0, 20, 5)) + 
  labs(title = "Value Function",
       x ="Objective Value",
       y = "Subjective Value", 
       color="Switching Probability") + 
  theme_minimal()

cpt_v %>% 
  group_by(boundary, a, s) %>% 
  ggplot(aes(x = x, y = v)) +
  geom_path(aes(color = s)) +
  scale_color_scico(palette = "buda") + 
  scale_x_continuous(breaks = seq(0, 20, 5)) +
  scale_y_continuous(breaks = seq(0, 20, 5)) + 
  facet_wrap(~a) +
  labs(title = "Value Function",
       x ="Objective Value",
       y = "Subjective Value", 
       color="Switching Probability") + 
  theme_minimal()
```

```{r eval=FALSE, include=FALSE}

# alpha
cpt_wide %>% 
  ggplot(aes(x = s, y = alpha, color = s)) +
  geom_point(size = 2) +
  geom_line(size = 1) +
  scale_color_scico(palette = "buda") + 
  scale_x_continuous(breaks = seq(0, 1, .2)) + 
  scale_y_continuous(breaks = seq(0, 2, .2)) +
  facet_grid(boundary~a, switch = "y") +
  labs(title = expression(alpha),
       x ="Switching Probability",
       y = expression(alpha), 
       color="Switching Probability") + 
  theme_minimal()
```

# Discussion and Conclusion

# References



