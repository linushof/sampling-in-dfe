---
title             : "Sampling Strategies in Decisions from Experience: Property Representation and Probability Weighting Approximation"
shorttitle        : "Sampling strategies"

header-includes: 
  - \renewcommand\author[1]{}
  - \renewcommand\affiliation[1]{}
  - \authorsnames[1]{Linus Hof}
  - \authorsaffiliations{{Department of Psychology, Heidelberg University}}

author: 
  - name          : "Linus Hof"
    affiliation   : "1"
    corresponding : yes    
    address       : "Department of Psychology, Heidelberg University, Hauptstra√üe 47-51, 69117 Heidelberg, Germany"
    email         : "linushof@posteo.de"

note: | 
  **Unsubmitted draft created from the commit with the hash**: ``r repro::current_hash()``

authornote: |
 This is a dynamic document which can be reproduced from the accompanying GitHub repository: https://github.com/linushof/sampling-in-dfe. 
 
 The current version of this manuscript is prepared for submission as a master's thesis. A modified later version might be prepared together with co-authors and submitted for peer-review.  
 
abstract: |
  Add short abstract.
  
keywords          : ""
wordcount         : ""

bibliography      : ["references_sampling-in-dfe.bib"]
csl               : apa.csl


floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
linkcolor         : "blue"
mask              : no
draft             : no

classoption       : "man"
documentclass     : "apa7"
output            : papaja::apa6_pdf
---

```{r}
# load packages
pacman::p_load(tidyverse, 
               knitr, 
               ggbeeswarm,
               scico, # for scientific color palettes
               papaja)
```

<!-- Introduction -->

The mind is a cognitive system that operates on inputs from an environment or memory.
Such inputs may be treated as a set *I* and its elements, the latter being distinguishable from each other on the basis of the *properties* they possess [cf. @jostMathematicalConcepts2015].
Often, however, the properties that the mind uses to control the output are not readily described, but must be inferred from incomplete or noisy information. 
Since how the inputs are represented determines the information that is made explicit and the costs at which certain operations can be carried out on that information, the feasibility and ease of these inferences depends greatly on the choice of *representation* [@marrVisionComputationalInvestigation1982; see also @griffithsProbabilisticModelsCognition2010; @kempStructuredStatisticalModels2009].
Hence, to explain how the mind turns certain inputs into meaningful outputs, one needs to understand the properties that control the output, the representation that is used to describe these properties, and the costs of making the descriptions in that representation, including their susceptibility to distortions from uncertainty or noise. 

In what follows, the discussion will turn to the domain of decision making and elaborate on the concepts touched upon thus far, however, primarily to illustrate one rather important point:
Given that the properties of choice alternatives serve as *selection criteria*, the more accurately these properties are described, the more systematic the choices should become.
Importantly, this principle is not restricted to any specific property, but rather does the choice pattern reflect the bias that the designation of a property as a selection criterion imposes on the assessment of choice alternatives. 

<!-- Computational level -->

Decision making is an information-processing problem whose solution requires a selection among choice alternatives. 
The basic structure of the problem may be modeled by the inclusion map $\iota : I' \mapsto I$, where $\iota(i) = i$ for all $i \in I'$, indicating that decision making is a *process* that starts from an input set $I$, including all choice alternatives, and stops at a proper subset $I' \subset I$, including only the chosen alternatives. 
Given that the selection process is systematic---as opposed to random---the chosen alternatives in $I'$ should possess the same property or combination of several properties which the other alternatives in the complementary subset $I \setminus I'$ do not possess.
We may label the properties that serve to distinguish between the chosen and non-chosen alternatives as *selection criteria*. 

Now, consider the power set $\mathcal{P}(I)$, which is the set of all possible subsets of $I$.
Each of the subsets $A' \in \mathcal{P}(I)$ corresponds to a possible distinction between choice alternatives that can be made after assessing for each alternative whether it fulfills the selection criteria or not [cf. @jostMathematicalConcepts2015, p. 16]. 
The concept of the power set thereby illustrates in a primitive way how different selection criteria can affect the consequential choice, provided the properties that can be captured and manipulated by a particular processing system, i.e., a living organism or a computer, are not uniformly distributed among the alternatives.

<!-- Algorithmic Level -->

A rather important part---though by far not the only one---of the computation that connects the initial set with its subset is then the comparison of the properties of the choice alternatives against some selection criteria. 
At the algorithmic level, such a comparison requires an explicit description of the properties using a representation which is useful to the processing system [cf. @marrVisionComputationalInvestigation1982]. 
In quite many cases, however, a number of different representations can be used to describe the properties of choice alternatives in a given set.
Yet, not every representation permits an accurate description of a given property at reasonable expenses or allows to describe a property at all [cf. @marrRepresentationRecognitionSpatial1978].
E.g., a consequential difference that has emerged in behavioral research on risky choice is whether the probability of a risky outcome is described using a decimal number (.2), ratio (2/10), or percentage (20%), or whether it must be inferred from the relative frequency of occurrence of the particular outcome in a collection of independent and identically distributed (i.i.d.) random variables, i.e., a *random sample*.
Noteworthy, for the latter case, which is usually referred to as *decisions from experience*, empirical protocols indicate that people decide as if they were underweighting small probability events [@barronSmallFeedbackbasedDecisions2003; @hertwigDecisionsExperienceEffect2004; @weberPredictingRiskSensitivity2004].
One undisputed factor that contributes to this underweighting pattern is the sampling error associated with people's reliance on small samples [e.g., @foxDecisionsExperienceSampling2006; @hertwigDecisionsExperienceEffect2004; @hauDescriptionexperienceGapRisky2008; @ungemachAreProbabilitiesOverweighted2009; see also @wulffMetaanalyticReviewTwo2018 for a meta-analytic review], indicating that, although the probabilities of outcomes can in principle be accurately described in decisions from experience, the actual description is rather inaccurate because it cannot be done at reasonable expenses.  

Another property of choice alternatives that is likely to play a role in risky choice but can only be made explicit using a sample representation as in decisions from experience is the sequence in which outcomes occur.
Although a discrete-time collection of i.i.d. random variables cannot, by definition, exhibit temporal regularity, i.e., it cannot produce a stable sequence of sampled outcomes, spurious temporal patterns might nevertheless be regarded as a property of choice alternatives and affect the consequential choice behavior. 
Indeed, the underweighting pattern and other behavioral phenomena like the wavy recency effect and probability matching, are reasonably well captured by models which incorporate the assumption that people rely on (spurious) outcome sequences [@cohenEffectPerceivedPatterns2021; @gaissmaierSmartPotentialProbability2008; @plonskyRelianceSmallSamples2015; @plonskyLearningSettingsPartial2017;  @yuSequentialEffectsSupersition2009]. 
 
The preceding two examples illustrate how some properties of choice alternatives can be described in one representation but not in another and how representations can differ with regards to the processing effort required to describe the same property.
Thus, there is generally a trade-off between different representations, where in a given representation certain properties become explicit while others are pushed into the background [@marrVisionComputationalInvestigation1982, p. 21].

<!-- In the same vein, there is a call for *representational pluralism* in cognition research [@griffithsProbabilisticModelsCognition2010] -->


<!-- Samples as representation -->

In research on the decision theory, a standard paradigm is the choice between at least two (monetary) prospects.
Let a prospect be a probability space $(\Omega, \mathcal{F}, P)$.
$\Omega$ is the sample space 

$$
\Omega = \{\omega_1, ..., \omega_n\}
$$ 

containing a finite set of possible outcomes $\omega$, monetary gains and/or losses respectively. 
$\mathcal{F}$ is the set of all possible subsets of $\Omega$: 

$$
\mathcal{F} = \{A_1, A_2, ...\} = \mathcal{P}(\Omega) 
\; .
$$

$P$ is a probability mass function  

$$
P: \mathcal{F} \mapsto [0,1] 
$$

that assigns each outcome $\omega$ a probability $0 < p(\omega) \leq 1$ with $P(\Omega) = 1$ [ @kolmogorovFoundationsTheoryProbability1950, pp. 2-3].

In such a choice paradigm, agents are asked to evaluate the prospects and build a preference for either one of them. 
It is common to make a distinction between two variants of this evaluation process [cf. @hertwigDescriptionexperienceGapRisky2009]. 
For decisions from description (DfD), agents are provided a full symbolic description of the prospects.
For decisions from experience [DfE; e.g., @hertwigDecisionsExperienceEffect2004], prospects are not described but must be explored by the means of sampling. 

To provide a formal definition of sampling in risky choice, we make use of the mathematical concept of a random variable and start by referring to a prospect as *"risky"* in the case where $p(\omega) \neq 1$ for all $\omega \in \Omega$.
Here, risky describes the fact that if agents would choose a prospect and any of its outcomes in $\Omega$ must occur, none of these outcomes will occur with certainty. 
It is acceptable to speak of the occurrence of $\omega$ as a realization of a random variable $X$ defined on a prospect iff the following conditions (1) and (2) are met: 

(1) $X$ is a measurable function $$X: (\Omega, \mathcal{F})  \mapsto (\Omega', \mathcal{F'}) \; ,$$ where $\Omega'$ is a set of real numbered values $X$ can take and $\mathcal{F'}$ is a set of subsets of $\Omega'$. I.e., $\Omega$ maps into $\Omega'$ such that correspondingly each subset $A' \in \mathcal{F'}$ has a pre-image $X^{-1}A' \in \mathcal{F}$, which is the set $\{\omega \in \Omega: X(\omega) \in A'\}$ [@kolmogorovFoundationsTheoryProbability1950, p. 21].

(2) The mapping is such that $X(\omega) = x \equiv \omega$. 

In (2), $x \equiv \omega$ means that the realization of a random variable $X(\omega) = x$ is numerically equivalent to its pre-image $\omega$.  
Given conditions (1) and (2), we denote any observation of $\omega$ as a *"single sample"*, or realization, of a random variable defined on a prospect and the act of generating a sequence of single samples in discrete time as *"sequential sampling"*. 
Note that, since random variables defined on the same prospect are independent and identically distributed (iid), the weak law of the large number applies to the relative frequency of occurrence of an outcome $\omega$ in a sequence of single samples originating from the same prospect [cf. @bernoulliArsConjectandiOpus1713].
Thus, long sample sequences in principle allow to obtain the same information about a prospect by sampling as by symbolic description.

Consider now a choice between prospects $1, ..., k$.
To construct a stochastic sampling model for DfE, we assume that agents base their decision on the information related to these prospects and define a decision variable as a function of the latter:

$$
D:= f((\Omega_1, \mathcal{F}_1, P_1), ..., (\Omega_k, \mathcal{F}_k, P_k))
\;.
$$

Now, since in DfE no symbolic descriptions of the prospects are provided, the model must be restricted to the case where decisions are based on sequences of single samples originating from the respective prospects:

$$
D := f(X_{i1}, ..., X_{ik}) 
\; ,
$$

where $i \in \{1, ..., N\}$ denotes a sequence of length $N$ of random variables that are iid.  

Concerning the form of $f$ and the measures it utilizes, it is quite proper to say that they reflect our assumptions about the exact kind of information agents process and the way they do and that these choices should be informed by psychological theory and empirical protocols. 
Taking the case of different sampling and decision strategies previously assumed to play a role in DfE, the following section demonstrates how such assumptions can be explicated in a stochastic model that builds on the sampling approach outlined so far.  

# Capturing Differences in Sampling and Decision Strategies  

Hills and Hertwig [-@hillsInformationSearchDecisions2010] discussed a potential link between sampling and decision strategies in DfE. 
Specifically, the authors suppose that if single samples originating from different prospects are generated in direct succession (piecewise sampling), the evaluation of prospects is based on multiple ordinal comparisons of single samples (round-wise decisions).
In contrast, if single samples originating from the same prospect are generated in direct succession (comprehensive sampling), it is supposed that the evaluation of prospects is based on a single ordinal comparison of long sequences of single samples (summary decisions) [@hillsInformationSearchDecisions2010, Figure 1 for a graphical summary]. 

We now consider choices between two prospects and the assumptions of Hills and Hertwig [-@hillsInformationSearchDecisions2010] in more detail to build the respective stochastic sampling model for DfE.

Let $X$ and $Y$ be random variables defined on the prospects $(\Omega_X, \mathcal{F}_X, P_X)$ and $(\Omega_Y, \mathcal{F}_Y, P_Y)$. 

Hills and Hertwig [-@hillsInformationSearchDecisions2010] suggest that any two sample sequences $X_i$ and $Y_i$ are compared by their means. 
Let thus $C = \mathbb{R}$ be the set of all possible outcomes of the comparison of the means over two unordered sequences $X_i$ and $Y_i$ of given lengths $N_X$ and $N_Y$.
That is, each element $c \in C$ represents a pair $X_i \cap Y_i$ with a unique combination of the number of times any possible realization of $X$ and $Y$ occurs. 
Let then

$$
\mathcal{G} = \left\{B_1 = \{c \in C: \overline{X}_{N_X} - \overline{Y}_{N_Y} > 0\}, B_2 = \{ c \in C: \overline{X}_{N_X} - \overline{Y}_{N_Y} \leq 0\}
\right\}
$$

be a set of subsets of $C$, indicating that comparisons of prospects on the ordinal (rather than on the metric) scale are of primary interest. 
We denote a single ordinal comparison by the event space $(C, \mathcal{G})$.
The observed outcome of such an ordinal comparison can be regarded as evidence for or against a prospect and the number of wins over a series of independent ordinal comparisons as accumulated evidence. 
To integrate the concept of evidence accumulation into the current model, we let the decision variable $D$ be a measurable function 

$$
D: C \mapsto C'
$$

that maps the possible outcomes of a mean comparison in $C$ onto a measure space $C' = \{0,1\}$, with $0$ $(1)$ indicating a lost (won) comparison:

$$
D(c \in C) \in C' = 
  \begin{cases}
    1 \quad \text{for} \quad \{c \in C: (\overline{X}_{N_X} - \overline{Y}_{N_Y} > 0)\}  \in \mathcal{G} \\
    0 \quad \text{else}. 
  \end{cases} 
$$

It can be shown that for fixed sequence lengths $N_X$ and $N_Y$, a sequence $D_i = D_1, ..., D_n$ representing multiple independent comparisons is a Bernoulli process where the number of won comparisons follows the binomial distribution

$$
D \sim \mathcal{B}\left( p \left(\overline{X}_{N_X} - \overline{Y}_{N_Y} > 0\right), n\right) \; ,
$$

with $p$ denoting probability of $X_i$ winning a single comparison, i.e., the probability of success, and $n$ denoting the number of comparisons.

It becomes evident that the two combinations of sampling and decision strategies proposed by Hills and Hertwig [-@hillsInformationSearchDecisions2010] are reflected in certain restrictions on the free parameters $N_X, N_Y$ and $n$ of the sampling model and that these restrictions translate to the shape of the binomial distribution associated to the decision variables, straightforwardly via the parameter $n$, more interestingly, however, via its parameter $p$. 

I.e., first, whether a single (summary decision) or multiple ordinal comparisons (round-wise decision) are conducted, is reflected by the length of the sequence $D_i = D_1, ..., D_n$, with $n = 1$ for summary decisions and $n \geq 1$ for round-wise decisions. 
The idea that multiple comparisons may be conducted can be formalized by extending the event space $(C, \mathcal{G})$ of a single comparison to its $n$-fold product 

$$
\left(\prod_{i = 1}^{n} C_i, \bigotimes_{i = 1}^{n} \mathcal{G}_i \right)
\; ,
$$

where $i \in \{1, ..., n\}$ denotes the number of comparisons. 

Second, whether comparisons are conducted on the basis of longer sample sequences or single samples is reflected by the lengths of sample sequences from the same prospect with $N_X, N_Y \geq 1$ for comprehensive sampling and $N_X = N_Y = 1$ for piecewise sampling. 

Note now that, in order to derive predictions of the choice behavior from the current stochastic model, the full product model

$$
\left(\prod_{i = 1}^{n} C_i, \bigotimes_{i = 1}^{n} \mathcal{G}_i, \bigotimes_{i = 1}^{n} P_i \right)
\; ,
$$

including the probability measure $P$, which assigns each $c \in C$, and each $B \in \mathcal{G}$ respectively, a probability, must be identified. 
Although this can be demonstrated for the random variables considered so far, it becomes rather quickly intractable with increasing elements in $\Omega$ and growing sequence lengths. 
However, by explicating the above probability measure ([Appendix 1][Appendix 1: Probability Measure Over Sample Mean Comparisons]), it becomes evident that it is largely influenced by the underlying sequence lengths $N_X$ and $N_Y$.
In the following section and a subsequent simulation analysis, we demonstrate that this influence is of such a form that it produces some principle and predictable trends in the choice behavior that can be modeled in the cumulative prospect theory.

# Predicting Choice Behavior 

Note first that the current stochastic sampling model allows to treat sampling and decision strategies independently by parameterization of both lengths of sample sequences and the number of comparisons.    

... 

# Method

## Generating Model

...

The switching probability $s$ is the probability with which agents draw a single sample from the prospect they did not get their most recent single sample from.
$s$ is varied between .1 to 1 in increments of .1.
The two boundary parameters resemble the concept of a decision threshold, i.e., if a prospect reaches a boundary, it is chosen by the synthetic agent.
The boundary type is either the minimum number of comparisons any prospect must win (absolute boundary) or the minimum difference between the number of won comparisons (relative boundary).
The boundary value $a$ is varied between 1 to 5 in increments of 1.

## Test set

For each parameter combination of the generating model, 100 synthetic agents are presented with 60 choices problems.
In sum, 100 (parameter combinations) x 100 (agents) x 60 (choices) = 600,000 choices are simulated.
We test a set of 2-prospect choice problems, where one of the prospects contains a safe outcome, i.e., $p(\omega) = 1$ and the other two outcomes where all $p(\omega) \neq 1$.
Both outcomes and probabilities are drawn from uniform distributions, ranging from 0 to 20 for outcomes and from .01 to .99 for probabilities of the smaller outcome of the risky prospect.
To omit dominant prospects within a choice problem, outcomes of the safe prospect always fall between both outcomes of the risky prospect.
Table A1 in [Appendix 2][Appendix 2: Choice Problems] contains the test set of 60 choice problems, which were sampled from an initial set of 10,000.
Sampling of gambles was stratified, randomly drawing an equal number of 20 gambles with no, an attractive, and an unattractive rare outcome.
Risky outcomes are considered *"rare"* if their probability is $p < .2$ and *"attractive"* (*"unattractive"*) if they are higher (lower) than the safe outcome.

## Describing Model: Cumulative Prospect Theory

...

```{r}

# read cpt data

cols <- list(.default = col_double(),
             boundary = col_factor(),
             a = col_factor(),
             parameter = col_factor())
cpt_long <- read_csv("data/estimates/estimates_cpt.csv", col_types = cols)

# store convergence diagnostics

gel_92 <- cpt_long %>% select(s, boundary, a, parameter, Rhat, n.eff) 
```

For each distinct parameter combination, we ran 20 chains of 40,000 iterations each, after a warm-up period of 1000 samples.
To reduce potential autocorrelation during the sampling process, we only kept every 20th sample (thinning).
The potential scale reduction factor $\hat{R}$ [@gelmanInferenceIterativeSimulation1992] was $\leq$ `r round(max(gel_92$Rhat), 3)` for all parameters, indicating good convergence.
The minimum effective sample size was `r min(gel_92$n.eff)`.

# Results

## Plausibility Check: Relationship between Switching Probability and Trial Length

```{r message=FALSE}

# read choice data 

cols <- list(.default = col_double(),
             boundary = col_factor(),
             gamble = col_factor(),
             rare = col_factor(),
             agent = col_factor(),
             choice = col_factor())
choices <- read_csv("data/choices/choices.csv", col_types = cols)

# get median trial length for each parameter combination

trial_length <- choices %>% 
  group_by(s, boundary, a) %>% 
  summarise(med = median(n_sample))
```

The median length of trials, i.e., the number of single samples drawn in a trial, generated by different parameter combinations ranged from `r min(trial_length$med)` to `r max(trial_length$med)`.
As expected, the scatter plot below shows an inverse relationship between switching probability and trial length.
I.e., the lower the switching probability, the larger become the sample sequences on which each comparison between prospects is based, which in turn leads to longer trials.  
This effect is particularly pronounced for low probabilities such that the increase in trial length accelerates as switching probability decreases.


```{r fig.cap="Test"}

# get median trial length for each switching probability 

trial_length_s <- choices %>% 
  group_by(s) %>%
  summarise(med = median(n_sample))

# plot

trial_length %>%
  ggplot(aes(x = s, y = med)) +
  geom_jitter(color = "#CECECE", size = 3) +
  geom_point(data = trial_length_s, aes(color = s), size = 3) +
  geom_path(data = trial_length_s, aes(color = s)) +
  scale_color_scico(palette = "buda") + 
  scale_x_continuous(breaks = seq(0, 1, .1)) + 
  scale_y_continuous(breaks = seq(0, 120, 10)) +
  labs(title = "Trial Length", 
       x ="Switching Probability",
       y = "Median Trial Length", 
       color="Switching Probability") + 
  theme_apa()
```


## Probability Weighting Function

```{r}

# tidy CPT data: parameters as separate columns 

cpt_wide <- cpt_long %>% 
  select(s, boundary, a, parameter, mean) %>% 
  pivot_wider(names_from = parameter, values_from = mean)
```

The figures below display the estimates of the $\gamma$ and $\delta$ parameter of the probability weighting function [@prelecProbabilityWeightingFunction1998] fitted to DfE simulated for different parameter values of the generating model.
The estimates are plotted against the switching probability, where each panel represents a distinct boundary type and level, i.e., number of comparisons. 
Grey dots represent agent level estimates, colored dots represent the mean across all agent level estimates. 

Most significantly, there is a strong relationship between the switching probability in the generating model and the $\gamma$ parameter.
I.e., large switching probabilities, which are indicative for small sample sequences, lead to larger estimates for $\gamma$. 
The resulting strong curvature leads to a compression of probabilities in the lower and upper range, reflecting underweighting of small probabilities and overweighting of large probabilities. 
This pattern is robust for varying degrees of the boundary level, except for $a = 1$. 
The latter deviation may be explained by the potential scale reduction factors for the respective parameter estimates, which indicate that the MCMC chains did not converge (see [Appendix 3][Appendix 3: Convergence Diagnostics for CPT Parameters]).   

```{r fig.cap="test 2"}

# Density Plot 

cpt_long %>% 
  filter(parameter == "gamma" | parameter == "delta") %>% 
  ggplot(aes(x = s, y = mean)) +
  geom_density_2d_filled(contour_var = "ndensity") + 
  scale_fill_scico_d(palette = "buda") +
  scale_color_scico_d(palette = "buda") + 
  scale_x_continuous(breaks = seq(0, 1, .2)) + 
  scale_y_continuous(limits = c(0,2), breaks = seq(0, 2, .2)) +
  labs(x ="Switching Probability",
       y = "Parameter Value",  
       fill="Density") + 
  facet_grid(parameter~a) + 
  theme_apa()
```

```{r eval=FALSE, include=FALSE}

# Scatter Plots 

# Gamma 

cpt_wide %>% 
  ggplot(aes(x = s, y = gamma, color = s)) +
  geom_point(size = 2) +
  geom_line(size = 1) +
  scale_color_scico(palette = "buda") + 
  scale_x_continuous(breaks = seq(0, 1, .2)) + 
  scale_y_continuous(breaks = seq(0, 2, .2)) +
  facet_grid(boundary~a) +
  labs(title = expression(paste("Curvature ", gamma)),
       x ="Switching Probability",
       y = expression(gamma), 
       color="Switching Probability") + 
  theme_minimal()

# Delta

cpt_wide %>% 
  ggplot(aes(x = s, y = delta, color = s)) +
  geom_point(size = 2) +
  geom_line(size = 1) +
  scale_color_scico(palette = "buda") + 
  scale_x_continuous(breaks = seq(0, 1, .2)) + 
  scale_y_continuous(breaks = seq(0, 2, .2)) +
  facet_grid(boundary~a, switch = "y") +
  labs(title = expression(paste("Curvature ", delta)),
       x ="Switching Probability",
       y = expression(delta), 
       color="Switching Probability") + 
  theme_minimal()
```


Below, the resulting probability weighting functions are displayed. 

```{r}

# Weighting Functions 

## compute decision weights 

cpt_w <- cpt_wide %>% 
  select(-c(alpha, rho)) %>% 
  expand_grid(ep = seq(0, 1, .1)) %>% #
  mutate(w = round(  (delta * ep^gamma)/ ((delta * ep^gamma)+(1-ep)^gamma), 2)) 

## plot curves

cpt_w %>% 
  group_by(boundary, a, s) %>% 
  ggplot(aes(x = ep, y = w, color = s)) +
  geom_path() + 
  scale_color_scico(palette = "buda") + 
  scale_x_continuous(breaks = seq(0, 1, .2)) +
  scale_y_continuous(breaks = seq(0, 1, .2)) + 
  labs(x ="Experienced Probability",
       y = "Probability Weighting", 
       color="Switching Probability") + 
  theme_minimal()

cpt_w %>% 
  group_by(boundary, a, s) %>% 
  ggplot(aes(x = ep, y = w, color = s)) +
  geom_path() + 
  scale_color_scico(palette = "buda") + 
  scale_x_continuous(breaks = seq(0, 1, .2)) +
  scale_y_continuous(breaks = seq(0, 1, .2)) + 
  facet_wrap(~a) +
  labs(x ="Experienced Probability",
       y = "Probability Weighting", 
       color="Switching Probability") + 
  theme_minimal()
```

The false response rates for different parameter values of the generating model reflect the probability weighting patterns from above. 
That is, the strong curvature resulting from large switching probabilities produces an underweighting of small probabilities. This in turn has the effect that the rarity of an attractive (unattractive) outcome leads to higher rates of choosing the safe (risky) prospect although the risky (safe) prospect had a higher experienced expected value.

```{r}

# compute false response rates

fr_rates <- choices %>% 
  mutate(ev_ratio_exp = round(a_ev_exp/b_ev_exp, 2), # experienced EV (eEV)
         norm = case_when(ev_ratio_exp > 1 ~ "A", ev_ratio_exp < 1 ~ "B")) %>% # normative choice according to eEV
  filter(!is.na(norm)) %>% # exclude trials with normative indifferent prospects
  group_by(s, boundary, a, rare, norm, choice) %>% # group correct and incorrect responses
  summarise(n = n()) %>% # absolute numbers 
  mutate(rate = round(n/sum(n), 2), # response rates 
         type = case_when(norm == "A" & choice == "B" ~ "false safe", norm == "B" & choice == "A" ~ "false risky")) %>% filter(!is.na(type))  # remove correct responses

# violin scatter plot

fr_rates %>% 
  ggplot(aes(x = rare, y = rate, color = s)) +
  geom_quasirandom(aes(shape = type), size = 3) +  
  scale_y_continuous(breaks = seq(0, 1, .2)) +
  scale_color_scico(palette = "buda") + 
  scale_shape_manual(values=c(8, 16))+
  labs(x = "Rare Event", 
       y = "False Response Rate", 
       color = "Switching Probability",
       shape = "False Response") + 
  theme_minimal() 
```

```{r eval=FALSE, include=FALSE}
fr_rates %>% 
  ggplot(aes(a, s, fill = rate)) + 
  geom_tile(colour="white", size=0.1) +
  scale_fill_scico(palette = "buda") + 
  facet_grid(type ~ fct_relevel(rare, "attractive", "none", "unattractive"), switch = "y") +
  scale_x_continuous(expand=c(0,0), breaks = seq(1, 5, 1)) +
  scale_y_continuous(expand=c(0,0), breaks = seq(.1, 1, .1)) +
  labs(title = "False Response Rates", 
       x = "a", 
       y= "s", 
       fill = "% False Responses") + 
  theme_minimal() 


fr_rates %>% 
  ggplot(aes(s, rate, color = s)) + 
  facet_grid(type ~ fct_relevel(rare, "attractive", "none", "unattractive"), switch = "y") +
  geom_jitter(size = 3) + 
  scale_x_continuous(breaks = seq(0, 1, .1)) +
  scale_y_continuous(breaks = seq(0, 1, .1)) +
  scale_color_scico(palette = "buda") + 
  labs(title = "False Response Rates", 
       x = "s", 
       y= "% False Responses", 
       color = "a") + 
  theme_minimal()
```


## Value Function 

```{r}

# Density Plot 

cpt_wide %>%
  ggplot(aes(x = s, y = alpha)) +
  geom_density_2d_filled(contour_var = "ndensity") + 
  scale_fill_scico_d(palette = "buda") +
  scale_x_continuous(breaks = seq(0, 1, .2)) + 
  scale_y_continuous(limits = c(0,2), breaks = seq(0, 2, .2)) +
  labs(x ="Switching Probability",
       y = "Parameter Value",  
       fill="Density") + 
  facet_wrap(~a, nrow = 1) + 
  theme_minimal()
```

```{r}
# value function 

## compute values 

cpt_v <- cpt_wide %>% 
  select(-c(gamma, delta, rho)) %>% 
  expand_grid(x = seq(0, 20, 2)) %>%  
  mutate(v = round(x^alpha, 2)) 

## plot curves

cpt_v %>% 
  group_by(boundary, a, s) %>% 
  ggplot(aes(x = x, y = v)) +
  geom_path(aes(color = s)) +
  scale_color_scico(palette = "buda") + 
  scale_x_continuous(breaks = seq(0, 20, 5)) +
  scale_y_continuous(breaks = seq(0, 20, 5)) + 
  labs(title = "Value Function",
       x ="Objective Value",
       y = "Subjective Value", 
       color="Switching Probability") + 
  theme_minimal()

cpt_v %>% 
  group_by(boundary, a, s) %>% 
  ggplot(aes(x = x, y = v)) +
  geom_path(aes(color = s)) +
  scale_color_scico(palette = "buda") + 
  scale_x_continuous(breaks = seq(0, 20, 5)) +
  scale_y_continuous(breaks = seq(0, 20, 5)) + 
  facet_wrap(~a) +
  labs(title = "Value Function",
       x ="Objective Value",
       y = "Subjective Value", 
       color="Switching Probability") + 
  theme_minimal()
```

```{r eval=FALSE, include=FALSE}

# alpha
cpt_wide %>% 
  ggplot(aes(x = s, y = alpha, color = s)) +
  geom_point(size = 2) +
  geom_line(size = 1) +
  scale_color_scico(palette = "buda") + 
  scale_x_continuous(breaks = seq(0, 1, .2)) + 
  scale_y_continuous(breaks = seq(0, 2, .2)) +
  facet_grid(boundary~a, switch = "y") +
  labs(title = expression(alpha),
       x ="Switching Probability",
       y = expression(alpha), 
       color="Switching Probability") + 
  theme_minimal()
```

# Discussion 

# Conclusion


# References

<div id="refs"></div>

# Software

# Appendix 

## Appendix 1: Probability Measure Over Sample Mean Comparisons

To proof is that the sequence $D_i = D_1, ..., D_n$ describes a Bernoulli process with the number of won comparisons following the binomial distribution $\mathcal{B}\left(p(\overline{X}_{N_X} - \overline{Y}_{N_Y} > 0), n \right)$ for fixed sequence lengths $N_X$ and $N_Y$.

Therefore we must demonstrate that the probability of success, i.e., $p \left(\overline{X}_{N_X} - \overline{Y}_{N_Y} > 0\right)$, is identified.
The derivation chain is first roughly paraphrased and then formalized as a mathematical equation. 

We start by noting that the probability of success equals the probability of the subset $B_1 \in \mathcal{G}$ which contains all comparisons $c \in C$ that satisfy the inequation $\overline{X}_{N_X} - \overline{Y}_{N_Y} > 0$.
Since the probability of the subset $B_1$ equals the probability that any of its elements occurs, the probabilities of all $c \in B_1$ are summed up. 
We therefore use the sum operator with the limits $j \in \{1, ..., |B_1|\}$, with $|B_1|$ denoting the cardinality of the set $B_1$, i.e., the number of elements within the set. 
As a major assumption is that the random variables within a sample sequence are iid and independent across sequences, the joint probability of a pair $\overline{X}_{N_X} \cap \overline{Y}_{N_Y}$, i.e., a pair $X_i \cap Y_i$ with a unique combination of the number of times any possible realization of $X$ and $Y$ occurs, is described by the product of their individual probabilities $p(\overline{X}_{N_X})$ and $p(\overline{Y}_{N_Y})$.
Finally, to get these individual probabilities, we must determine the probability that each possible realization indeed occurs the respective number of times in the underlying sample sequence.
This probability is given by the multinomial distribution which reduces to a binomial distribution in the case of only two possible realizations. 
We denote the number of times a realization occurs in a sample sequence with $k_{\omega}$, where $\omega$ signals that the realizations of the random variables are numerical equivalent to the outcomes in the sample space of the prospects.  

$$
  \begin{split}
    p(D = 1) 
      & = p \left( \; B_1 = \{c \in C: (\overline{X}_{N_X} - \overline{Y}_{N_Y} > 0)\} \in \mathcal{G} \; \right) \\\\\\ 
      & = \sum_{j=1}^{|B_1|} \left( \;
          p(\overline{X}_{N_Xj})  
          \times 
          p(\overline{Y}_{N_Yj}) \; 
        \right)_{j \, \in \, \{1, ..., |B_1|\}} \\\\\\
      & = \sum_{j=1}^{|B_1|} \left( \; 
        p \left( 
          \frac{1}{N_X} \sum\limits_{i = 1}^{N_X} X(\omega \in \Omega_X)_{ij}
        \right) 
        \times
        p \left( 
          \frac{1}{N_Y} \sum\limits_{i = 1}^{N_Y} Y(\omega \in \Omega_Y)_{ij}
        \right) \; 
        \right)_{j \, \in \, \{1, ..., |B_1|\}} \\\\\\
      & = \sum_{j=1}^{|B_1|} \left( \;
        \left(
          {N_X\choose k_{\omega_{1X}j}, ..., k_{\omega_{nX}j}} \prod_{\omega_{iX} \in \Omega_X} p(\omega_{iX})^{k_{\omega_{iX}j}}
        \right)
        \times
        \left(
        {N_Y\choose k_{\omega_{1Y}j}, ..., k_{\omega_{nY}j} } \prod_{\omega_{iY} \in \Omega_Y} p(\omega_{iY})^{k_{\omega_{iY}j}}
        \right)
        \right)_{j \, \in \, \{1, ..., |B_1|\}} \\\\\\
      & = \sum_{j=1}^{|B_1|} \left( \;
        \left(
          \frac{N_X!}{k_{\omega_{1X}j}! \times ... \times k_{\omega_{nX}j}!} 
          \times 
          p(\omega_{1X})^{k_{\omega_{1X}j}} 
          \times 
          ... 
          \times p(\omega_{nX})^{k_{\omega_{nX}j}}
        \right)
        \times
        \left(
          \frac{N_Y!}{k_{\omega_{1Y}j}! \times ... \times k_{\omega_{nY}j}!} 
          \times 
          p(\omega_{1Y})^{k_{\omega_{1Y}j}} 
          \times 
          ... 
          \times p(\omega_{nY})^{k_{\omega_{nY}j}}
        \right)
      \right)_{j \, \in \, \{1, ..., |B_1|\}}
  \end{split}
$$


## Appendix 2: Choice Problems 

```{r message=FALSE}
gambles <- read_csv("data/gambles/sr_subset.csv")
gambles %>% apa_table()
```

