---
title             : |
  Sampling Strategies in Decisions from Experience
  
shorttitle        : "Sampling strategies"

author: 
  - name          : "Linus Hof"
    affiliation   : "1"
    corresponding : yes
    address       : "Institute of Psychology, Heidelberg University, Hauptstr. 47 â€“ 51, 69117 Heidelberg"    
    email         : "linus.hof@stud.uni-heidelberg.de"
    
affiliation:
  - id            : "1"
    institution   : "Heidelberg University"
    
authornote: |
 This manuscript is a dynamic document which can be reproduced using the materials and instructions provided on the `GitHub` repository: https://github.com/linushof/sampling-in-dfe. The current version of the manuscript, created from the commit with the hash ``r repro::current_hash()``, is prepared for submission as a master's thesis in fulfillment of the requirements for the degree Master of Science (M.Sc.) at Heidelberg University, Faculty of Behavioural and Cultural Studies, Institute of Psychology. Supervisors:
  
  \indent
  1. Prof. Dr. Thorsten Pachur, Technical University of Munich
  
  \indent
  2. Dr. Veronika Zilker, Max Planck Institute for Human Development
 
abstract: |
  Add abstract
  
keywords          : "risky choice, information sampling, evidence accumulation, computational modeling, cumulative prospect theory"
wordcount         : ""

bibliography      : references.bib
csl               : "`r system.file('rmd', 'apa7.csl', package = 'papaja')`"

figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
linkcolor         : "blue"
link-citations    : true
mask              : no
draft             : no

classoption       : "man"
documentclass     : "apa7"
output            : papaja::apa6_pdf

header-includes:
   - \usepackage[capposition=top]{floatrow}
---

```{r setup, include=FALSE}
# set global chunk options
knitr::opts_chunk$set(echo = FALSE, 
                      fig.align = "l", 
                      fig.pos = "t", 
                      fig.height=7, 
                      fig.width=10)
```

```{r packages, include=FALSE}
# load required packages
pacman::p_load(papaja,
               tidyverse,
               viridis,
               viridisLite, 
               ggpubr,  
               latex2exp 
               )
```

```{r data, include=FALSE}
# load and prepare required data

## choice data

### specify column types
cols_choices <- list(.default = col_double(),
                     boundary = col_factor(),
                     gamble = col_factor(),
                     rare = col_factor(),
                     agent = col_factor(),
                     choice = col_factor())

### load data files from round-wise and summary model
choices_roundwise <- read_csv("data/choices_roundwise.csv", col_types = cols_choices)
choices_summary <- read_csv("data/choices_summary.csv", col_types = cols_choices)

### merge data files 
choices_roundwise <- choices_roundwise %>%
  mutate(model = "roundwise") %>% 
  select(model, everything())

choices_summary <- choices_summary %>%
  mutate(model = "summary") %>% 
  select(model, everything())

choices <- bind_rows(choices_roundwise, choices_summary)

## cpt estimates
cols_cpt <- list(.default = col_double(),
                 boundary = col_factor(),
                 a = col_factor(),
                 parameter = col_factor())

cpt_roundwise <- read_csv("data/cpt-parameters_roundwise.csv", col_types = cols_cpt)
cpt_summary <- read_csv("data/cpt-parameters_summary.csv", col_types = cols_cpt)

cpt_roundwise <- cpt_roundwise %>%
  mutate(model = "roundwise") %>% 
  select(model, everything())

cpt_summary <- cpt_summary %>%
  mutate(model = "summary") %>% 
  select(model, everything())
  
cpt <- bind_rows(cpt_roundwise, cpt_summary)
```

<!-- Introduction -->

The human mind is a cognitive system that operates on inputs from an environment or memory, and in the domain of decision making, such inputs provide information about the alternatives between which people can choose.
Let a prospect (or: gamble) be a choice alternative that possesses only two kinds of fundamental properties: the possible outcomes of the alternative (e.g., gains or losses of some amount) and the probabilities with which these outcomes occur following the choice of the alternative.
Importantly, the information that the inputs provide about these properties can come in different forms.
Specifically, in *decisions from description* (DFD), the inputs take the form of explicit and complete descriptions of all outcomes and probabilities.
In this case, people make decisions under risk, provided that choosing one of the prospects does not inevitably lead to a particular outcome.
Yet, in *decisions from experience* [DFE, @hertwigDecisionsExperienceEffect2004], the same properties are latent and therefore not known with certainty, but they must be inferred from the relative frequencies with which the outcomes occurred in the past, e.g., when the prospects were chosen in similar past decisions.
In other words, in DFE, the inputs take the form of sampled outcomes, with the lack of precise knowledge about all possible outcomes and their probabilities rendering the decision one under uncertainty rather than just risk.
The differences in the input formats between DFD and DFE are illustrated in the following `code example`:

```{r code-example, echo=TRUE}
# define prospect with two outcomes: 6 with 20%, 0 otherwise (80%)
outcomes <- c(6, 0)
probabilities <- c(.2, .8)

# input to decisions from description
input_DFD <- data.frame(outcomes, probabilities)
print(input_DFD)

# input to decisions from experience, here: 10 sampled outcomes
set.seed(2022)
input_DFE <- sample(outcomes, size = 10, prob = probabilities, replace = T)
print(input_DFE)
```

Although these distinct input forms may in principle carry the same information about a given set of prospects, behavioral decision research has so far produced a large body of papers indicating that there are robust differences between the choices that are made in DFD and DFE, leading to the notion of the *description-experience gap* [@hertwigDescriptionexperienceGapRisky2009].
In one reading of the gap,^[<!--Open footnote-->See @hertwigConstructbehaviorGapDescription2018, for a discussion on the interpretation of the gap.<!--Close footnote-->]
choice patterns in DFD deviate from the normative solutions of expected value (EV) maximization, as if small-probability outcomes are given more weight than would be warranted by their objective probabilities (*as-if overweighting of rare outcomes*);
in turn, in DFE, choice patterns deviate from the solutions of EV maximization, as if the same small-probability outcomes are given less weight than would be objectively warranted [*as-if underweighting of rare outcomes*, e.g., @barronSmallFeedbackbasedDecisions2003; @hertwigDecisionsExperienceEffect2004; @weberPredictingRiskSensitivity2004; @erevAnomaliesForecastsDescriptive2017; @regenwetterConstructbehaviorGapBehavioral2017; see @wulffMetaanalyticReviewTwo2018, for a comprehensive meta-analytic review].
More generally, in DFD, people choose as if small-probability outcomes are more probable than they objectively are, and in DFE, people choose as if the same small-probability outcomes are less probable.

What causes the description-experience gap?
A common notion is that of the mind being sensitive to the information provided by the small number of sampled outcomes that people tend to rely on in DFE [see @foxDecisionsExperienceSampling2006; @hertwigDecisionsExperienceEffect2004; @hertwigDecisionsExperienceWhy2010; @rakowBiasedSamplesNot2008; @wulffMetaanalyticReviewTwo2018].
Specifically, small samples---i.e., sets of sampled outcomes---often differ from descriptions in the information they actually provide about the prospects because they tend to underrepresent small-probability outcomes (Box \hyperlink{box}{1}, see also for illustrative purposes the `code example`, where the objective probability of outcome `6` is `.2`, and the sampled relative frequency is `.1`).
This distortion of the sampled information (sampling error) is then assumed to carry over to the decision-making process, eventually causing, ceteris paribus, distinct choice patterns in DFD and DFE.
\newline

\noindent
\fbox{
    \parbox{\textwidth}{
        \textbf{\hypertarget{box}{Box 1.}} 
        \textit{Reliance on Small Samples in DFE.}
        The binomial distribution of an outcome is the probability distribution of the number of times an outcome occurs in a sample.
        According to the \textit{de Moivre-Laplace theorem} (see, e.g., \protect\hyperlink{ref-georgiiStochastikEinfuhrungWahrscheinlichkeitstheorie2015}{Georgii, 2015}), the binomial distribution can be approximated by a symmetric normal distribution given a large sample.
        That is, the relative frequency with which an outcome occurs in a large sample is predicted to correspond to the objective probability of the outcome most of the time and is not predicted to be lower more often than higher or vice versa.
        However, for small samples, the binomial distribution associated with a small-probability outcome is positive skewed, causing the relative frequency with which this outcome occurs in the sample to be smaller rather than larger than its objective probability.
        That is, small-probability outcomes tend to be underrepresented in small samples.
        An explanation for the as-if underweighting of rare outcomes pattern may thus build on the assumption that the mind is sensitive to the information provided by small samples (\protect\hyperlink{ref-hertwigDecisionsExperienceEffect2004}{Hertwig et al., 2004}; \protect\hyperlink{ref-plonskyRelianceSmallSamples2015}{Plonsky et al., 2015}; see also \protect\hyperlink{ref-erevChoicePredictionCompetition2010}{Erev et al., 2010}, \protect\hyperlink{ref-erevAnomaliesForecastsDescriptive2017}{2017}, for the performance of models assuming reliance on small samples in prediction competitions).
    }
}
\newline
(ref:references-box-1) @georgiiStochastikEinfuhrungWahrscheinlichkeitstheorie2015; @hertwigDecisionsExperienceEffect2004; @erevChoicePredictionCompetition2010; @plonskyRelianceSmallSamples2015; @erevAnomaliesForecastsDescriptive2017

Yet, peoples' reliance on a small number of sampled outcomes in DFE, and the associated sampling error, are presumably not the sole cause of the description-experience gap [see @hauDescriptionexperienceGapRisky2008; @ungemachAreProbabilitiesOverweighted2009; @wulffMetaanalyticReviewTwo2018, who show that the as-if underweighting of rare outcomes pattern may appear, even if the overall number of sampled outcomes is large and the relative frequencies closely resemble the latent objective probabilities].
Rather, the specifics of the actual decision-making process---i.e, how the mind exercises the capacity of making decisions from experience as opposed to descriptions---might contribute to the gap.
In experimental studies of DFE using the *sampling paradigm*, people can, as much as they want, sequentially sample the outcomes of two latent prospects according to their objective probabilities before making a final, consequential choice between the prospects [see @hertwigDescriptionexperienceGapRisky2009, see also Fig.\ \@ref(fig:sampling-paradigm)].
Because in the sampling paradigm people cannot rely on the descriptions of outcomes and probabilities to make a decision, but are required to learn about the prospects from experiential sampling of outcomes over time, the tasks involved in the decision-making process differ from those in DFD.
That is, in order to model the process by which people arrive at the final choice in the sampling paradigm, one must explicate a number of assumptions concerning, in particular, the sampling process and the mind's operations on the sequence of sampled outcomes, which are of no relevance to DFD whatsoever, e.g.: How the mind

* decides when to switch between prospects during the sampling phase;

* decides when to stop sampling;

* integrates the information obtained from the sampled outcomes.

\noindent
The questions of interest are then how the mind solves these tasks and, subordinate to that,^[<!--Open footnote-->
As van Rooij and Baggio [-@vanrooijTheoryTestHow2021] put it: "Only in the manner in which we postulate that
such capacities [i.e., decision making] are exercised do our explanations of capacities come to imply effects [e.g., as-if underweighting of rare outcomes]" (p. 682, square brackets added).
<!--Close footnote-->]
whether this leads to choice patterns that are consistent with an as-if underweighting of rare outcomes pattern, irrespective of the existence or absence of sampling error.

While approaching the above questions, the current paper focuses on the effects of the potential interplay between sampling and decisions strategies.
Figure\ \@ref(fig:sampling-paradigm) displays what Hills and Hertwig [-@hillsInformationSearchDecisions2010] considered to be paradigmatic instances of both types of strategies. 
Sampling strategies refer to the sequential pattern with which people sample outcomes from the two prospects over time.
That is, in the sampling paradigm, people can decide without costs when to sample outcomes from which prospect and when to switch between them.
They can do so entirely randomly, resulting in no discernible sequential sampling pattern, or they proceed more systematically.
Hills and Hertwig [-@hillsInformationSearchDecisions2010] proposed that people may either switch just once between prospects (comprehensive sampling) or after each sampled outcome (piecewise sampling).^[<!--Open footnote-->
Note that in their analyses of empirical sampling data,  Hills and Hertwig [-@hillsInformationSearchDecisions2010] used the number of switches between prospects relative to the overall number of sampled outcomes from both prospects (switching frequency) as an approximate measure for the two proposed sampling strategies, and acknowledged that "many [sampling] strategies will fall on the continuum in between [the two paradigmatic strategies]" (p. 1788).
<!--Close footnote-->]
Decision strategies refer to the way people integrate the information obtained from the sampled outcomes in order to evaluate the prospects and make a corresponding decision.
Hills and Hertwig [-@hillsInformationSearchDecisions2010] proposed that people compare the prospects either on the basis of the means across all sampled outcomes (summary integration/decision) or they compare two sampled outcomes---i.e., one from each prospect---over repeated rounds and choose the prospect that won the majority of such comparisons (round-wise integration/decision).
In more general terms, whereas sampling strategies determine how the information about prospects are collected, decision strategies determine how the collected information are processed, or transformed, to arrive at a final choice.

\begin{figure}[t!]
\caption{Possible Sampling and Decision Strategies in the Sampling Paradigm and Their Impact on the Final Choice}
\includegraphics{manuscript_files/sampling-paradigm} \label{fig:sampling-paradigm}
\raggedright \textit{Note.} In the sampling paradigm, people are offered the choice between at least two prospects (here: violet vs. orange), whose exact properties (outcomes and probabilities) remain hidden. To learn about the properties, people can, as much as they want, sample the prospects' outcomes according to their probabilities. Sampling strategies: Refer to the sequential pattern with which people sample the outcomes from both prospects over time. People may switch infrequently (comprehensive sampling) or frequently (piecewise sampling) or something in between. Decision strategies: Refer to the way people integrate the sampled outcomes in order to evaluate the prospects. People may integrate all outcomes stemming from the same prospect in order to compute and compare summary scores (summary integration/decision); or they integrate and compare outcomes of the two prospects over repeated rounds (round-wise integration/decision).  
\end{figure}

To provide an explanation for the as-if underweighting of rare outcomes pattern and the related description-experience gap, Hills and Hertwig [-@hillsInformationSearchDecisions2010] suggested a systematic link between sampling and decision strategies.
Specifically, they assumed that comprehensive sampling is associated with making summary decisions, while piecewise sampling is associated with making round-wise decisions.
Since in round-wise decisions small-probability outcomes are expected to be left unconsidered in the majority of comparisons---each of which is weighted equally---, such a coupling could translate to an as-if underweighting of rare events pattern, even if a large number of outcomes is sampled.
That is, if the prospect that is more likely to produce a higher outcome most of the time does *not* also possess a higher EV, round-wise decisions are expected to cause deviations from the solutions of EV maximization that take the form of an as-if underweighting of rare outcomes pattern (see Fig.\ \@ref(fig:sampling-paradigm), for illustrative purposes).
Pointing to the suggested coupling of sampling and decision strategies, Hills and Hertwig [-@hillsInformationSearchDecisions2010] found that a larger proportion of the choices of people who, on average, switched infrequently between prospects were better predicted by a summary strategy (rather than a round-wise strategy), and that this predictive pattern reversed for people who switched frequently.
Moreover, simulating both round-wise and summary decisions on the basis of peoples' sampled outcomes, they showed that round-wise decisions led to a higher proportion of choices that are consistent with an as-if underweighting of rare outcomes.

To summarize the preceding paragraphs: 
Since small samples may distort the information about the properties of latent prospects, a great deal of research on the sampling paradigm has been invested into the question which factors affect how much people sample [see @wulffAdaptiveExplorationWhat2019, Table 7.2, for a recent summary of relevant factors and papers].
Assuming that people proceed systematically when sampling outcomes, and that their sampling strategies are systematically related to decision strategies, Hills and Hertwig [-@hillsInformationSearchDecisions2010] emphasize that in order to understand how people make DFE and how this comes to imply certain choice patterns, one might need a richer understanding of peoples' sampling behavior.
That is, not only we need to know how much outcomes are sampled, but also in what way they are sampled, and how this affects the way the obtained information is processed to evaluate the latent prospects.

While offering a novel, convincing inroad into the broader question how sampling and information-processing can act together to shape DFE in environments where people can sample without costs,^[<!--Open footnote-->
Meaning that people do not gain/loose the amount of each sampled outcome. Of course, sampling implies other costs, such as time and cognitive capacities.
<!--Close footnote-->]
Hills and Hertwig's [-@hillsInformationSearchDecisions2010] work invites some objections, both theoretical and methodological.
The following gaps and problems stand out:

1. *No formal modeling/unstated theoretical commitments:*
The authors provide a verbal description of their theoretical claims, but no formal specification or computational implementation thereof.
That is, they claim that sampling strategies are systematically linked to decision strategies, and that the proposed links can explain the (non)occurrence of an as-if underweighting of rare outcomes pattern.
Yet, no formal and complete accounts of the potential choice-generating mechanisms---which must contain the proposed links---are offered.
Such a lack of mathematical and/or computational modeling can, among others, hide needed auxiliary assumptions and potential gaps and inconsistencies in the theory [@guestHowComputationalModeling2021; @vanrooijPsychologicalModelsTheir2022].
For instance, one key characteristic of the sampling paradigm is that people can freely decide when to stop sampling.
Thus, in order to be considered complete, models aiming to explain how the mind exercises the capacity of making DFE in the sampling paradigm must account for stopping.
However, theoretical frameworks and their model members that do so, e.g., sequential sampling models [e.g., @markantModelingChoiceSearch2015], make their own assumptions about the decision-making process that not only have implications for what choice data is expected, but may also constrain how the proposed link between sampling and decision strategies can plausibly be integrated in a given framework (see model specifications/implementations in this paper).
Assuming that "every scientific output is model- and theory-laden (i.e., contains theoretical and modeling commitments)" [@guestHowComputationalModeling2021, p. 5], not explicitly spelling out such commitments and/or formally modeling the respective decision-making process reduces (at least) the clarity of the theoretical claims and everything that follows in the scientific inference process.

2. *No systematic consideration of intermediate and hybrid models:*
In principal, the sampling and decision strategies, as proposed, can be combined without constraints---i.e., each instance of a sampling strategy can be combined with each instance of a decision strategy.
Moreover, the discussed sampling strategies (comprehensive and piecewise sampling) are considered paradigmatic not in the sense that people are expected to adopt either one or the other sampling strategy, but in the sense that they are the ends of a continuum and that many people are expected to adopt strategies that fall in between these two (see Fig.\ \@ref(fig:sampling-paradigm)).
Focusing only on the proposed links between sampling and decision strategies, the authors do not discuss the potential implications of either one of these points for the resulting choice patterns.
That is, the presented work does not systematically consider the potential interplay between sampling and decision strategies and its effects.
In defense, one might object that the decision strategies determine how the prospects are evaluated on the basis of the sampled outcomes, and thus, once a given decision strategy is adopted, the sampling strategy upfront has no more impact on the final choice.
In other words, sampling strategies may alter which decision strategies are used, but they do not alter how a given decision strategy evaluates a prospect.
But this would be a superficial and vulnerable argument for reasons that become obvious when trying to specify or implement a plausible model of the decision-making process.
For instance, if people do not exactly follow the piecewise sampling strategy, one might still be willing to allow for the adoption of a round-wise decision strategy and, in order to make this go through, assume that a comparison-round may encompass more than one outcome sampled from each prospect.
In this case, however, sampling strategies can have a severe impact on the evaluation of prospects because they alter the size of the samples that a round encompasses (see analyses in this paper).
One may otherwise waive the assumption that a comparison-round encompasses more than one outcome per prospect, but this can cause the decision-making process to take a form that is fundamentally different from how it might have been perceived---at least implicitly according to unstated theoretical commitments---when only the paradigmatic instances were considered.
In sum, to understand whether and how sampling and decision strategies act together to shape DFE, one should not only explicitly model their links, but also consider their interplay broadly and systematically.

3. *Use of heuristic analysis methods:*
The empirical re-analyses relies on various heuristic methods to assess whether the observed choices are consistent with the claimed links between sampling and decision strategies and their impact on the as-if weighting of rare events pattern.
For instance, aggregated data over participants and choice problems is used to compare, within and between sampling strategies, the proportions of choices that are consistent with the predictions of the decision strategies and an as-if underweighting of rare outcomes.
In addition, to obtain indicators for the applied sampling strategies, each participant's mean switching frequency across all choice problems was calculated and a median split was used to identify infrequently and frequently switching people, supposedly indicating comprehensive and piecewise sampling, respectively.
While acknowledging that existing data was used, such heuristic methods can be problematic, e.g., because they can cause a construct-behavior gap where inferences from the data to the tested theory, although statistically sound, become logically invalid.
In a rigorous analysis, @regenwetterConstructbehaviorGapBehavioral2017 demonstrate how the use of aggregate data and heuristic scoring rules, e.g., counting the proportion of correct choice predictions across problems and people, lead at least to hard-to-interpret inferences about individuals' as-if weighting patterns and the description-experience gap.
One can alleviate such risks by taking heterogeneity between individuals into account and modeling the as-if weighting more explicitly.
Also, the averaging of switching frequencies across choice problems to group into frequent and infrequent switchers can be falsely interpreted as if sampling strategies are more a characteristic of the decision maker than of the sampling process.
Finally, sampling error is only controlled for to the extent that choice trials in which small-probability outcomes were not sampled at least once are excluded from the analyses.
This still leaves room for the sampled relative frequencies to deviate from the latent objective probabilities.

4. *No discussion of choice problems:*
Assuming representative sampled frequencies, summary and round-wise decisions only predict different choices, if the prospect with the larger EV is not also more likely to produce a higher outcome most of the time (see Fig.\ \@ref(fig:sampling-paradigm)).
In consequence, the structure of the choice problems---i.e, whether a small-probability outcome exists and how desirable it is---can quite dramatically shape how the (interplay of) strategies differ in the choices and as-if weighting patterns they produce.
Focusing only on problems of such a structure may provide a parsimonious but narrow-in-scope interpretation. 

The current paper aims to extend the work of @hillsInformationSearchDecisions2010 and to address the outlined problems.
The rest of the paper is structured as follows:
In the first section, the current approach and its theoretical commitments are briefly outlined.
The paper starts from the proposition that systematic choices in DFE must be based on the assessment of differences in the latent properties of prospects (e.g., differences in EV or differences in the probability to produce a higher outcome most of the time).
Sequential sampling models of decision making are considered as a theoretical framework that can be used to explain how the mind samples and accumulates evidence about such differences over time, and---importantly---how the mind stops sampling once an evidence threshold is reached.
In the section that follows, two models are specified, each of which integrates the continuum of sampling strategies and one of the proposed decision strategies into the computational framework of sequential sampling models.
The expected behavior of the models are outlined.
Thereafter, a simulation study is presented, where the proposed models are used to simulate the sampling and accumulation processes for a set of choices between a safe and a two-outcome prospect, the latter of which may or may not possess a small-probability outcome of different rank/desirability.
Controlling for sampling error---i.e., considering the sampled relative frequencies rather than the latent objective probabilities---, this paper then seeks to investigate in a computational analysis whether and how the potential interplay of sampling and decision strategies shapes DFE and comes to imply choice patterns and functional forms of cumulative prospect theory [CPT, @tverskyAdvancesProspectTheory1992] that are consistent with an as-if underweighting of rare outcomes pattern.^[<!--Open footnote-->
CPT's value function is also considered in this paper.
However, the focus is on the weighting function, as it is commonly used to describe an as-if overweighting and underweighting of rare outcomes, and the description-experience gap [see, e.g., @kellenHowVariantAre2016; @regenwetterConstructbehaviorGapBehavioral2017; but see @hertwigConstructbehaviorGapDescription2018].
<!--Close footnote-->]
It is shown that within a sequential sampling framework, the interplay of sampling and integration/decision strategies produces systematically different choice patterns that depend on the structure of the choice problems and translate to distinct signatures in CPT's weighting and value function.
The simulated choices and estimated CPT parameters can be understood as model-implied data and data models, which remain to be tested against empirical data.
The paper closes with a brief discussion and conclusion.
Note that all materials---i.e., code and data---underlying this paper, including instructions on how to reproduce this manuscript, can be retrieved from the accompanying `GitHub` repository.

# Sampling and Decision Strategies Within a Sequential Sampling Framework

Because there is an uncountable number of decisions people confront in their daily life, behavioral decision research routinely abstracts from choices between particular alternatives, e.g., the choice between job offers, political parties to vote for, investment plans, and whatnot.
Rather, with the choice between at least two prospects, it studies a case which is counterfactual in that it omits the many particularities of each choice situation, but retains in the form of prospects the fundamental properties that almost all choice alternatives are assumed to possess.
These fundamental properties are considered the $n$ possible outcomes of a choice alternative, denoted as $x_1, ..., x_n$, and the probabilities with which these outcomes occur following the choice of the alternative, denoted as $p_X(x_1), ..., p_X(x_n)$. 
That is, each choice alternative that is fully described by a set of outcome-probability pairs, $\{(x_i, p_X(x_i))\}_{i \in \{1, ..., n\}}$, satisfying $\sum_{i = 1}^n p_X(x_i) = 1$, is a prospect.
It is only through these fundamental properties, or combinations thereof, that any necessary distinction between the prospects can be made.
Hence, systematic---as opposed to random---choices between prospects must be based on the assessment of differences in their properties.

Rarely in peoples' daily life, the information provided about the prospects take the form of explicit and complete descriptions of all their outcome-probability pairs.
In such rare cases, people would make DFD and can---strongly simplified---directly proceed by selecting a property or combinations thereof to assess differences among the prospects and make a respective choice.
In stark contrast and rather often, people are required to make DFE, where the mind can learn about the latent properties only by "experiential sampling [of outcomes] over time" [@hertwigDescriptionexperienceGapRisky2009, p. 517, square bracket added].
The sampling paradigm is just one paradigm that can be used to study DFE experimentally and allows people to sample the prospects' outcomes without costs and without constraints regarding the sequential pattern and the amount of sampled outcomes.

How do people arrive at a final choice in the sampling paradigm? 
Sequential sampling models of decision making under risk (DFD) and/or uncertainty [DFE, see @markantModelingChoiceSearch2015, who explicitly consider DFE and the sampling paradigm; see also, e.g., @bhatiaSequentialSamplingParadoxes2014; @busemeyerDecisionFieldTheory1993] assume that the decision-making process takes the form of an accumulation of evidence for the prospects over time.
The process is assumed to stop---i.e., a choice is made---once the accumulated evidence for one of the prospects reaches an evidence threshold.
More specifically, for DFE, it is assumed that the outcomes of prospects are sequentially sampled and integrated into dynamic decision variables and that a choice is made once the value of a decision variable exceeds a threshold.
How exactly this integration is carried out and what the value of a dynamic decision variable represents, is a theoretical question---and thus a matter of the exact specification of the model---largely concerned with the way the prospects are assumed to be evaluated.
Depending on the integration and evaluation mechanisms they assume, the member models of the sequential sampling framework have been shown to explain some of the robust deviations from EV maximization, including the as-if underweighting of rare outcomes in the sampling paradigm [@markantModelingChoiceSearch2015].
Moreover, these models implement speed-accuracy trade-offs, with the amount of evidence that must be sampled in favor for a prospect increasing with thresholds, leading to more accurate but slower choices for high thresholds, and vice versa.

The current paper commits to the core assumptions of the sequential sampling framework and uses its flexibility regarding the exact mechanisms of sampling, integration, and evaluation to incorporate the idea of sampling and decision strategies, including their interplay.
The two models presented below, one assuming an integration and evaluation mechanism resembling the round-wise strategy, the other assuming mechanisms resembling the summary strategy, offer a formal route to explore the potential interplay between sampling and integration/decision strategies, and their effects on the choice behavior.
The sequential sampling framework is chosen as a modeling outlet not only because it makes parsimonious assumptions about the decision-making process and accounts for the behavior in various decision-making tasks [@ratcliffComparisonSequentialSampling2004], but primarily because it makes---taking a Marrian perspective [cf. @marrVisionComputationalInvestigation1982; @marrUnderstandingComputationUnderstanding1977; see also, e.g., @griffithsProbabilisticModelsCognition2010]---explicit a core part of the computational problem the mind is trying to solve in DFE, and the principle logic that governs the solution:
In order to make systematic choices between prospects, the mind must assess true differences in their *latent* properties, and it can only do so by searching for evidence for such differences in information samples.
Sequential sampling models of DFE make this clear by assuming an evidence accumulation process over sampled outcomes that is more or less directed towards particular latent properties.
Round-wise and summary decisions are both indicative for distinct latent properties, i.e., the expected value and the probability to produce a higher outcome most of the time, respectively.
The remainder of the paper discusses and demonstrates in a simulation, how integration/decision strategies can act together with sampling strategies and evidence thresholds to alter the degree and the accuracy with which differences about either one of these properties are assessed.

# Models 

Let us consider the sampling paradigm and a choice between two prospects, $P^X$ and $P^Y$.
Let $P^X$ be a probability---or pushforward---measure assigning to each possible outcome $x$ of the random variable $X$ a probability $p_X(x)$.
$P^Y$ is defined in a similar way.
Graphical sketches of the round-wise and summary integration model are displayed in Figure\ \@ref(fig:models).

\begin{figure}[t!]
\caption{Integration of Sampling and Integration/Decision Strategies into a Sequential Sampling Model}
\includegraphics{manuscript_files/models} \label{fig:models}
\raggedright \textit{Note.} In both models, to arrive at a final choice, it is assumed that outcomes from two prospects (here: violet vs. orange, see Fig.~\ref{fig:sampling-paradigm}, for details), are sequentially sampled and integrated
into dynamic decision variables until one of the variables reaches a threshold (not displayed). Round-wise integration model: The value of the decision variable is the cumulative sum of won mean comparisons. Each mean comparison encompasses the outcomes that were sampled from the same prospect in direct succession (without a switch in between). The thresholds govern the required (difference in the) number of won comparisons. Summary integration: The value of the decision variable is the cumulative sum across all outcomes sampled from a prospect. The thresholds govern the required (difference in the) cumulative sums. Both integration models allow for differences in thresholds and switching probabilities.  
\end{figure}

## Round-Wise Integration Model

The model assumes that the sampling process starts at random with a stochastic process on one of the two prospects.
Specifically, it is assumed that an agent starts with equal probability to sample from one of the prospects, say $P^X$, and generates a sequence of sampled outcomes, denoted by the random variables $X_1, X_2,...$, where the mean over this sequence is computed or updated with each new sampled outcome.
The respective stochastic process $\{S^X_t\}_{t \in \mathbb{N}}$ is defined by
$$
S^X_t = \frac{1}{t} \sum_{k = 1}^t X_k, \quad k,t \in \mathbb{N}
\; ,
\tag{1}
$$
where $S^X_t$ is the mean across $t$ outcomes that were *successively* sampled from prospect $P^X$.
Each such stochastic process terminates as soon as an outcome is sampled from the other prospect, i.e., the stochastic process on prospect $P^X$ stops as soon as an outcome is sampled from prospect $P^Y$, and vice versa.
The switching probability $\psi \in (0,1]$, which is assumed to be adopted prior the start of the sampling and accumulation process, is fixed throughout and controls the probability with which outcomes from different prospects are sampled in direct succession.
In other words, the switching probability $\psi$ governs the probability with which the stochastic process on a given prospect, defined as in (1), stops and a new stochastic process on the other prospect starts with the subsequently sampled outcome.

To implement the evidence accumulation process, the round-wise integration model assumes that each time after a *new* stochastic process was started and terminated on both prospects, their respective means are compared, with the prospect underlying the stochastic process with the greater mean receiving a round win.
Accordingly, assuming that $S^Y_m$ is the mean across $m$ outcomes successively sampled from prospect $P^Y$, this stage is modeled for the prospect $P^X$as the mapping
$$
\begin{aligned}
  Z^X : \mathbb{R} &\to \mathbb{N} \\
  S^X_t - S^Y_m &\mapsto 
  \begin{cases}
     1 &\text{if} \qquad  S^X_t > S^Y_m, \\
     0 &\text{else}
     \; ,
  \end{cases}
\end{aligned}
\tag{2}
$$
where $Z^X$ is a random variable that takes only the two values $0$ and $1$, denoting a lost or won mean comparison, respectively.
Let us denote the steps defined in (1) and (2) as a round of mean comparison (short: *comparison round*, see Fig.\ \@ref(fig:models)).

The model assumes that such comparison rounds are repeatedly carried out until the cumulative sum of round wins for one of the two prospects reaches a threshold $\theta \in \mathbb{N}$.
Similar to the switching probability, it is assumed that the threshold $\theta$ is adopted prior the start of the sampling and accumulation process and is fixed throughout.
For $P^X$, then, one obtains a sequence of mean comparisons, denoted as $Z^X_1, Z^X_2, ...$, and an evidence accumulation process taking the form of a random walk $\{D^X_v\}_{v \in \mathbb{N}}$ defined by
$$
D^X_v = \sum_{c = 1}^v Z^X_c, \quad c,v \in \mathbb{N}
\;.
\tag{3}
$$
$D^X_v$ is the cumulative sum of round wins of prospect $P^X$ after $v$ comparison rounds, and resembles a dynamic decision variable.
Note that since (2) describes a Bernoulli trial, the random walk in (3) is based on a Bernoulli process.
Prospect $P^X$ would be chosen once $D^Y_v < \theta \leq D^X_v$ is satisfied, and vice versa.

Finally, note that the described model assumes two random walks---i.e., one for each prospect---approaching the same threshold, which is the absolute number of mean comparisons a prospect must win in order to be chosen.
However, one may also define a single random walk, resembling a common dynamic decision variable, which approaches a positive threshold $\theta^+$ and a negative threshold $\theta^-$.
That is, (2) may be adapted such that $Z$ takes the value $-1$ instead of $0$, such that the threshold determines how many more mean comparisons a prospect must win in order to be chosen.

## Summary Integration Model

The model assumes that an agent starts with equal probability to sample outcomes from one of the prospects and continues sampling while switching between prospects according to the switching probability $\psi$.
As in the round-wise integration model, the switching probability $\psi$ is assumed to be adopted prior the start of the sampling and accumulation process and is fixed throughout.

To implement the evidence accumulation process, the summary integration model assumes that an agent continues sampling from both prospects until the cumulative sum across all outcomes sampled from one of the prospects reaches a threshold $\theta$. 
As in the round-wise integration model, the threshold $\theta$ is assumed to be adopted prior the start of the sampling and accumulation process and is fixed throughout.
For $P^X$, then, one obtains a sequence of sampled outcomes, $X_1, X_2, ...$, and an evidence accumulation process taking the form of a random walk $\{D^X_t\}_{t \in \mathbb{N}}$ defined by 
$$
D^X_t = \sum_{k = 1}^t X_k, \quad k,t \in \mathbb{N}
\; ,
\tag{4}
$$
where $D^X_t$ is the cumulative sum across $t$ outcomes sampled from prospect $P^X$ and resembles a dynamic decision variable.
Assuming that $D^Y_m$ is the cumulative sum across $m$ sampled outcomes from prospect $P^Y$, prospect $P^X$ would be chosen once $D^Y_m < \theta \leq D^X_t$ is satisfied, and vice versa.

Note again that the described model assumes two random walks---i.e., one for each prospect---approaching the same threshold, which now is the cumulative sum across all sampled outcomes a prospect must produce in order to be chosen.
However, one may also define a single random walk, resembling a common dynamic decision variable, which approaches a positive threshold $\theta^+$ and a negative threshold $\theta^-$.
That is, (4) may be substituted by
$$
D^X_{t,m} = \sum_{k = 1}^t X_k - \sum_{j = 1}^m Y_j \quad k,t,j,m \in \mathbb{N}
\; ,
\tag{5}
$$
where $D^X_{t,m}$ is the difference between the cumulative sums across $t$ and $m$ outcomes sampled from prospects $P^X$ and $P^Y$, respectively.
The thresholds thus determine how much larger the cumulative sum of a prospect must be in order for the prospect to be chosen.

# What Model Behaviors Are Expected?

Below, it is discussed which effects the key model parameters, i.e., the switching probability $\psi$ and the threshold $\theta$, could have on the choices and the corresponding as-if weighting pattern that are implied by the round-wise and the summary integration model.
The effects of the switching probability $\psi$ should be informative about the interplay between different sampling strategies and the integration/decision strategy each model resembles.
Assuming that both models aim to assess true differences in the latent properties of prospects, the effects of the threshold $\theta$ should indicate whether these differences are indeed more accurately assessed---thus, leading to more systematic choices and as-if weighting patterns---the more evidence is accumulated.
In addition, previous research has shown that differences as to how information about the properties of prospects is processed can translate to distinct signatures in CPT's psychoeconomic value and weighting function, without actually assuming these functions to be computed whatsoever [see @pachurHowTwainCan2017; @stewartDecisionSampling2006; @zilkerNonlinearProbabilityWeighting2021].^[<!--Open footnote-->
Note that it has been argued that the distinction between CPT's functions used to describe choice data and models aiming to explain how this data is generated in first place are reminiscent of Marr's [-@marrVisionComputationalInvestigation1982] different levels of explanation [@zilkerMeasuringModelingConstruction2020; also cf. @griffithsProbabilisticModelsCognition2010].
That is, whereas CPT may be considered a computational-level theory that describes an abstract solution to the problem the mind aims to solve when offered a choice between prospects, process models [cf. @jareckiFrameworkBuildingCognitive2020] such as heuristics and sequential sampling models may be considered algorithmic-level theories that specify how the mind can approximate the abstract solution with the capacity limits it possesses.
<!--Close footnote-->]
Accordingly, this section also formulates expectations regarding the potential relations between the switching probability $\psi$ and threshold $\theta$ (data-generating parameters) and the CPT parameters (data-modeling parameters).
Summaries of the expected model behaviors can be found in Tables\ \@ref(tab:expectations-roundwise) and \@ref(tab:expectations-summary).

## Length of Sampling and Accumulation Processes and Final Choice

### Round-Wise Integration Model

The required number of independent mean comparisons increases with thresholds $\theta$.
Since each comparison is based on its own pair of stochastic processes, an increase in $\theta$ translates to a larger number of required stochastic processes and, ceteris paribus, therefore also to a larger overall number of outcomes sampled over the course of a sampling and accumulation process.
In turn, each stochastic process is predicted to terminate faster, the higher the probability that the subsequent outcome is sampled from the other prospect, determined by the switching probability $\psi$.
Accordingly, an increase in $\psi$ leads to shorter stochastic processes, which translate, ceteris paribus, to a smaller overall number of sampled outcomes.

Because the switching probability $\psi$ is predicted to alter the length of the stochastic processes underlying each mean comparison, $\psi$ should affect whether low-probability outcomes contribute to the majority of the required mean comparisons about as much as would be warranted based on their objective probabilities, or whether they contribute rather less.
That is, for short stochastic processes, the binomial distribution associated with small-probability outcomes is positive skewed, therefore causing the relative frequencies with which these outcomes occur over the course of such a process to be smaller rather than larger than their latent objective probabilities.
Respectively, if the small-probability outcome is smaller (larger) than the EV, high values of $\psi$---and therefore pairs of short stochastic processes---cause the mean to be an inflated (deflated) estimate of the EV.
Such an inflation and deflation of the means can then translate to an as-if underweighting of rare outcomes.
Specifically, if the mean of the prospect with the lower (higher) EV is inflated (deflated), the outcome of the comparison between the means may be a reversal---in sign---of the outcome of a comparison between the EVs, leading to choices that take the form of an as-if underweighting of rare outcomes.
Yet, if the mean of the prospect with the lower (higher) EV is deflated (inflated), the difference between the means should be larger than the differences between the EV, which can simplify EV maximization [see @hertwigDecisionsExperienceWhy2010].
In turn, for low values of $\psi$---and therefore pairs of long stochastic processes---the means should closely correspond to the EVs, leading to choices that are in line with EV maximization.

To summarize the choice predictions and simplify: 
If the prospect that is more likely to return a higher outcome most of the time does not also possess the higher EV, high values of $\psi$---and therefore pairs of short stochastic processes---are predicted to cause an as-if underweighting of rare outcomes.
While these predictions emphasize the close link between small samples and the as-if underweighting of rare outcomes, they suggest that the reliance on multiple or many small samples can serve the accumulation of evidence for differences in a latent property.
Specifically, for an increasing sequence of independent comparisons between single outcomes sampled from two prospects, the relative winning frequency of a prospect should converge against the objective probability of producing a higher outcome in each comparison.
Respectively, then, the as-if underweighting of rare outcomes should become more robust as the thresholds $\theta$ increase.^[This clarifies the role of $\theta$: Whatever systematic differences between the prospects is reflected in a mean comparison should be more accurately assessed for high values of $\theta$.]

### Summary Integration Model

[...]

## Cumulative Prospect Theory

Before elaborating how the expectation outlined above may translate into estimates of the CPT parameters, the following section reviews how CPT in general and its weighting function in particular capture deviations from EV maximization.

### Description of CPT

The actual choices people make between prospects are often *described* in terms of deviations from EV maximization, according to which the prospect with the largest EV
$$
EV = \sum_{i \in \mathbb{N}}^n x_i \times p_X(x_i)
\tag{6}
$$ 
should be chosen.
To describe how people's choices deviate from EV maximization [see @erevAnomaliesForecastsDescriptive2017, for a recent replication of classical demonstrations; see also @kahnemanProspectTheoryAnalysis1979; @tverskyAdvancesProspectTheory1992], CPT and similar rank-dependent models [see @stottCumulativeProspectTheory2006, for an overview of models] fit choice data by assuming that people maximize the valuation
$$
V = \sum_{i \in \mathbb{N}}^n v(x_i) \times \pi_i
\; ,
\tag{7}
$$ 
with objective outcomes $x_i$ being transformed by a value function $v$, and *cumulative decision weights* $\pi_i$ being determined by the difference between transformed cumulative probabilities of the distribution $P$.
More specifically, following @tverskyAdvancesProspectTheory1992, the objective outcomes are transformed by a value function
$$
\begin{aligned}
  v : \Omega' &\to \mathbb{R} \\
  x &\mapsto 
  \begin{cases}
     x_i^\alpha &\forall x_i \geq 0, \\
     -\lambda |x_i|^\alpha &\text{else}
     \; ,
  \end{cases}
\end{aligned}
\tag{8}
$$
with $\alpha \in [0,1]$ determining the degree of the function's concavity (convexity) over the positive (negative) outcome interval, and $\lambda > 1$ increasing the function's slope over the negative outcome interval only.
Each subjective value $v(x)$ is then multiplied (or: weighted) with a cumulative decision weight that takes the form
$$
\pi_i =
  \begin{cases}
     w^+(P(X \geq x_i)) - w^+(P(X > x_i)) \quad \forall x_i \geq 0, \\
     w^-(P(X \leq x_i)) - w^-(P(X < x_i)) \quad \text{else}
     \; , 
  \end{cases}
\tag{9}
$$
where $w$ is a monotonic increasing, nonlinear weighting function satisfying $w^+(0) = w^-(0) = 0$ and $w^+(1) = w^-(1) = 1$.
To simplify matters, the further discussion and the computational analysis are constrained to choices between a safe prospect---possessing just one outcome that occurs with certainty---and a two-outcome prospect (hereafter: risky prospect).
All outcomes are assumed to be positive and the safe outcome is assumed to fall between the the low-rank outcome, $x_{low}$, and the high-rank outcome, $x_{high}$, of the risky prospect to omit dominance.
One can then disregard the negative parts of (8) and (9), including the parameter $\lambda$ of the value function, and (9) simplifies to
$$
\begin{aligned}
  \pi_{high} &= w(p_x(x_{high})) \\
  \pi_{low} &= w(p_x(x_{low}) + p_x(x_{high})) - w(p_x(x_{high})) \\
  &= 1 - \pi_{high}
  \; .
\end{aligned}
\tag{10}
$$

Essentially, the model is such that the cumulative decision weights derived for the outcomes of the risky prospect may be greater or smaller than their objective probabilities, causing the transformed outcomes---i.e., the subjective values---to be over- or underweighted *within* CPT, respectively.
Accordingly, for the description of choices, one may adopt an as-if weighting terminology by stating that people choose as if they maximized the value in (7) and applied the weighting pattern that was estimated in CPT.
Importantly, the as-if prefix indicates that there is no claim that the mind indeed performs any of the computations associated with the weighting of subjective values; 
rather, the mind processes the information about the properties of choice alternatives in a way that the resulting choices translate to the estimated weighting pattern [cf. @gigerenzerHowExplainBehavior2020].
Accordingly, once again, this paper treats CPT as a data model and discusses---and refines in a simulation---what weighting patterns can be expected, were the mind to carry out the sampling and accumulation processes that are assumed by the presented round-wise and summary integration model [see @pachurHowTwainCan2017; @zilkerNonlinearProbabilityWeighting2021, for similar theoretical inquiries].

Now, several consequences for the weighting patterns that can be estimated in CPT follow from the exact specification of the weighting function $w$, which are briefly reviewed next:
Therefore, Figure\ \@ref(fig:weighting-function) illustrates some of the possible graphical shapes of the two-parameter weighting function of @goldsteinExpressionTheoryPreference1987, which, however, is just one of several parameterizations that have been proposed [e.g., @prelecProbabilityWeightingFunction1998; @tverskyAdvancesProspectTheory1992; see @stottCumulativeProspectTheory2006, for an overview].
Each graph in Figure\ \@ref(fig:weighting-function) displays the graphical shape of the weighting function
$$
\begin{aligned}
  w : [0,1] &\to [0,1] \\
  p_X(x_{high}) &\mapsto \frac{\delta \times (p_X(x_{high}))^{\gamma}}
  {\delta \times (p_X(x_{high}))^{\gamma} + (1-p_X(x_{high}))^{\gamma}}
  \; ,
\end{aligned}
\tag{11}
$$
for the respective values of the parameters $\gamma \in [0,2]$ and $\delta > 0$.
Evidently, both parameters have distinct effects on the graphs' shape, with $\gamma$ affecting the curvature and $\delta$ the elevation.
[As a consequence from (13), then, each combination of parameters implies a particular weighting pattern, with some of them being similar and others being rather distinct.]

```{r weighting-function, include=FALSE}
#compute images of weighting function (wf)
wf <- tibble(p = seq(0, 1, .01)) %>%  #cumulative probabilities
  expand_grid(gamma = seq(.1, 2, .1), #gamma values
              delta = c(.1, .5, 1, 2, 5, 10)) %>% #delta values
  mutate(wp = (delta*(p^gamma))/((delta*p^gamma)+(1-p)^gamma)) #images of wf

#labeller function for facet labels with LateX math expressions 
label_delta <- function(string) {
  TeX(paste("$\\delta=$", string, sep = ""))  
}

#plot shapes of weighting function
wf %>% 
  ggplot(aes(p, wp, group = gamma)) +
  facet_wrap(~delta, labeller = as_labeller(label_delta, default = label_parsed)) + 
  scale_x_continuous(breaks = seq(0, 1, .5)) +
  scale_y_continuous(breaks = seq(0, 1, .5)) + 
  labs(x = "Probability of High-Rank Risky Outcome",
       y = "Transformed Probability / Decision Weight", 
       color = expression(gamma)) + 
  theme_minimal() + 
  geom_line(aes(color = gamma), size = .5) +
  scale_color_viridis(option = "inferno") + 
  geom_abline(intercept = 0, slope = 1, color = "gray", linetype = "dashed")
```

\begin{figure}[t]
\caption{Possible Graphical Shapes of Goldstein and Einhorn's (\protect\hyperlink{ref-goldsteinExpressionTheoryPreference1987}{1987}) Weighting Function}
\includegraphics{manuscript_files/figure-latex/weighting-function-1} \label{fig:weighting-function}
\raggedright \textit{Note.} Add note
\end{figure}

More specifically: The gray-dashed identity lines in Figure\ \@ref(fig:weighting-function) imply a linear weighting pattern.
That is, since the images of $w$, $w(p)$, are equal to the respective cumulative probabilities $p$, all cumulative decision weights $\pi$ that are derived according to (13) would be equal to the differences between the respective objective cumulative probabilities---and therefore also to the objective probabilities of the outcomes for which the weights are determined.
Such a linear weighting pattern is implied by $\gamma = 1$ and $\delta = 1$. 
Now, both deviations from $\gamma = 1$ and $\delta = 1$ produce a nonlinear graphical shape.
That is, for $\gamma > 1$, the graphs take a S-shape---running below (above) the identity line for small (large) cumulative probabilities---which is more accentuated for larger deviations from $\gamma = 1$.
Importantly, the same *small* numerical difference between two cumulative probabilities $p$ then translates to varying differences between the respective images $w(p)$ depending on the interval on the cumulative probability scale. 
Specifically, because of the S-shaped curvature, the differences between the images $w(p)$ may be smaller than those between the respective cumulative probabilities in the lower and upper part of the cumulative probability scale, but may be considerably greater in the middle part.
As a consequence, the cumulative decisions weights are smaller than the respective objective probabilities for small-probability outcomes of low and high rank, but they are larger for small-probability outcomes of a middle rank.^[<!--Open footnote-->
Note that the distinction between outcomes of either low/high rank or middle rank is unnecessary for at most two-outcome prospects [see @tverskyAdvancesProspectTheory1992, see also below].
However, for finite many outcomes, the distinction is critical. More so, the fact that CPT may weigh outcomes of the same small probability differently, adds another reasons why the description-experience gap should not be equated with a reversed weighting pattern in CPT [see @hertwigConstructbehaviorGapDescription2018].
I.e., neither could one interpretation of the gap be in terms of an as-if underweighting (overweighting) of rare outcomes in DFE (DFD), nor would any of the potential contributors to the gap considered so far [cf. @hertwigDescriptionexperienceGapRisky2009; @wulffMetaanalyticReviewTwo2018] treat some small-probability outcomes differently than other small-probability outcomes.
<!--Close footnote-->]
Note that for all $\gamma < 1$ the graph of $w$ takes an inverse S-shape and the entire weighting pattern is reversed.
Moreover, the shape of the weighting function, and thus the weighting pattern, depends strongly on $\delta$, which affects the overall elevation of the graph.
That is, for $\delta < 1$, the interval on the cumulative probability scale over which the weighting function runs below the identity line is greater than the interval over which the weighting function runs above the identity line; and this pattern reverses for $\delta > 1$.
In other words, $\delta$ shifts across the entire cumulative probability scale the intervals within which small differences between objective cumulative probabilities translate to either smaller or greater cumulative decision weights.

To summarize the preceding paragraphs: CPT is a data model that can be used to describe choices between prospects by capturing systematic deviations from EV maximization in the parameter estimates of its value and weighting function.
Because the graph of the weighting function usually takes a more or less accentuated (inverse) S-shape, the implied weighting pattern is such that small probability outcomes of low and high rank are underweighted (overweighted) in CPT's valuation of a prospect according to (11).

### How Is CPT Expected to Reflect the Model Predictions?


A summary of the predictions for the CPT parameters can be found in Table\ \@ref(tab:predictions).

To begin with, note through simplifying (13) to
$$
\begin{aligned}
  \pi_{high} &= w(p_x(x_{high})) \\
  \pi_{low} &= w(p_x(x_{low}) + p_x(x_{high})) - w(p_x(x_{high})) \\
  &= 1 - \pi_{high}
  \; ,
\end{aligned}
\tag{15}
$$
that the possible weighting patterns amount to an over- and underweighting of the high-rank outcome of the risky prospect, depending on its probability.
That is, the weighting function is supplied with the probability of the high-rank outcome of the risky prospect and its images are the decision weights assigned to this outcome.
Respectively, as can be seen from Figure\ \@ref(fig:weighting-function), for $\gamma > 1$, the high-rank outcome of the risky prospect is overweighted (underweighted) in CPT, if it is of large (small) probability, and $\delta \neq 1$ shifts the range of the over- and underweighting.

#### Round-Wise Integration Model

Now, since for low values of $\psi$ the model tends to choose the prospect possessing the larger EV, the parameters of the value and weighting function in this case are predicted to correspond to a linear weighting of outcomes and probabilities, which is given for $\alpha = 1$, $\gamma = 1$, and $\delta = 1$.
However, for high values of $\psi$, the model tends to choose the risky prospect for $p_x(x_{high}) > .5$---i.e., the risky prospect produces the higher outcome most of the time---, irrespective of the differences in the EV.
Vice versa, the model tends to choose the safe outcome for $p_x(x_{high}) < .5$---i.e., the safe prospect produces the higher outcome most of the time.
Respectively, for high values of $\psi$, the weighting pattern should be such that the high-rank outcome of the risky prospect is overweighted for $p_x(x_{high}) > .5$, and underweighted otherwise.
Hence, the graph of the weighting function should take a S-shape, which is given for $\gamma > 1$.
Because the over- and underweighting of the high-rank outcome is not predicted to extend across the mid-point of the probability scale, $\delta$ should take values around 1.
Note that the graph of the weighting function for high values of $\psi$ reflects the round-wise strategy's high sensitivity to differences in the probability to produce the higher outcome most of the time:
Specifically, the slope of the weighting function's graph is most severe around the midpoint of the probability scale.
The predicted flat ends of the graph, however, indicate that the round-wise strategy more or less ignores how much larger the probability to produce a higher outcome is. 
In sum, then, we do not predict $\delta$ to change with $\psi$.
However, we expect $\gamma$ to increase with $\psi$, thereby reflecting differences in the extent to which the prospect with the higher EV or with the probability to produce a higher outcome most of the time is chosen.

Moreover, because the model is such that the magnitudes of outcomes are ignored once a comparison is carried out, increases in $\psi$ should lead to a more concave value function.
That is, the higher the values of $\psi$, the more the choices depend on which prospect produces the higher outcome most of the time, irrespective of how much larger the outcomes are.
Respectively, $\alpha$ is predicted to decrease with increases in $\psi$.

(ref:expectations-roundwise) Expected Effects of Switching Probability $\psi$ and Threshold $\theta$ in the Round-Wise Integration Model

```{r expectations-roundwise}
exp_roundwise <- tibble("Measure" = 
                        c("Choices pattern",
                          "",
                          "$\\gamma$ (curvature of $w$)",
                          "$\\delta$ (elevation of $w$)",
                          "$\\alpha$ (concavity of $v$)"),
                      "Predictions for changes in $\\psi$" = 
                        c("More EV maximization for decreasing $\\psi$",
                          "More as-if underweighting of rare outcomes for increasing $\\psi$",
                          "$\\geq 1$, increases with $\\psi$",
                          "1, no effect of $\\psi$",
                          "$\\leq 1$, decreases with increasing $\\psi$"))

apa_table(exp_roundwise, caption = "(ref:expectations-roundwise)", note = "The predictions should become more robust for high values of $\\theta$.", escape = FALSE)
```

#### Summary Integration Model

[...]

(ref:expectations-summary) Expected Effects of Switching Probability $\psi$ and Threshold $\theta$ in the Summary Integration Model

```{r expectations-summary}
exp_summary <- tibble("Measure" = 
                        c("Choices pattern",
                          "",
                          "$\\gamma$ (curvature of $w$)",
                          "$\\delta$ (elevation of $w$)",
                          "$\\alpha$ (concavity of $v$)"),
                      "Predictions for changes in $\\psi$" = 
                        c("More EV maximization for decreasing $\\psi$",
                          "More as-if underweighting of rare outcomes for increasing $\\psi$",
                          "$\\geq 1$, increases with $\\psi$",
                          "1, no effect of $\\psi$",
                          "$\\leq 1$, decreases with increasing $\\psi$"))

apa_table(exp_summary, caption = "(ref:expectations-summary)", note = "The predictions should become more robust for high values of $\\theta$.", escape = FALSE)
```


# Simulation Study

To substantiate and refine the predictions of the model, a computational implementation was used to simulate the sampling and accumulation process for a set of choices between a safe prospect and a two-outcome (risky) prospect.
The computational model was written in R and is embodied in the GitHub repository accompanying this paper.
The simulation was conducted for varying combinations of $\psi$ and $\theta$.
For each of these combinations, the simulated data was modeled with a stochastic version of CPT, using Bayesian parameter estimation.
The simulated choices and estimated CPT parameters can be roughly understood as theory-implied data and data models, which remain to be tested against empirical data [see, e.g., @guestHowComputationalModeling2021; @haslbeckModelingPsychopathologyData2021].

## Method

Below, the simulation and modeling procedure is briefly described.

### Choice Problems

A test set of $60$ choice problems was obtained by stratified sampling from an initial set of $10,000$ problems.
The stratification procedure was used to ensure that the test set contained choice problems with small-probability outcomes of different rank---i.e., small-probability outcomes that are either higher or lower than the EV of the risky prospect---as well as choice problems without small-probability outcomes.
The exact procedure was as follows:
For each of the $10.000$ problems in the initial set, three outcomes were randomly drawn from a uniform distribution over the interval $[0, 20]$, with the smallest and highest of these three outcomes being assigned to the risky prospect to omit dominance.^[I.e., when one prospect always produces a higher outcome.]
The probability for the lower outcome of the risky prospect, $p_x(x_{low})$, was drawn from a uniform distribution over the interval $(0, 1)$, with the probability of the higher outcome being set to $p_x(x_{high}) = 1-p_x(x_{low})$.
To obtain the $60$ choice problems of the test set, the initial set was divided into three subsets, where each subset contained either all problems with $p_x(x_{high}) \in (0,.2)$ or $p_x(x_{high}) \in [.2,.8]$ or $p_x(x_{high}) \in (.8,1)$.
From each of these subset, then, $20$ choice problems were randomly sampled.

### Data Generation

To simulate the sampling and accumulation processes, the following parameter values were used and combined with each other.
That is, each parameter combination was used to solve the test set.

* *Switching probability*: 
$\psi$ was varied in the interval $[.1, 1]$ in increments of $.1$.

* *Threshold*: 
$\theta$ was varied in the interval $[1,5]$ in increments of 1. 

* *Decision variable*: The decision variable of a prospect either counts the number of won comparisons---implying an absolute threshold---or how many more comparisons were won relative to the other prospect---implying a relative threshold.^[<!--Open footnote-->
The results are similar for absolute and relative thresholds. 
For simplification, only the results of the absolute thresholds are reported in the main text.
The results for relative thresholds are reported in the Appendix.
<!--Close footnote-->]

```{r subset-absolute, include=FALSE}
choices_absolute <- choices %>% filter(boundary == "absolute")
cpt_absolute <- cpt %>% filter(boundary == "absolute")
```

\noindent
Since $\psi$ introduces some stochasticity into the sampling and accumulation process, each of the 100 resulting parameter combinations was used 100 times---resembling 100 independent agents---to solve the test set. 
In sum, then 100 (parameter combinations) $\times$ 100 (agents) $\times$ 60 (choice problems)  = 600,000 sampling and accumulation processes were simulated.
The respective dataset can be retrieved from the accompanying `GitHub repository`.

### Estimation of Stochastic CPT

The choice data for each parameter combination was separately modeled in CPT, using the value function of @tverskyAdvancesProspectTheory1992 and the weighting function of @goldsteinExpressionTheoryPreference1987.
To demonstrate that the distortions from linear probability weighting are not just due to differences between the latent objective probabilities and the relative frequencies with which the outcomes were actually sampled over the course of a sampling and accumulation process, the latter were supplied as probability information to the weighting function.
To accommodate for possible random choice behavior that may be exerted by a parameter combination, the CPT model was amended with the logit choice rule
$$
p(P'^{safe}) = \frac{1} {1 + e^{-\rho(V_{safe}^{\frac{1}{\alpha}}-V_{risky}^{\frac{1}{\alpha}})}}
\; ,
\tag{16}
$$
where $p(P'^{safe})$ is the probability that the safe prospect is chosen over the risky prospect, $V_{safe}$ and $V_{risky}$ are the valuations of the safe and risky prospect that result from the parameter estimates of the value and weighting function, and $\rho \geq 0$ is a sensitivity parameter.
$\rho$ governs how strongly the choice between prospects depends on the differences in the valuations of the prospects, i.e., the core CPT model.
Specifically, $\rho = 0$ implies no sensitivity, and thus a random choice.
In turn, for increasing values of $\rho$, the probability of choosing the prospect with the higher valuation increases.
Note further that the valuation of the prospects were rescaled by $\frac{1}{\alpha}$, as proposed by @stewartPsychologicalParametersHave2018, to guard against possible parameter interdependencies between $\alpha$ and $\rho$ [@krefeld-schwalbStructuralParameterInterdependencies2022; @scheibehenneUsingBayesianHierarchical2015].
In sum, then, the model has four free parameters: $\alpha, \gamma, \delta, \rho$.

Since for a given choice problem and parameter combination of the sampling and accumulation model there should be no systematic differences among the 100 iterations (synthetic agents), their data is combined for the estimation of the free parameters.
Such a grouped approach should reveal the main effects of a given parameter combination of the sampling and accumulation model on the resulting choices and ignore unsystematic differences or noise that may be caused by the stochasticity introduced by $\psi$.
To facilitate comparisons across parameter combinations of the sampling and accumulation model, uninformative prior distributions were used throughout for all four parameters.
Specifically, the prior distributions $\alpha \sim U(0,1)$, $\gamma \sim U(0,2)$, $\delta \sim U(0,10)$, and $\rho \sim U(0,5)$ were used.  
 
To estimate the posterior distributions of the free parameters, Markov Chain Monte Carlo sampling as implemented in JAGS was used, where 20 chains of 40,000 samples each were run after a warm-up period of 1000 samples.
To reduce potential autocorrelation during the sampling process, only every 20th sample was kept (thinning), leaving a total of 40,000 samples across chains.
The minimum effective sample size was `r min(cpt$n.eff)`.
The potential scale reduction factor $\hat{R}$ [@gelmanInferenceIterativeSimulation1992] was $\leq$ `r round(max(cpt$Rhat), 4)` for all models and parameters, indicating good convergence.

## Results

Overall, the results of the simulation match the predictions from Table\ \@ref(tab:predictions), however, with some exceptions concerning the estimates for the parameter $\delta$ of the weighting function. 
We start by inspecting how the length of the information-processing chain changes with varying combinations of $\psi$ and $\theta$.

### Simulated Length of the Information-Processing Chains

Figure\ \@ref(fig:chain-length) displays the median number of outcomes sampled over the course of a sampling and accumulation process for each parameter combination.
The model predicts a decrease in the overall number of sampled outcomes with increases in the switching probability $\psi$.
As can be seen from the change in the slopes of the lines in Figure\ \@ref(fig:chain-length), the magnitude of this decrease diminishes as the relative increase in $\psi$ diminishes.^[<!--Open footnote-->I.e., the ratio of high values of $\psi$ is smaller than the ratio of small values of $\psi$.<!--Close footnote-->]
Moreover, the overall number of sampled outcomes increases with thresholds at a relatively constant rate.
Consider also the dashed horizontal line in Figure\ \@ref(fig:chain-length), which displays the median number of 14 outcomes that @wulffMetaanalyticReviewTwo2018 found to be sampled in 10,712 trials involving a choice between a safe and a two-outcome prospect in the sampling paradigm.
Notably, the parameter combinations that closely resemble the round-wise strategy---i.e., high values of $\psi$ and $\theta$---or the summary strategy---i.e., low values of $\psi$ and $\theta$---lead to overall numbers of sampled outcomes that come close to the meta-analytic median.
Moreover, while parameter combinations that can be understood as falling in between the round-wise and the summary strategy also lead to overall numbers of sampled outcomes that approximate the meta-analytic median, extreme combinations produce numbers that are considerably lower or higher, and thus less plausible from an empirical stance.

```{r chain-length, eval = FALSE}
chain_lengths <- choices_absolute %>%
  ggplot(aes(x = s, y = n_sample, color = model)) +
  facet_wrap(~a, nrow = 2, scales = "free") + 
  labs(x = expression(psi),
       y = "Number of Sampled Outcomes",
       color = "Model") + 
  scale_x_continuous(limits = c(0,1), breaks = seq(0,1,.5))+
  geom_jitter(alpha = .05) + 
  geom_smooth(color = "gray") + 
  geom_hline(yintercept = 14, linetype = "dashed", color = "gray") +
  scale_color_manual(values = c("#35b779" ,"#3e4989")) + 
  guides(color = guide_legend(override.aes = list(alpha = 1))) +
  theme_minimal()
chain_lengths
```

\begin{figure}[t]
\caption{Lengths of Information-Processing Chains}
\includegraphics[width=1\linewidth]{manuscript_files/chain-length} \label{fig:chain-length}
\raggedright \textit{Note.} Add note
\end{figure}

### Simulated Final Choice

Figures\ \@ref(fig:choice-rates-roundwise) and \@ref(fig:choice-rates-summary) display the rates of choices that did not maximize the latent EV (hereafter: false response rates) for each combination of $\psi$ and $\theta$.
To demonstrate that the deviations from EV maximization are not just due to sampling error, the rates of choices that did not maximize the sampled mean are displayed in gray. 
Rates are separated for presence and rank of small-probability outcomes.
Consider first the top panel showing the false response rates across all problems where the low-rank outcome of the risky prospect possesses a small probability:
Over the course of short stochastic processes, then, the low-rank outcome tends to be sampled less often than would be objectively warranted, leading to an inflation of the respective mean of the risky prospect.
Accordingly, the top panels show that the rates of false choices of the risky prospect increase with $\psi$, but the rates of false safe choices remain generally low.
More specifically, the higher the values of $\psi$, the more severe should the inflation of the mean of the risky prospect be. 
As a consequence, if the risky prospect possesses the smaller EV, an increase in $\psi$ increases the risk to deviate from EV maximization---taking the form of an as-if underweighting of rare outcomes---by falsely choosing the risky prospect on the basis of a mean comparison.
In turn, if the risky prospect possesses the larger EV, the inflation of their means decreases the risk to falsely choose the safe prospect.
Note from the middle panel that the pattern of false response rates reverses for problems where the high-rank outcome of the risky prospect possesses a small probability.
In this case, then, the mean of the risky prospect tends to be more deflated the higher the value of $\psi$, which increases the risk of falsely choosing the safe prospect.
From the reduced false response rates in the bottom panel, we see that the inflation and deflation of the means for high values of $\psi$ is significantly attenuated when a prospect possesses no small-probability outcome.  

```{r choice-rates-roundwise, include=FALSE}
#prepare data

## determine normative choice according to latent EV and sampled mean

### latent EV

fr_rates_EV <- choices_absolute %>%
  filter(model == "roundwise") %>% 
  mutate(norm = case_when(ev_ratio > 1 ~ "A", ev_ratio < 1 ~ "B")) %>% # determine normative choice
  filter(!is.na(norm)) %>% #exclude trials with equal EV
  group_by(s, boundary, a, rare, norm, choice) %>% #separate by model parameter and type of rare event
  summarise(n = n()) %>%
  mutate(rate = round(n/sum(n), 2), #compute response rates
         type = case_when(norm == "A" & choice == "B" ~ "Safe", #determine false response type
                          norm == "B" & choice == "A" ~ "Risky")) %>%
  ungroup() %>%
  filter(!is.na(type)) %>% #remove correct responses
  select(-c(norm, choice, n)) %>% 
  mutate(rare = case_when(rare == "none" ~ "\\in \\[.2,.8\\]", #change facet labels for rare events
                          rare == "attractive" ~ "\\in (0,.2)",
                          rare == "unattractive" ~ "\\in (.8,1)"),
         Norm = "EV")

### sampled mean

fr_rates_mean <- choices_absolute %>% 
  filter(model == "roundwise") %>% 
  mutate(norm = case_when(a_ev_exp/b_ev_exp > 1 ~ "A", a_ev_exp/b_ev_exp < 1 ~ "B")) %>% # determine normative choice
  filter(!is.na(norm)) %>% #exclude trials with equal EV
  group_by(s, boundary, a, rare, norm, choice) %>% #separate by model parameter and type of rare event
  summarise(n = n()) %>%
  mutate(rate = round(n/sum(n), 2), #compute response rates
         type = case_when(norm == "A" & choice == "B" ~ "Safe", #determine false response type
                          norm == "B" & choice == "A" ~ "Risky")) %>%
  ungroup() %>%
  filter(!is.na(type)) %>% #remove correct responses
  select(-c(norm, choice, n)) %>% 
  mutate(rare = case_when(rare == "none" ~ "\\in \\[.2,.8\\]", #change facet labels for rare events
                          rare == "attractive" ~ "\\in (0,.2)",
                          rare == "unattractive" ~ "\\in (.8,1)"),
         Norm = "Mean")

fr_rates <- bind_rows(fr_rates_EV, fr_rates_mean)

# labeller functions
label_theta <- function(string) {
  TeX(paste("$\\theta=$", string, sep = "")) #threshold parameter theta
}

label_rare <- function(string) {
  TeX(paste("$\\p_{High}$", string, sep = "")) #type of rare event
}

#plot

fr_rates_risky <- fr_rates %>% filter(type == "Risky")
fr_rates_safe <- fr_rates %>% filter(type == "Safe")

fr_rates_risky %>%
  ggplot(aes(s, rate, color = Norm)) +
  facet_grid(rare~a, labeller = labeller(rare = as_labeller(label_rare, default = label_parsed), 
                                         a = as_labeller(label_theta, default = label_parsed))) +
  labs(x = expression(psi),
       y = "False Response Rate",
       color = "Norm",
       shape = "False\nResponse") +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, .5)) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, .5)) +
  geom_point(aes(shape = type), size = 2) +
  geom_line() + 
  geom_point(data = fr_rates_safe, aes(shape = type), size = 2) +
  geom_line(data = fr_rates_safe) +
  scale_shape_manual(values = c(8, 16)) +
  scale_color_manual(values = c("#d3d3d3", "#35b779")) + 
  theme_minimal()
```

\begin{figure}[t]
\caption{Rates of False Risky and False Safe Choices in the Round-Wise Integration Model}
\includegraphics{manuscript_files/figure-latex/choice-rates-roundwise-1} \label{fig:choice-rates-roundwise}
\raggedright \textit{Note.} Add note
\end{figure}

```{r choice-rates-summary, include=FALSE}
#prepare data

## determine normative choice according to latent EV and sampled mean

### latent EV

fr_rates_EV <- choices_absolute %>%
  filter(model == "summary") %>% 
  mutate(norm = case_when(ev_ratio > 1 ~ "A", ev_ratio < 1 ~ "B")) %>% # determine normative choice
  filter(!is.na(norm)) %>% #exclude trials with equal EV
  group_by(s, boundary, a, rare, norm, choice) %>% #separate by model parameter and type of rare event
  summarise(n = n()) %>%
  mutate(rate = round(n/sum(n), 2), #compute response rates
         type = case_when(norm == "A" & choice == "B" ~ "Safe", #determine false response type
                          norm == "B" & choice == "A" ~ "Risky")) %>%
  ungroup() %>%
  filter(!is.na(type)) %>% #remove correct responses
  select(-c(norm, choice, n)) %>% 
  mutate(rare = case_when(rare == "none" ~ "\\in \\[.2,.8\\]", #change facet labels for rare events
                          rare == "attractive" ~ "\\in (0,.2)",
                          rare == "unattractive" ~ "\\in (.8,1)"),
         Norm = "EV")

### sampled mean

fr_rates_mean <- choices_absolute %>% 
  filter(model == "summary") %>% 
  mutate(norm = case_when(a_ev_exp/b_ev_exp > 1 ~ "A", a_ev_exp/b_ev_exp < 1 ~ "B")) %>% # determine normative choice
  filter(!is.na(norm)) %>% #exclude trials with equal EV
  group_by(s, boundary, a, rare, norm, choice) %>% #separate by model parameter and type of rare event
  summarise(n = n()) %>%
  mutate(rate = round(n/sum(n), 2), #compute response rates
         type = case_when(norm == "A" & choice == "B" ~ "Safe", #determine false response type
                          norm == "B" & choice == "A" ~ "Risky")) %>%
  ungroup() %>%
  filter(!is.na(type)) %>% #remove correct responses
  select(-c(norm, choice, n)) %>% 
  mutate(rare = case_when(rare == "none" ~ "\\in \\[.2,.8\\]", #change facet labels for rare events
                          rare == "attractive" ~ "\\in (0,.2)",
                          rare == "unattractive" ~ "\\in (.8,1)"),
         Norm = "Mean")

fr_rates <- bind_rows(fr_rates_EV, fr_rates_mean)

# labeller functions
label_theta <- function(string) {
  TeX(paste("$\\theta=$", string, sep = "")) #threshold parameter theta
}

label_rare <- function(string) {
  TeX(paste("$\\p_{High}$", string, sep = "")) #type of rare event
}

#plot

fr_rates_risky <- fr_rates %>% filter(type == "Risky")
fr_rates_safe <- fr_rates %>% filter(type == "Safe")

fr_rates_risky %>%
  ggplot(aes(s, rate, color = Norm)) +
  facet_grid(rare~a, labeller = labeller(rare = as_labeller(label_rare, default = label_parsed), 
                                         a = as_labeller(label_theta, default = label_parsed))) +
  labs(x = expression(psi),
       y = "False Response Rate",
       color = "Norm",
       shape = "False\nResponse") +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, .5)) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, .5)) +
  geom_point(aes(shape = type), size = 2) +
  geom_line() + 
  geom_point(data = fr_rates_safe, aes(shape = type), size = 2) +
  geom_line(data = fr_rates_safe) +
  scale_shape_manual(values = c(8, 16)) +
  scale_color_manual(values = c("#d3d3d3", "#3e4989")) + 
  theme_minimal()
```

\begin{figure}[t]
\caption{Rates of False Risky and False Safe Choices in the Summary Integration Model}
\includegraphics{manuscript_files/figure-latex/choice-rates-summary-1} \label{fig:choice-rates-summary}
\raggedright \textit{Note.} Add note
\end{figure}

### Simulation-Implied Cumulative Prospect Theory

Figure\ \@ref(fig:weighting-parameters) displays the estimated means and the 95%-intervals of the posterior distributions of the curvature parameter $\gamma$ and the elevation parameter $\delta$, as well as the resulting graphical shape of the weighting function.
Note that the estimates for the parameter combination $\theta = 1$ and $\psi = 1$ have large posterior intervals.
In these cases, no relative frequencies other than 0 and 1 are supplied to the weighting function, which may explain that the values in between cannot be reliably accounted for.
However, for $\theta = 1$ and $\psi < 1$, the estimates imply a linear weighting pattern, in line with the explanation that if the mind were to infer the latent objective probabilities of outcomes from the sampled relative frequencies and to follow the principle of EV maximization, sampling error can account for any deviations from it.^[<!--Open footnote-->
Note that the same line of reasoning holds, were the mind to approximate the solutions of EV maximization by choosing the prospect with higher sampled mean.
<!--Close footnote-->]
Note also Figure\ \@ref(fig:choice-rates-roundwise), where there are no choices for $\theta = 1$ that do not maximize the sampled mean.

For $\theta > 1$, the estimates substantiate the prediction that $\gamma$ takes values $\geq 1$ which increase with $\psi$, resulting in a increasingly pronounced S-shaped weighting function.
In other words, the higher the value of $\psi$, the more severe is the underweighting (overweighting) of the high-rank outcome of the risky prospect in CPT, if it is of small (large) probability.
$\delta$ takes values $\geq 1$ and increases slightly with $\psi$, causing the overweighting of the high-rank outcome of the risky prospect to extend across the mid-point. 
[...]

```{r weighting-parameters, include=FALSE}

label_theta <- function(string) {
  TeX(paste("$\\theta=$", string, sep = "")) #threshold parameter theta
}

# plot gamma estimates
gamma <- cpt_absolute %>%
  filter(parameter == "gamma") %>%
  ggplot(aes(s, mean, color = model)) +
  facet_wrap(~a, nrow = 2, labeller = labeller(a = as_labeller(label_theta, default = label_parsed)), scales = "free") +
  scale_x_continuous(limits = c(0,1), breaks = seq(0,1,.5)) + 
  labs(x = expression(paste("Switching Probability ", psi)), 
       y = expression(gamma),
       color = "Model") +
  geom_errorbar(aes(ymin=`2.5%`, ymax=`97.5%`)) + 
  geom_point() +
  geom_line() +
  scale_color_manual(values = c("#35b779" ,"#3e4989")) + 
  theme_minimal()

# plot delta estimates
delta <- cpt_absolute %>%
  filter(parameter == "delta") %>%
  ggplot(aes(s, mean, color = model)) +
  facet_wrap(~a, nrow = 2, labeller = labeller(a = as_labeller(label_theta, default = label_parsed)), scales = "free") +
  scale_x_continuous(limits = c(0,1), breaks = seq(0,1,.5)) + 
  labs(x = expression(paste("Switching Probability ", psi)), 
       y = expression(delta),
       color = "Model") +
  geom_errorbar(aes(ymin=`2.5%`, ymax=`97.5%`)) + 
  geom_point() +
  geom_line() +
  scale_color_manual(values = c("#35b779" ,"#3e4989")) + 
  theme_minimal()

# merge plots
ggarrange(gamma, delta, ncol = 1, nrow = 2, common.legend = TRUE, legend = "right")
```

\begin{figure}[t]
\caption{Parameter Estimates of the Weighting Function}
\includegraphics{manuscript_files/figure-latex/weighting-parameters-1} \label{fig:weighting-parameters}
\raggedright \textit{Note.} Add note
\end{figure}

Figure\ \@ref(fig:sensitivity-parameters) displays the estimated means and the 95%-intervals of the posterior distributions of the outcome sensitivity parameter $\alpha$ and the choice sensitivity parameter $\rho$.
An increase in $\psi$ leads to a decrease in $\alpha$, reflecting that outcome information are largely ignored if choices are based on ordinal comparisons of single outcomes rather than on means of large samples.
The respective shape of the value function is more concave, the higher the values of $\psi$. 
Finally, $\rho$ varied systematically such that the choices were more sensitive to the valuations of the core CPT model for low values of $\psi$.
However, even for high values of $\psi$, the choices did not appear to be random. [...]

```{r sensitivity-parameters, include=FALSE}

# plot alpha estimates
alpha <- cpt_absolute %>%
  filter(parameter == "alpha") %>%
  ggplot(aes(s, mean, color = model)) +
  facet_wrap(~a, nrow = 2, labeller = labeller(a = as_labeller(label_theta, default = label_parsed)), scales = "free") +
  scale_x_continuous(limits = c(0,1), breaks = seq(0,1,.5)) + 
  labs(x = expression(paste("Switching Probability ", psi)), 
       y = expression(alpha),
       color = "Model") +
  geom_errorbar(aes(ymin=`2.5%`, ymax=`97.5%`)) + 
  geom_point() +
  geom_line() +
  scale_color_manual(values = c("#35b779" ,"#3e4989")) + 
  theme_minimal()

# plot rho estimates
rho <- cpt_absolute %>%
  filter(parameter == "rho") %>%
  ggplot(aes(s, mean, color = model)) +
  facet_wrap(~a, nrow = 2, labeller = labeller(a = as_labeller(label_theta, default = label_parsed)), scales = "free") +
  scale_x_continuous(limits = c(0,1), breaks = seq(0,1,.5)) + 
  labs(x = expression(paste("Switching Probability ", psi)), 
       y = expression(rho),
       color = "Model") +
  geom_errorbar(aes(ymin=`2.5%`, ymax=`97.5%`)) + 
  geom_point() +
  geom_line() +
  scale_color_manual(values = c("#35b779" ,"#3e4989")) + 
  theme_minimal()

# merge plots
ggarrange(alpha, rho, ncol = 1, nrow = 2, common.legend = TRUE, legend = "right") 
```

\begin{figure}[t]
\caption{Parameter Estimates of the Outcome and Choice Sensitivity Parameter}
\includegraphics{manuscript_files/figure-latex/sensitivity-parameters-1} \label{fig:sensitivity-parameters}
\raggedright \textit{Note.} Add note
\end{figure}

Figure\ \@ref(fig:function-forms) displays the resulting graphical shapes of the weighting and value function.

```{r function-forms, include=FALSE}

# plot weighting function

## compute decision weights

weights <- cpt_absolute %>%
  select(model, s, boundary, a, parameter, mean) %>%
  pivot_wider(names_from = parameter, values_from = mean) %>%
  select(-c(alpha, rho)) %>%
  expand_grid(p = seq(0, 1, .05)) %>%
  mutate(w = round(  (delta * p^gamma)/ ((delta * p^gamma)+(1-p)^gamma), 2))

## plot graphs

wf <- weights %>% 
  ggplot(aes(p, w, group = s, color = s)) +
  facet_wrap(~model~a, nrow = 2, labeller = labeller(a = as_labeller(label_theta, default = label_parsed))) + 
  labs(x = "Experienced Probability",
       y = expression(pi),
       color = expression(psi)) +
  scale_x_continuous(breaks = seq(0, 1, .5)) +
  scale_y_continuous(breaks = seq(0, 1, .5)) +
  geom_line(size = .5) +
  scale_color_viridis(option = "mako") +
  theme_minimal()
  
# value functions

values <- cpt_absolute %>%
  select(model, s, boundary, a, parameter, mean) %>%
  pivot_wider(names_from = parameter, values_from = mean) %>%
  select(-c(gamma, delta, rho)) %>%
  expand_grid(x = seq(0, 20, .5)) %>%  
  mutate(v = round(x^alpha, 2)) 

vf <- values %>% 
  ggplot(aes(x, v, group = s, color = s)) +
  facet_wrap(model~a, nrow = 2, labeller = labeller(a = as_labeller(label_theta, default = label_parsed))) + 
  labs(x = "Objective Outcome",
       y = "Subjective Value",
       color=expression(psi)) +
  scale_x_continuous(limits = c(0,20), breaks = seq(0, 20, 10)) +
  geom_line(size = .5) +
  scale_color_viridis(option = "mako") +
  theme_minimal()

# merge plots
ggarrange(wf, vf, ncol = 1, nrow = 2, common.legend = TRUE, legend = "right")
```

\begin{figure}[t]
\caption{Estimated Graphical Shape of the Weighting and Value Function}
\includegraphics{manuscript_files/figure-latex/function-forms-1} \label{fig:function-forms}
\raggedright \textit{Note.} Add note
\end{figure}

# Discussion

The presented sampling and accumulation model ought to sketch how the mind might exercise the capacity of making decisions from experience.
In DFE, one cannot rely on symbolic descriptions of all relevant properties of the choice alternatives, but only on samples of past experiences, e.g., when the alternatives were chosen in similar decisions in the past.
To study DFE experimentally, one may use the sampling paradigm where people can sequentially sample the outcomes of prospects according to their probability of occurrence before making a final choice.
To model the process by which people arrive at the final choice in this paradigm, one must explicate a number of assumptions, e.g.: How do people decide when to switch between prospects; how do they integrate the information obtained from the sampled outcomes; and, because they can sample as much as they want, how do they decide to stop sampling.
To approach these questions, this paper presented a model that integrated a proposed link between sampling and decision strategies into the computational framework of sequential sampling models.

## Modeling Sampling Strategies and the Decision-Making Process

@hillsInformationSearchDecisions2010 proposed---and provided tentative empirical support for---a systematic relation between what they considered two paradigmatic sampling and decision strategies. 
That is, the authors assumed that people either switch prospects after each sampled outcome (piecewise sampling) to compare single outcomes over repeated rounds (round-wise decision strategy), or they switch only once between prospects (comprehensive sampling) and make only one comparison between the means across all sampled outcomes (summary decision strategy).
In sequential sampling models [see, e.g., @ratcliffComparisonSequentialSampling2004], the decision-making process is modeled as accumulation of evidence for or against a prospect over time, with the prospect being chosen for which the accumulated evidence first reaches a threshold.
The sampling and accumulation model that was presented in this paper integrates both propositions by assuming that people accumulate the wins and losses resulting from a sequence of mean comparisons, with switching probabilities determining the size of the samples underlying each comparison and thresholds determining how much (more) comparisons a prospect must win in order to be chosen.
Hence, the model treats the two paradigmatic sampling and decision strategies as the extreme ends of possible continua, allowing the switching probability not only to take values close to 0 and 1 but also values in between, and allowing single and multiple mean comparisons between prospects to be based on samples of varying size.
Moreover, the model explicates the suggested link between sampling and decision strategies by assuming that the accumulation process more closely resembles the round-wise strategy for high switching probabilities, and, vice versa, the summary strategy for low switching probabilities.
This is because high (low) switching probabilities cause the accumulation process to unfold over mean comparisons based on samples of small (large) size.

## Sampling Strategies Might Shape Final Choice

In a computational analysis, the presented model was used to simulate how the proposed sampling and accumulation processes shape the final choice.
Therefore, the switching probabilities and evidence thresholds were systematically varied, while simulating choices between a safe and a risky prospect.
The model predicts the choices to vary systematically with the switching probability.
Specifically, whereas the choice patterns correspond closely to the solutions of EV maximization for low switching probabilities, increases in the switching probability can lead to an increasingly pronounced as-if underweighting of rare outcomes pattern.
This is because for high switching probabilities, each independent mean comparison is based on small samples, in which small-probability outcomes are underrepresented rather than overrepresented.
As a consequence, then, small-probability outcomes are predicted to contribute to the accumulated evidence less rather than more than would be warranted by their objective probabilities.
Accordingly, the analyses showed that the structure of the choice problem---i.e., whether a small-probability outcome is present and which value it takes---is an important moderator for the effect of sampling strategies on the choice pattern. 
Importantly, while this explanation emphasizes that the reliance on small samples is a strong contributor to the as-if underweighting of rare outcomes pattern in DFE, it has little to do with the common notion of sampling error [e.g., @foxDecisionsExperienceSampling2006; @hertwigDecisionsExperienceEffect2004].
That is, the as-if underweighting of rare outcomes is not caused by a deviation of the sampled relative frequencies from the objective probabilities, but by the way the sampled outcomes are integrated and processed to evaluate the prospects.
The analyses made this clear by showing that an as-if underweighting of rare outcomes pattern occurred, even if the sampling error is controlled for.
In sum, by explicitly modeling their link, the current computational analysis showed how the interplay of sampling and decision strategies can shape the final choice.

## CPT Can Reflect the Adoption of Sampling Strategies

The current paper further extended the analysis of @hillsInformationSearchDecisions2010 by showing that the effects of sampling and decision strategies on the final choice can translate to characteristic signatures in cumulative prospect theory's value and weighting function. 
Having controlled for sampling error, the estimated CPT parameters and the resulting shapes of the value and weighting function primarily reflected how sensitive the combinations of sampling and decision strategies are to particular changes in the outcomes and their probabilities.
Thereby, the analyses extends the notion that CPT can be used as a measurement tool to characterize information processing [@pachurHowTwainCan2017] from risky to uncertain choice.

## Why Sampling and Decision Strategies?

[...]

# Conclusion

[...]

\newpage

# Acknowledgements

\newpage

# References
