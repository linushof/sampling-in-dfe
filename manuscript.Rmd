---
title             : |
  Sampling Strategies in Decisions from Experience
  
shorttitle        : "Sampling strategies"

author: 
  - name          : "Linus Hof"
    affiliation   : "1"
    corresponding : yes
    address       : "Institute of Psychology, Heidelberg University, Hauptstr. 47 – 51, 69117 Heidelberg"    
    email         : "linus.hof@stud.uni-heidelberg.de"
    
affiliation:
  - id            : "1"
    institution   : "Heidelberg University"
    
authornote: |
 This manuscript is a dynamic document which can be reproduced using the materials and instructions provided on the GitHub repository: https://github.com/linushof/sampling-in-dfe. The current version of the manuscript, created from the commit with the hash `r repro::current_hash()`, is prepared for submission as a master's thesis in fulfillment of the requirements for the degree Master of Science (M.Sc.) at Heidelberg University, Faculty of Behavioural and Cultural Studies, Institute of Psychology. Supervisors:
  
  \indent
  1. Prof. Dr. Thorsten Pachur, Technical University of Munich
  
  \indent
  2. Dr. Veronika Zilker, Max Planck Institute for Human Development
 
abstract: |
  In decisions from experience, the probability to produce a higher outcome most of the time and the expected value are two latent properties that can be used to distinguish among the prospects of choice. This paper sketches a sampling and accumulation model that allows sampling strategies—i.e., the probability of sampling outcomes from different prospects in direct succession (switching probability)—to alter the degree to which evidence is accumulated for either one of these properties. A computational implementation of the model is used to simulate the respective sampling and accumulation process upfront choices between a safe and a two-outcome prospect, while varying switching probabilities and evidence thresholds. The simulation data shows that, depending on which latent property evidence is accumulated for and to which extent, respectively, choice patterns reflect various degrees of an as-if underweighting of rare outcomes or expected value maximization. Both choice patterns become more robust the more evidence is sampled and translate to characteristic parameter estimates for cumulative prospect theory’s value and weighting function.
  
keywords          : "risky choice, information sampling, evidence accumulation, cognitive modeling, cumulative prospect theory"
wordcount         : ""

bibliography      : references.bib
csl               : apa.csl

figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
linkcolor         : "blue"
link-citations    : true
mask              : no
draft             : no

classoption       : "man"
documentclass     : "apa7"
output: papaja::apa6_pdf

header-includes:
   - \usepackage[capposition=top]{floatrow}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, #omit messages
                      fig.align = "l", #align figures to left margin
                      fig.pos = "t", #display figures at the top of a page
                      fig.height=7, 
                      fig.width=10)
```

```{r packages, include=FALSE}
#load required packages
pacman::p_load(papaja,
               knitr,
               readr,
               tidyr, 
               dplyr,
               ggplot2,
               viridis,
               scico,
               ggpubr,
               latex2exp)
```

The human mind is a cognitive system that operates on inputs from an environment or memory, and in the domain of decision making, such inputs provide information about the alternatives between which people can choose.
<!--Accordingly, irrespective of their exact mathematical specification or computational implementation, decision models generally take as inputs information about a set of choice alternatives, each possessing some defining *properties*, and return a subset of chosen alternatives as outputs [@heOntologyDecisionModels2020].
Thus, decision making can be understood as an information-processing problem whose solution requires a selection among choice alternatives, where the selection can only be systematic to the extent it is based on the assessment of differences in the alternatives' properties.-->
Now, let a prospect be a choice alternative that possesses only two kinds of fundamental properties: the possible outcomes of the alternative (e.g., gains or losses of some amount) and the probabilities with which these outcomes occur following the choice of the alternative.
Importantly, the information that the inputs provide about these properties can come in different forms.
Specifically, in *decisions from description* (DFD), the inputs take the form of explicit and complete descriptions of all outcomes and probabilities.
In this case, people make decisions under risk, provided that choosing one of the prospects does not inevitably lead to a certain outcome.
Yet, in *decisions from experience* [DFE, @hertwigDecisionsExperienceEffect2004], the properties are latent and therefore not known with certainty, but they must be inferred from the relative frequencies with which the outcomes occurred in the past, e.g., when the prospect was chosen in similar past decisions.
In other words, in DFE, the inputs take the form of sampled outcomes, with the lack of precise knowledge about all possible outcomes and their probabilities rendering the decision one under uncertainty rather than just risk.
The differences in the input formats between DFD and DFE are illustrated in the following `code example`:

```{r echo=TRUE}
# define a prospect with two outcomes
outcomes <- c(5, 0)
probabilities <- c(.2, .8)

# input to decisions from description
input_DFD <- data.frame(outcomes, probabilities)
print(input_DFD)

# input to decisions from experience, here: 10 sampled outcomes
set.seed(2022)
input_DFE <- sample(outcomes, size = 10, prob = probabilities, replace = T)
print(input_DFE)
```

\noindent
To study DFE experimentally, one can use several paradigms, one of which is the sampling paradigm [see @hertwigDescriptionexperienceGapRisky2009], where people can, as much as they want, sequentially sample the outcomes of two prospects according to their objective probabilities before making a final choice between the prospects (see Figure\ \@ref(fig:sampling-paradigm)).

Although the distinct input forms---i.e., descriptions or sampled outcomes---may in principle carry the same information about a given set of prospects, behavioral decision research has so far produced a large body of papers indicating that---taking a quite general perspective---there are robust differences between the choices that are made in DFD and DFE, leading to the notion of the *description-experience gap* [@hertwigDescriptionexperienceGapRisky2009].
In one reading of the gap,^[<!--Open footnote-->See @hertwigConstructbehaviorGapDescription2018, for a discussion on the interpretation of the gap.<!--Close footnote-->]
choice patterns in DFD deviate from the solutions of expected value (EV) maximization, as if small-probability outcomes are given more weight than would be warranted by their objective probabilities (*as-if overweighting of rare outcomes*);
in turn, in DFE, choice patterns deviate from the solutions of EV maximization, as if the same small-probability outcomes are given less weight than would be objectively warranted [*as-if underweighting of rare outcomes*, e.g., @barronSmallFeedbackbasedDecisions2003; @hertwigDecisionsExperienceEffect2004; @weberPredictingRiskSensitivity2004; @erevAnomaliesForecastsDescriptive2017; @regenwetterConstructbehaviorGapBehavioral2017; see @wulffMetaanalyticReviewTwo2018, for a comprehensive meta-analytic review].
In other words, in DFD, people choose as if small-probability outcomes are more probable than objectively warranted, and in DFE, people choose as if the same small-probability outcomes are less probable than objectively warranted.

<!--Similar to explanations for other behavioral phenomena that rest on the assumption that the human mind is sensitive to the composition of the *samples*---i.e., sets of sampled outcomes---on which it operates [cf. @fiedlerBewareSamplesCognitiveecological2000], an explanation for the as-if underweighting of rare outcomes may build on the assumption that the mind is sensitive to the information provided by small samples [@hertwigDecisionsExperienceEffect2004; @plonskyRelianceSmallSamples2015; see also @erevChoicePredictionCompetition2010; @erevAnomaliesForecastsDescriptive2017, for the performance of models assuming reliance on small samples in prediction competitions]:
Specifically, following from the laws of large numbers [cf., e.g., @kolmogorovFoundationsTheoryProbability1950], the relative frequencies with which the prospect's outcomes occur in a sample of infinite size converge almost surely to the respective objective probabilities.
In such a case---or in the less strict case of large samples---, then, the binomial distribution of each outcome can be approximated by a symmetric normal distribution.^[The binomial distribution of an outcome is the probability distribution of the number of times an outcome occurs in a sample. According to the *de Moivre-Laplace theorem* [see, e.g., @georgiiStochastikEinfuhrungWahrscheinlichkeitstheorie2015], the binomial distribution can be approximated by a normal distribution given a large sample. Respectively, the relative relative frequencies with which an outcome occurs in a large sample is predicted to correspond to the objective probability of the outcome most of the time and is not predicted to be lower more often than higher or vice versa.]
However, for samples of small size, the binomial distribution associated with small-probability outcomes is positive skewed, therefore causing the relative frequencies with which these outcomes occur in the sample to be smaller rather than larger than their latent objective probabilities.

In fact, in DFE, people tend to sample only a small number of outcomes from each prospect [@hertwigDecisionsExperienceEffect2004; @foxDecisionsExperienceSampling2006; @hertwigDecisionsExperienceWhy2010; @rakowBiasedSamplesNot2008; see also @wulffMetaanalyticReviewTwo2018], which is a sufficient condition for the reliance on small samples in the evaluation of prospects.
Yet, the as-if underweighting of rare outcomes may appear, even if the overall number of sampled outcomes is large and the relative frequencies closely resemble the latent objective probabilities [@hauDescriptionexperienceGapRisky2008; @hauDecisionsExperienceStatistical2010; @ungemachAreProbabilitiesOverweighted2009; @wulffMetaanalyticReviewTwo2018].
While this latter finding does not preclude the reliance on small samples---i.e., the mind may rely on only a portion of the sampled outcomes, discount them, or, as proposed in the current paper, distribute them over multiple smaller samples---, it suggests that an explanation for the as-if underweighting of rare outcomes cannot be reduced to inaccurate inferences about the latent properties of prospects that are caused by frugal sampling and the implied sampling error.
Rather the opposite, this paper presents a sampling and accumulation model that suggests that the as-if underweighting of rare outcomes can be the consequence of an accurate assessment of differences among prospects in a latent property [but see @hertwigDescriptionexperienceGapRisky2009; @plonskyRelianceSmallSamples2015; @wulffMetaanalyticReviewTwo2018, for alternative explanations].
-->

@hillsInformationSearchDecisions2010 argued that one contributing factor to the description-experience gap might be a potential link between what they considered two paradigmatic sampling and decision strategies (see Figure\ \@ref(fig:sampling-paradigm)).
That is, the authors assumed that in the sampling paradigm, people either switch prospects after each sampled outcome (piecewise sampling) to compare single outcomes over repeated rounds (round-wise decision), or they switch just once between prospects (comprehensive sampling) and make only one comparison between the means across all sampled outcomes (summary decision).
In fact, @hillsInformationSearchDecisions2010 provided tentative evidence for such a coupling of sampling and decision strategies by finding that the choices of people who switched frequently between prospects during the sampling phase were better predicted by a round-wise strategy (rather than a summary strategy), and that this predictive pattern reversed for people who switched infrequently.^[<!--Open footnote-->
Note that in their analyses of empirical sampling data,  @hillsInformationSearchDecisions2010 used the number of switches between prospects relative to the overall number of sampled outcomes from both prospects (switching frequency) as an approximate measure for the two proposed sampling strategies, and acknowledged that "many [sampling] strategies will fall on the continuum in between [the two paradigmatic strategies]" (p. 1788).
<!--Close footnote-->]
Using the round-wise strategy may translate to an as-if underweighting of rare events pattern and the description-experience gap because the strategy selects the prospect that wins a majority of comparisons, while small-probability outcomes are predicted not to be considered in most of these comparisons.

(ref:sampling-paradigm) Possible Sampling and Decision Strategies in the Sampling Paradigm and Their Impact on the Final Choice

```{r sampling-paradigm, fig.cap="(ref:sampling-paradigm)"}
knitr::include_graphics(path = "manuscript_files/sampling-paradigm.png")
```


<!-- Problems of Hills and Hertwig -->


<!--Considering this paradigm and the choice between two prospects, @hillsInformationSearchDecisions2010 distinguished sampling strategies along the frequency of switches between the two prospects during the sampling phase,^
and proposed two different decision strategies that people may adopt.
Specifically, for a summary strategy, it is assumed that the prospect is chosen which produced the greater mean across all sampled outcomes.
In turn, for a round-wise strategy, it is assumed that over multiple rounds two sampled outcomes---i.e., one from each of the prospects---are compared and that the prospect is chosen which won the comparison in most rounds.-->

<!--To model the process by which people arrive at the final choice, one must explicate a number of assumptions, e.g.: How do people decide when to switch between prospects during the sampling phase; how do they integrate and process the information obtained from the sampled outcomes; and, because they can sample as much as they want, how do they decide to stop sampling.
To approach these questions, the model presented in this paper integrates the proposed link between sampling and decision strategies into the computational framework of sequential sampling models.-->
Now, the current paper presents a sampling and accumulation model that provides an algorithmic link between sampling and decision strategies, and uses the model to discuss how the mind may rely on sampling strategies to alter which differences in the latent properties of prospects evidence is accumulated for.
More specifically, the proposed model assumes an accumulation process over a sequence of mean comparisons, with switching probabilities---i.e., the probability of sampling
outcomes from different prospects in direct succession---determining the size of the samples underlying each mean comparison and thresholds determining how many comparisons a prospect must win in order to be chosen.
The model is such that when switching probabilities are high, the accumulation process unfolds over mean comparisons based on small samples, thus resembling the round-wise strategy and accumulating evidence for differences in the probability to produce a higher outcome most of the time.
Notably, in this case, small-probability outcomes are predicted to contribute to the majority of mean comparisons less than would be objectively warranted, which can translate to an as-if underweighting of rare outcomes, irrespective of what the thresholds require the number of won comparisons to be.
<!--Accordingly, while increases in thresholds---and thus in the number of sampled outcomes distributed over multiple small samples, each of which is considered---allow to choose with increasing accuracy the prospect that is more likely to return a higher outcome, the resulting choices can produce an as-if underweighting of rare outcomes.-->
In turn, for low switching probabilities, the accumulation process unfolds over mean comparisons based on large samples, thus resembling the summary strategy---for low thresholds, respectively---and accumulating evidence for differences in the EV.

To substantiate the discussion, the model is used to simulate the sampling and accumulation process for choices between a safe and a two-outcome prospect, while systematically combining different switching probabilities and thresholds.
Considering choice rates and parameter estimates of cumulative prospect theory [@tverskyAdvancesProspectTheory1992], the computational analysis shows that the choices converge to the solutions of EV maximization for low switching probabilities, but take the form of an as-if underweighting of rare outcomes for high switching probabilities.
Importantly, the analysis also shows that these distinct choice patterns become more robust with increasing thresholds.
Before presenting the model and the computational analysis, the following section provides a definition of prospects.

# Definition of Prospects

Since there is an uncountable number of choices people confront in the natural world, behavioral decision research routinely abstracts from choices between particular choice alternatives, e.g., the choice between job offers, political parties to vote for, investment plans, and whatnot.
Rather, with the choice between at least two prospects, it studies a case which is counterfactual in that it omits the many particularities of each choice situation, but retains in the form of prospects the fundamental properties that almost all choice alternatives are assumed to possess.
These fundamental properties are considered the possible outcomes of a choice alternative and the probabilities with which these outcomes occur following the choice of the alternative.
Roughly speaking, then, all choice alternatives that can be fully described by their outcome-probability pairs are prospects.
As such, prospects can be understood as the distribution of a discrete random variable.
More specifically, following @kolmogorovFoundationsTheoryProbability1950 and adopting the notation from @georgiiStochastikEinfuhrungWahrscheinlichkeitstheorie2015, let the tuple $(\Omega, \mathcal{F}, P)$ be a probability space.
Accordingly, $\Omega$ is a finite sample space with each element $\omega \in \Omega$ denoting a possible consequence of choosing the respective prospect.
$\mathcal{F}$ is the power set $\mathcal{P}(\Omega)$ with each element $A \in \mathcal{F}$ being a subset of $\Omega$, also denoted a random event.
$P$ is the probability measure
$$
P: \mathcal{F} \to [0,1]
\tag{1}
$$
assigning the elements of $\mathcal{F}$ a probability $0 < p(A) \leq 1$ with $P(\Omega) = 1$.
Now, the random variable is a function
$$
\begin{aligned}
  X : (\Omega, \mathcal{F}) &\to (\Omega', \mathcal{F'})  \\
  \omega &\mapsto x
  \; ,
\end{aligned}
\tag{2}
$$
where $(\Omega', \mathcal{F'})$ is a measurable space with each element $x \in \Omega'$ denoting a possible outcome of a prospect and each element $A' \in \mathcal{F'}$ being a subset of $\Omega'$.
An outcome's probability $p_X(x)$ is then provided by the pushforward measure
$$
P' : \mathcal{F'} \to [0,1]
\tag{3}
$$
with 
$$
P'(A' \in \mathcal{F'}) := P(\{\omega: X(\omega) = x \in A'\})
\; .
\tag{4}
$$
Note that the measure in (3) and (4), respectively, is a random variable's distribution and therefore a prospect. 
Accordingly, given a choice between $1, ..., k$ prospects, the inputs from the environment or memory should provide information about the set
$$
\{P'^1, ... , P'^k\}
\; ,
\tag{5}
$$
where each element $P'$ can itself be considered a finite set of outcome-probability pairs $\{(x_i, p_X(x_i)\}_{i \in \mathbb{N}}$.

# Sampling Strategies and the Accumulation of Evidence

Since it is only through their fundamental properties, or combinations thereof, that any distinction between the prospects can be made, systematic---as opposed to random---choices between prospects must be based on the assessment of differences in their properties.
However, only rarely in peoples' daily life, the inputs to the decision-making process take the form of explicit descriptions of all outcome-probability pairs.
In such cases, people would make DFD, since the information from which the mind learns about the properties of prospects are complete descriptions thereof.
Yet, rather often, people make DFE, where the mind can learn about the latent properties only by experiential sampling over time [@hertwigDescriptionexperienceGapRisky2009, p. 517].
Accordingly, for the choice between $1, ..., k$ prospects, the inputs to the decision-making process are generated by a set of prospect-specific stochastic processes
$$
\{\{X^1_t\}, ... , \{X^k_t\}\}
\; ,
\tag{6}
$$
where each $\{X_{t}\}_{t \in \mathbb N}$ is a collection of independent and identically distributed (i.i.d.) random variables.^[Note that it has been emphasized that in the natural world samples are "virtually never random" [@fiedlerBewareSamplesCognitiveecological2000, p. 660]. However, in the sampling paradigm, i.i.d. random variables are commonly assumed.]
The inputs to DFE are then the realizations of the random variables in (6).

The mind may use these realizations---i.e., the sampled outcomes---to first infer the latent properties and then proceed by selecting a property, or combinations thereof, to assess differences among the prospects and make a respective choice.
However, the mind may also approximate such a process by directly accumulating evidence about the differences in latent properties from the sampled outcomes.
To this end, sequential sampling models of decision making [e.g., @busemeyerDecisionFieldTheory1993] assume that the outcomes of prospects are sequentially sampled and integrated into dynamic decision variables and that a choice is made once a decision variable exceeds a threshold in favor for one of the prospects.
This class of process models thereby provides an algorithmic link between the actual sampling process and the choices in DFE, and has shown to explain some of the robust deviations from EV maximization [see @bhatiaSequentialSamplingParadoxes2014; @busemeyerDecisionFieldTheory1993], including the as-if underweighting of rare outcomes in the sampling paradigm [see @markantModelingChoiceSearch2015].
Moreover, these models implement speed-accuracy trade-offs, with the amount of evidence that must be sampled in favor for a prospect increasing with thresholds, leading to more (less) accurate but slower (faster) choices for high (low) thresholds.

Yet, there are several distinct properties that can be used to assess differences between the prospects, among them the EV, for which the summary decision strategy is indicative, and the probability to produce a higher outcome most of the time, for which the round-wise decision strategy is indicative [@hillsInformationSearchDecisions2010; see also @wulffHowShortLongrun2015].
Respectively, in the following, both decision strategies are implemented into a common sampling and accumulation model to demonstrate how the mind may rely on different sampling strategies to alter the degree to which evidence is accumulated for differences in either one of these properties.
More specifically, the proposed model accumulates the outcomes obtained from a sequence of comparisons between sample means, with the size of the samples underlying each mean comparison tending to be large (small) for low (high) switching probabilities.
Accordingly, whereas the decision variables of the model capture how many comparisons a prospect has won, the unfolding accumulation process more closely resembles the summary strategy for low switching probabilities and the round-wise strategy for high switching probabilities.

## Model 

We consider the sampling paradigm and a choice between two prospects denoting the distributions $P'^X$ and $P'^Y$ of the discrete random variables $X$ and $Y$, respectively.
The model assumes that the sampling process starts at random with a stochastic process on one of the two random variables.
Specifically, it is assumed that an agent starts with equal probability to sample from one of the prospects, say $P'^X$, and generates a sequence of random variables $X_1, X_2,...$, where the mean over the sequence is computed or updated with each new outcome that is sampled.
The respective stochastic process $\{S^X_n\}_{n \in \mathbb{N}}$ is defined by
$$
S^X_n = \frac{1}{n} \sum_{t = 1}^n X_t, \quad n \in \mathbb{N}
\; ,
\tag{7}
$$
and terminates as soon as an outcome is sampled from the other prospect.
The switching probability $\psi \in (0,1]$, which is assumed to be adopted prior the start of the information-processing chain and is fixed throughout, controls the probability with which outcomes from different prospects are sampled in direct succession.
In other words, the switching probability is the probability with which the stochastic process on the current random variable stops and a new stochastic process on the other random variable starts with the subsequently sampled outcome.
To implement the accumulation process, the model assumes that each time after a new stochastic process was started and terminated on both random variables, their respective values are compared, with the prospect underlying the stochastic process with the greater final value receiving a round win.
Accordingly, for the prospect $P'^X$, this stage is modeled as the mapping
$$
\begin{aligned}
  Z^X : \mathbb{R} &\to \mathbb{N} \\
  S^X_n - S^Y_n &\mapsto 
  \begin{cases}
     1 &\text{if} \quad  S^X_n > S^Y_n, \\
     0 &\text{else}
     \; ,
  \end{cases}
\end{aligned}
\tag{8}
$$
where $Z^X$ is a random variable that takes only the two values $0$ and $1$, denoting a lost or won mean comparison, respectively. 
The model assumes that a round consisting of a pair of stochastic processes---i.e., one for each prospect---and the subsequent comparison of their final values is repeated until the sum of round wins for one of the two prospects reaches a threshold $\theta \in \mathbb{N}$.
Similar to the switching probability, it is assumed that the threshold is adopted prior the start of the information-processing chain and is fixed throughout.
For $P'^X$, then, we obtain a sequence of independent comparisons $Z^X_1, Z^X_2, ...$ and model the respective evidence accumulation process as a random walk $\{D^X_m\}_{m \in \mathbb{N}}$ defined by 
$$
D^X_m = \sum_{i = 1}^m Z^X_i, \quad m \in \mathbb{N}
\;,
\tag{9}
$$
resembling a dynamic decision variable.
Note that since (8) describes a Bernoulli trial, the random walk in (9) is based on a Bernoulli process.

Finally, note that the described model assumes two random walks---i.e., one for each prospect---approaching the same threshold, which is the absolute number of mean comparisons a prospect must win in order to be chosen.
However, one may also define a single random walk, resembling a common dynamic decision variable, which approaches a positive threshold $\theta^+$ and a negative threshold $\theta^-$.
Therefore, (8) may be adapted such that $Z$ takes the value $-1$ instead of $0$, such that the threshold determines how many more mean comparisons a prospect must win in order to be chosen.

# Model Predictions

Below, the key predictions concerning the effects of the model parameters $\psi$ and $\theta$ on the length of the sampling and accumulation process and the resulting choice are outlined.
Moreover, previous research has shown that differences as to how information about the properties of prospects is processed can translate to characteristic shapes of cumulative prospect theory's (CPT) value and weighting function [see @pachurHowTwainCan2017; @zilkerNonlinearProbabilityWeighting2021].
Respectively, this section also contains predictions about the relations between $\psi$ and $\theta$ and the CPT parameters.
A summary of the model predictions can be found in Table\ \@ref(tab:predictions).

(ref:predictions) Summary of Model Predictions

```{r predictions}
predictions <- tibble("Measure" = 
                        c("Choices pattern",
                          "",
                          "$\\gamma$ (curvature of $w$)",
                          "$\\delta$ (elevation of $w$)",
                          "$\\alpha$ (concavity of $v$)"),
                      "Predictions for changes in $\\psi$" = 
                        c("More EV maximization for decreasing $\\psi$",
                          "More as-if underweighting of rare outcomes for increasing $\\psi$",
                          "$\\geq 1$, increases with $\\psi$",
                          "1, no effect of $\\psi$",
                          "$\\leq 1$, decreases with increasing $\\psi$"))
apa_table(predictions, caption = "(ref:predictions)", note = "The predictions should become more robust for high values of $\\theta$.", escape = FALSE)
```

## Length of Information-Processing Chains

The required number of independent mean comparisons increases with thresholds $\theta$.
Since each comparison is based on its own pair of stochastic processes, an increase in $\theta$ translates to a larger number of required stochastic processes and, ceteris paribus, therefore also to a larger overall number of outcomes sampled over the course of a sampling and accumulation process.
In turn, each stochastic process is predicted to terminate faster, the higher the probability that the subsequent outcome is sampled from the other prospect, determined by the switching probability $\psi$.
Accordingly, an increase in $\psi$ leads to shorter stochastic processes, which translate, ceteris paribus, to a smaller overall number of sampled outcomes.

## Final Choice

Because the switching probability $\psi$ is predicted to alter the length of the stochastic processes underlying each mean comparison, $\psi$ should affect whether low-probability outcomes contribute to the majority of the required mean comparisons about as much as would be warranted based on their objective probabilities, or whether they contribute rather less.
That is, for short stochastic processes, the binomial distribution associated with small-probability outcomes is positive skewed, therefore causing the relative frequencies with which these outcomes occur over the course of such a process to be smaller rather than larger than their latent objective probabilities.
Respectively, if the small-probability outcome is smaller (larger) than the EV, high values of $\psi$---and therefore pairs of short stochastic processes---cause the mean to be an inflated (deflated) estimate of the EV.
Such an inflation and deflation of the means can then translate to an as-if underweighting of rare outcomes.
Specifically, if the mean of the prospect with the lower (higher) EV is inflated (deflated), the outcome of the comparison between the means may be a reversal---in sign---of the outcome of a comparison between the EVs, leading to choices that take the form of an as-if underweighting of rare outcomes.
Yet, if the mean of the prospect with the lower (higher) EV is deflated (inflated), the difference between the means should be larger than the differences between the EV, which can simplify EV maximization [see @hertwigDecisionsExperienceWhy2010].
In turn, for low values of $\psi$---and therefore pairs of long stochastic processes---the means should closely correspond to the EVs, leading to choices that are in line with EV maximization.

To summarize the choice predictions and simplify: 
If the prospect that is more likely to return a higher outcome most of the time does not also possess the higher EV, high values of $\psi$---and therefore pairs of short stochastic processes---are predicted to cause an as-if underweighting of rare outcomes.
While these predictions emphasize the close link between small samples and the as-if underweighting of rare outcomes, they suggest that the reliance on multiple or many small samples can serve the accumulation of evidence for differences in a latent property.
Specifically, for an increasing sequence of independent comparisons between single outcomes sampled from two prospects, the relative winning frequency of a prospect should converge against the objective probability of producing a higher outcome in each comparison.
Respectively, then, the as-if underweighting of rare outcomes should become more robust as the thresholds $\theta$ increase.^[This clarifies the role of $\theta$: Whatever systematic differences between the prospects is reflected in a mean comparison should be more accurately assessed for high values of $\theta$.]

## Cumulative Prospect Theory

Before elaborating how the above model predictions may translate into estimates of the CPT parameters, the following section reviews how CPT in general and its weighting function in particular capture deviations from EV maximization.

### Description of CPT

The choices people make between prospects are often *described* in terms of deviations from EV maximization, according to which the prospect with the largest EV
$$
EV = \sum_i^n x_i \times p_X(x_i)
\tag{10}
$$ 
should be chosen.
To describe how people's choices deviate from EV maximization [see @erevAnomaliesForecastsDescriptive2017, for a recent replication of classical demonstrations],^[<!--Open footnote-->@tverskyAdvancesProspectTheory1992 addressed the *fourfold pattern of risk attitudes*---i.e., choices between prospects indicate risk aversion for gains of high probability and losses of low probability, and risk seeking for gains of low probability and losses of high probability---and related effects, e.g., the *certainty effect* and the *reflection effect*, where the latter were already addressed by CPT's predecessor, prospect theory [@kahnemanProspectTheoryAnalysis1979]. Together, these empirical choice patterns indicate a violation of EV and expected utility (EU) maximization [see @bernoulliExpositionNewTheory1954\/1738].<!--Close footnote-->]
CPT and similar rank-dependent models [see @stottCumulativeProspectTheory2006, for an overview of models] fit choice data by assuming that people maximize the valuation
$$
V = \sum_i^n v(x_i) \times \pi_i
\; ,
\tag{11}
$$ 
with objective outcomes $x_i$ being transformed by a value function $v$, and *cumulative decision weights* $\pi_i$ being determined by the difference between transformed cumulative probabilities of the distribution $P'$.
More specifically, following @tverskyAdvancesProspectTheory1992, the objective outcomes are transformed by a value function
$$
\begin{aligned}
  v : \Omega' &\to \mathbb{R} \\
  x &\mapsto 
  \begin{cases}
     x_i^\alpha &\forall x_i \geq 0, \\
     -\lambda |x_i|^\alpha &\text{else}
     \; ,
  \end{cases}
\end{aligned}
\tag{12}
$$
with $\alpha \in [0,1]$ determining the degree of the function's concavity (convexity) over the positive (negative) outcome interval, and $\lambda > 1$ increasing the function's slope over the negative outcome interval only.
Each subjective value $v(x)$ is then multiplied (or: weighted) with a cumulative decision weight that takes the form
$$
\pi_i =
  \begin{cases}
     w^+(P(X \geq x_i)) - w^+(P(X > x_i)) \quad \forall x_i \geq 0, \\
     w^-(P(X \leq x_i)) - w^-(P(X < x_i)) \quad \text{else}
     \; , 
  \end{cases}
\tag{13}
$$
where $w$ is a monotonic increasing, nonlinear weighting function $w: [0,1] \to [0,1]$ satisfying $w^+(0) = w^-(0) = 0$ and $w^+(1) = w^-(1) = 1$.

Essentially, the model is such that the cumulative decision weights derived for the outcomes may be greater or smaller than their objective probabilities, causing the transformed outcomes---i.e., the subjective values---to be over- or underweighted *in* CPT, respectively.
Accordingly, for the description of choices, one may adopt an as-if weighting terminology by stating that people choose as if they maximized the value in (11) and applied the weighting pattern that was estimated in CPT.
Importantly, the as-if prefix indicates that it cannot be concluded from the estimated weighting pattern that the mind indeed performs any of the computations associated with the weighting of subjective values; 
rather, the mind processes the information about the properties of choice alternatives in a way that the resulting choices translate to the estimated weighting pattern [@gigerenzerHowExplainBehavior2020].
Respectively, this paper treats CPT as a data model and discusses---and refines in a simulation---what weighting patterns can be expected, were the mind to carry out the information-processing chains that are assumed by the proposed sampling and accumulation model [see @pachurHowTwainCan2017; @zilkerNonlinearProbabilityWeighting2021, for similar theoretical inquiries].

Now, several consequences for the weighting patterns that can be estimated in CPT follow from (13), which are briefly reviewed next:

* From the transformation of cumulative probabilities it follows that the value of a cumulative decision weight $\pi$ depends on the rank of the outcome $x$ for which $\pi$ is determined.

* The value of $\pi$ depends on the estimated shape of the graph of $w$, i.e., the weighting function's parameters.
Specifically, the shape of the graph of $w$ displays over which interval on the cumulative probability scale $[0,1]$ the images of $w$ take values that are greater or smaller than the respective cumulative probabilities and how much greater or smaller these images are.
Hence, for a nonlinear graphical shape of $w$, the same numerical difference between objective cumulative probabilities may translate to cumulative decision weights of different value.

* For prospects containing either only positive or only negative outcomes, $\sum_i^n\pi_i = 1$ is satisfied.
Accordingly, for such prospects, the weighting of subjective values with cumulative decision weights rather than with objective probabilities may be roughly understood as a redistribution of the entire probability mass of $P(\Omega') = 1$ across outcomes [@zilkerNonlinearProbabilityWeighting2021].

\noindent
Because the value of each $\pi$ depends on the two transformed cumulative probabilities in (13)---i.e., the probability of obtaining a positive (negative) outcome equal to or greater (smaller) than a respective outcome $x$, and the probability of obtaining a strictly greater (smaller) outcome---the remaining consequences for the possible weighting patterns are reviewed by considering the actual weighting function.
Therefore, Figure\ \@ref(fig:weighting-function) illustrates some of the possible graphical shapes of the two-parameter weighting function of @goldsteinExpressionTheoryPreference1987, which, however, is just one of several parameterizations that have been proposed [e.g., @prelecProbabilityWeightingFunction1998; @tverskyAdvancesProspectTheory1992; see @stottCumulativeProspectTheory2006, for an overview].
Now, let each $p$ on the abscissa be one of the four cumulative probabilities from (13). 
Then each graph in Figure\ \@ref(fig:weighting-function) displays the graphical shape of the weighting function
$$
\begin{aligned}
  w : [0,1] &\to [0,1] \\
  p &\mapsto \frac{\delta \times p^{\gamma}}
  {\delta \times p^{\gamma} + (1-p)^{\gamma}}
  \; ,
\end{aligned}
\tag{14}
$$
for the respective values of the parameters $\gamma \in [0,2]$ and $\delta > 0$.
Evidently, both parameters have distinct effects on the graphs' shape, with $\gamma$ affecting the curvature and $\delta$ the elevation.
As a consequence from (13), then, each combination of parameters implies a particular weighting pattern, with some of them being similar and others being rather distinct.

(ref:weighting-function) Possible Graphical Shapes of Goldstein and Einhorn's -@goldsteinExpressionTheoryPreference1987 Weighting Function

```{r weighting-function, fig.cap="(ref:weighting-function)"}
#compute images of weighting function (wf)
wf <- tibble(p = seq(0, 1, .01)) %>%  #cumulative probabilities
  expand_grid(gamma = seq(.1, 2, .1), #gamma values
              delta = c(.1, .5, 1, 2, 5, 10)) %>% #delta values
  mutate(wp = (delta*(p^gamma))/((delta*p^gamma)+(1-p)^gamma)) #images of wf

#labeller function for facet labels with LateX math expressions 
label_delta <- function(string) {
  TeX(paste("$\\delta=$", string, sep = ""))  
}

#plot shapes of weighting function
wf %>% 
  ggplot(aes(p, wp, group = gamma)) +
  facet_wrap(~delta, labeller = as_labeller(label_delta, default = label_parsed)) + 
  scale_x_continuous(breaks = seq(0, 1, .5)) +
  scale_y_continuous(breaks = seq(0, 1, .5)) + 
  labs(x = expression(p),
       y = expression(w(p)), 
       color = expression(gamma)) + 
  theme_apa() + 
  geom_line(aes(color = gamma), size = .5) +
  scale_color_viridis(option = "inferno") + 
  geom_abline(intercept = 0, slope = 1, color = "gray", linetype = "dashed")
```

More specifically: The gray-dashed identity lines in Figure\ \@ref(fig:weighting-function) imply a linear weighting pattern.
That is, since the images of $w$, $w(p)$, are equal to the respective cumulative probabilities $p$, all cumulative decision weights $\pi$ that are derived according to (13) would be equal to the differences between the respective objective cumulative probabilities---and therefore also to the objective probabilities of the outcomes for which the weights are determined.
Such a linear weighting pattern is implied by $\gamma = 1$ and $\delta = 1$. 
Now, both deviations from $\gamma = 1$ and $\delta = 1$ produce a nonlinear graphical shape.
That is, for $\gamma > 1$, the graphs take a S-shape---running below (above) the identity line for small (large) cumulative probabilities---which is more accentuated for larger deviations from $\gamma = 1$.
Importantly, the same *small* numerical difference between two cumulative probabilities $p$ then translates to varying differences between the respective images $w(p)$ depending on the interval on the cumulative probability scale. 
Specifically, because of the S-shaped curvature, the differences between the images $w(p)$ may be smaller than those between the respective cumulative probabilities in the lower and upper part of the cumulative probability scale, but may be considerably greater in the middle part.
As a consequence, the cumulative decisions weights are smaller than the respective objective probabilities for small-probability outcomes of low and high rank, but they are larger for small-probability outcomes of a middle rank.^[<!--Open footnote-->Note that the distinction between outcomes of either low/high rank or middle rank is unnecessary for at most two-outcome prospects [see @tverskyAdvancesProspectTheory1992, see also below]. However, for finite many outcomes, the distinction is critical. More so, the fact that CPT may weigh outcomes of the same small probability differently, adds another reasons why the description-experience gap should not be equated with a reversed weighting pattern in CPT [see @hertwigConstructbehaviorGapDescription2018]. I.e., neither could one interpretation of the gap be in terms of an as-if underweighting (overweighting) of rare outcomes in DFE (DFD), nor would any of the potential contributors to the gap considered so far [cf. @hertwigDescriptionexperienceGapRisky2009; @wulffMetaanalyticReviewTwo2018] treat some small-probability outcomes differently than other small-probability outcomes.<!--Close footnote-->]
Note that for all $\gamma < 1$ the graph of $w$ takes an inverse S-shape and the entire weighting pattern is reversed.
Moreover, the shape of the weighting function, and thus the weighting pattern, depends strongly on $\delta$, which affects the overall elevation of the graph.
That is, for $\delta < 1$, the interval on the cumulative probability scale over which the weighting function runs below the identity line is greater than the interval over which the weighting function runs above the identity line; and this pattern reverses for $\delta > 1$.
In other words, $\delta$ shifts across the entire cumulative probability scale the intervals within which small differences between objective cumulative probabilities translate to either smaller or greater cumulative decision weights.

To summarize the preceding paragraphs: CPT is a data model that can be used to describe choices between prospects by capturing systematic deviations from EV maximization in the parameter estimates of its value and weighting function.
Because the graph of the weighting function usually takes a more or less accentuated (inverse) S-shape, the implied weighting pattern is such that small probability outcomes of low and high rank are underweighted (overweighted) in CPT's valuation of a prospect according to (11).

### How Is CPT Expected to Reflect the Model Predictions?

To simplify matters, the further discussion and the computational analysis are constrained to choices between a safe prospect---possessing just one outcome that occurs with certainty---and a two-outcome prospect (hereafter: risky prospect).
All outcomes are assumed to be positive and the safe outcome is assumed to fall between the the low-rank outcome, $x_{low}$, and the high-rank outcome, $x_{high}$, of the risky prospect to omit dominance.
A summary of the predictions for the CPT parameters can be found in Table\ \@ref(tab:predictions).

To begin with, note through simplifying (13) to
$$
\begin{aligned}
  \pi_{high} &= w(p_x(x_{high})) \\
  \pi_{low} &= w(p_x(x_{low}) + p_x(x_{high})) - w(p_x(x_{high})) \\
  &= 1 - \pi_{high}
  \; ,
\end{aligned}
\tag{15}
$$
that the possible weighting patterns amount to an over- and underweighting of the high-rank outcome of the risky prospect, depending on its probability.
That is, the weighting function is supplied with the probability of the high-rank outcome of the risky prospect and its images are the decision weights assigned to this outcome.
Respectively, as can be seen from Figure\ \@ref(fig:weighting-function), for $\gamma > 1$, the high-rank outcome of the risky prospect is overweighted (underweighted) in CPT, if it is of large (small) probability, and $\delta \neq 1$ shifts the range of the over- and underweighting.

Now, since for low values of $\psi$ the model tends to choose the prospect possessing the larger EV, the parameters of the value and weighting function in this case are predicted to correspond to a linear weighting of outcomes and probabilities, which is given for $\alpha = 1$, $\gamma = 1$, and $\delta = 1$.
However, for high values of $\psi$, the model tends to choose the risky prospect for $p_x(x_{high}) > .5$---i.e., the risky prospect produces the higher outcome most of the time---, irrespective of the differences in the EV.
Vice versa, the model tends to choose the safe outcome for $p_x(x_{high}) < .5$---i.e., the safe prospect produces the higher outcome most of the time.
Respectively, for high values of $\psi$, the weighting pattern should be such that the high-rank outcome of the risky prospect is overweighted for $p_x(x_{high}) > .5$, and underweighted otherwise.
Hence, the graph of the weighting function should take a S-shape, which is given for $\gamma > 1$.
Because the over- and underweighting of the high-rank outcome is not predicted to extend across the mid-point of the probability scale, $\delta$ should take values around 1.
Note that the graph of the weighting function for high values of $\psi$ reflects the round-wise strategy's high sensitivity to differences in the probability to produce the higher outcome most of the time:
Specifically, the slope of the weighting function's graph is most severe around the midpoint of the probability scale.
The predicted flat ends of the graph, however, indicate that the round-wise strategy more or less ignores how much larger the probability to produce a higher outcome is. 
In sum, then, we do not predict $\delta$ to change with $\psi$.
However, we expect $\gamma$ to increase with $\psi$, thereby reflecting differences in the extent to which the prospect with the higher EV or with the probability to produce a higher outcome most of the time is chosen.

Moreover, because the model is such that the magnitudes of outcomes are ignored once a comparison is carried out, increases in $\psi$ should lead to a more concave value function.
That is, the higher the values of $\psi$, the more the choices depend on which prospect produces the higher outcome most of the time, irrespective of how much larger the outcomes are.
Respectively, $\alpha$ is predicted to decrease with increases in $\psi$.

# Simulation Study

To substantiate and refine the predictions of the model, a computational implementation was used to simulate the sampling and accumulation process for a set of choices between a safe prospect and a two-outcome (risky) prospect.
The computational model was written in R and is embodied in the GitHub repository accompanying this paper.
The simulation was conducted for varying combinations of $\psi$ and $\theta$.
For each of these combinations, the simulated data was modeled with a stochastic version of CPT, using Bayesian parameter estimation.
The simulated choices and estimated CPT parameters can be roughly understood as theory-implied data and data models, which remain to be tested against empirical data [see, e.g., @guestHowComputationalModeling2021; @haslbeckModelingPsychopathologyData2021].

## Method

Below, the simulation and modeling procedure is briefly described.

### Choice Problems

A test set of $60$ choice problems was obtained by stratified sampling from an initial set of $10,000$ problems.
The stratification procedure was used to ensure that the test set contained choice problems with small-probability outcomes of different rank---i.e., small-probability outcomes that are either higher or lower than the EV of the risky prospect---as well as choice problems without small-probability outcomes.
The exact procedure was as follows:
For each of the $10.000$ problems in the initial set, three outcomes were randomly drawn from a uniform distribution over the interval $[0, 20]$, with the smallest and highest of these three outcomes being assigned to the risky prospect to omit dominance.^[I.e., when one prospect always produces a higher outcome.]
The probability for the lower outcome of the risky prospect, $p_x(x_{low})$, was drawn from a uniform distribution over the interval $(0, 1)$, with the probability of the higher outcome being set to $p_x(x_{high}) = 1-p_x(x_{low})$.
To obtain the $60$ choice problems of the test set, the initial set was divided into three subsets, where each subset contained either all problems with $p_x(x_{high}) \in (0,.2)$ or $p_x(x_{high}) \in [.2,.8]$ or $p_x(x_{high}) \in (.8,1)$.
From each of these subset, then, $20$ choice problems were randomly sampled.

### Data Generation

To simulate the sampling and accumulation processes, the following parameter values were used and combined with each other.
That is, each parameter combination was used to solve the test set.

* *Switching probability*: 
$\psi$ was varied in the interval $[.1, 1]$ in increments of $.1$.

* *Threshold*: 
$\theta$ was varied in the interval $[1,5]$ in increments of 1. 

* *Decision variable*: The decision variable of a prospect either counts the number of won comparisons---implying an absolute threshold---or how many more comparisons were won relative to the other prospect---implying a relative threshold.^[The results are similar for absolute and relative thresholds. For simplification, only the results of the absolute thresholds are reported in the main text.]

\noindent
Since $\psi$ introduces some stochasticity into the sampling and accumulation process, each of the 100 resulting parameter combinations was used 100 times---resembling 100 independent agents---to solve the test set. 
In sum, then 100 (parameter combinations) $\times$ 100 (agents) $\times$ 60 (choice problems)  = 600,000 sampling and accumulation processes were simulated.
The respective dataset can be retrieved from the accompanying `GitHub` repository.

### Estimation of Stochastic CPT

The choice data for each parameter combination was separately modeled in CPT, using the value function of @tverskyAdvancesProspectTheory1992 and the weighting function of @goldsteinExpressionTheoryPreference1987.
To demonstrate that the distortions from linear probability weighting are not just due to differences between the latent objective probabilities and the relative frequencies with which the outcomes were actually sampled over the course of a sampling and accumulation process, the latter were supplied as probability information to the weighting function.
To accommodate for possible random choice behavior that may be exerted by a parameter combination, the CPT model was amended with the logit choice rule
$$
p(P'^{safe}) = \frac{1} {1 + e^{-\rho(V_{safe}^{\frac{1}{\alpha}}-V_{risky}^{\frac{1}{\alpha}})}}
\; ,
\tag{16}
$$
where $p(P'^{safe})$ is the probability that the safe prospect is chosen over the risky prospect, $V_{safe}$ and $V_{risky}$ are the valuations of the safe and risky prospect that result from the parameter estimates of the value and weighting function, and $\rho \geq 0$ is a sensitivity parameter.
$\rho$ governs how strongly the choice between prospects depends on the differences in the valuations of the prospects, i.e., the core CPT model.
Specifically, $\rho = 0$ implies no sensitivity, and thus a random choice.
In turn, for increasing values of $\rho$, the probability of choosing the prospect with the higher valuation increases.
Note further that the valuation of the prospects were rescaled by $\frac{1}{\alpha}$, as proposed by @stewartPsychologicalParametersHave2018, to guard against possible parameter interdependencies between $\alpha$ and $\rho$ [@krefeld-schwalbStructuralParameterInterdependencies2022; @scheibehenneUsingBayesianHierarchical2015].
In sum, then, the model has four free parameters: $\alpha, \gamma, \delta, \rho$.

Since for a given choice problem and parameter combination of the sampling and accumulation model there should be no systematic differences among the 100 iterations (synthetic agents), their data is combined for the estimation of the free parameters.
Such a grouped approach should reveal the main effects of a given parameter combination of the sampling and accumulation model on the resulting choices and ignore unsystematic differences or noise that may be caused by the stochasticity introduced by $\psi$.
To facilitate comparisons across parameter combinations of the sampling and accumulation model, uninformative prior distributions were used throughout for all four parameters.
Specifically, the prior distributions $\alpha \sim U(0,1)$, $\gamma \sim U(0,2)$, $\delta \sim U(0,10)$, and $\rho \sim U(0,5)$ were used.  
 
```{r data, include=FALSE}
#load choice data
cols_choices <- list(.default = col_double(),
                     boundary = col_factor(),
                     gamble = col_factor(),
                     rare = col_factor(),
                     agent = col_factor(),
                     choice = col_factor())
choices <- read_csv("data/simulation_summary.csv", col_types = cols_choices)
choices <- choices %>% 
  filter(boundary == "absolute")

#load CPT estimates
cols_cpt <- list(.default = col_double(),
                 boundary = col_factor(),
                 a = col_factor(),
                 parameter = col_factor())
cpt <- read_csv("data/cpt_parameters.csv", col_types = cols_cpt)
cpt <- cpt %>% 
  filter(boundary == "absolute")
```

To estimate the posterior distributions of the free parameters, Markov Chain Monte Carlo sampling as implemented in JAGS was used, where 20 chains of 40,000 samples each were run after a warm-up period of 1000 samples.
To reduce potential autocorrelation during the sampling process, only every 20th sample was kept (thinning), leaving a total of 40,000 samples across chains.
The minimum effective sample size was `r min(cpt$n.eff)`.
The potential scale reduction factor $\hat{R}$ [@gelmanInferenceIterativeSimulation1992] was $\leq$ `r round(max(cpt$Rhat), 4)` for all models and parameters, indicating good convergence.

## Results

Overall, the results of the simulation match the predictions from Table\ \@ref(tab:predictions), however, with some exceptions concerning the estimates for the parameter $\delta$ of the weighting function. 
We start by inspecting how the length of the information-processing chain changes with varying combinations of $\psi$ and $\theta$.

### Simulated Length of the Information-Processing Chains

Figure\ \@ref(fig:chain-length) displays the median number of outcomes sampled over the course of a sampling and accumulation process for each parameter combination.
The model predicts a decrease in the overall number of sampled outcomes with increases in the switching probability $\psi$.
As can be seen from the change in the slopes of the lines in Figure\ \@ref(fig:chain-length), the magnitude of this decrease diminishes as the relative increase in $\psi$ diminishes.^[<!--Open footnote-->I.e., the ratio of high values of $\psi$ is smaller than the ratio of small values of $\psi$.<!--Close footnote-->]
Moreover, the overall number of sampled outcomes increases with thresholds at a relatively constant rate.
Consider also the dashed horizontal line in Figure\ \@ref(fig:chain-length), which displays the median number of 14 outcomes that @wulffMetaanalyticReviewTwo2018 found to be sampled in 10,712 trials involving a choice between a safe and a two-outcome prospect in the sampling paradigm.
Notably, the parameter combinations that closely resemble the round-wise strategy---i.e., high values of $\psi$ and $\theta$---or the summary strategy---i.e., low values of $\psi$ and $\theta$---lead to overall numbers of sampled outcomes that come close to the meta-analytic median.
Moreover, while parameter combinations that can be understood as falling in between the round-wise and the summary strategy also lead to overall numbers of sampled outcomes that approximate the meta-analytic median, extreme combinations produce numbers that are considerably lower or higher, and thus less plausible from an empirical stance.

(ref:chain-length) Lengths of Information-Processing Chains

```{r chain-length, fig.cap="(ref:chain-length)"}
choices %>%
  group_by(s, a) %>% 
  summarize(n_med = median(n_sample)) %>% 
  ggplot(aes(x = s, y = n_med, group = as.factor(a), color = as.factor(a))) + 
   labs(x = expression(psi),
       y = "Median of Overall Sampled Outcomes",
       color = expression(theta)) +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, .5)) +
  scale_y_continuous(limits = c(0, 120), breaks = seq(0, 150, 50)) +
  geom_point() +
  geom_line() + 
  geom_hline(yintercept = 14, linetype = "dashed", color = "gray") +
  annotate(geom="text", x=.95, y=16, label="Wulff et al. (2018)", color="gray") + 
  scale_color_viridis_d(option = "plasma") +
  theme_apa()
```


### Simulated Final Choice

Figure\ \@ref(fig:choice-rates) displays the rates of choices that did not maximize the latent EV (hereafter: false response rates) for each combination of $\psi$ and $\theta$.
To demonstrate that the deviations from EV maximization are not just due to sampling error, the rates of choices that did not maximize the sampled mean are displayed in gray. 
Rates are separated for presence and rank of small-probability outcomes.
Consider first the top panel showing the false response rates across all problems where the low-rank outcome of the risky prospect possesses a small probability:
Over the course of short stochastic processes, then, the low-rank outcome tends to be sampled less often than would be objectively warranted, leading to an inflation of the respective mean of the risky prospect.
Accordingly, the top panels show that the rates of false choices of the risky prospect increase with $\psi$, but the rates of false safe choices remain generally low.
More specifically, the higher the values of $\psi$, the more severe should the inflation of the mean of the risky prospect be. 
As a consequence, if the risky prospect possesses the smaller EV, an increase in $\psi$ increases the risk to deviate from EV maximization---taking the form of an as-if underweighting of rare outcomes---by falsely choosing the risky prospect on the basis of a mean comparison.
In turn, if the risky prospect possesses the larger EV, the inflation of their means decreases the risk to falsely choose the safe prospect.
Note from the middle panel that the pattern of false response rates reverses for problems where the high-rank outcome of the risky prospect possesses a small probability.
In this case, then, the mean of the risky prospect tends to be more deflated the higher the value of $\psi$, which increases the risk of falsely choosing the safe prospect.
From the reduced false response rates in the bottom panel, we see that the inflation and deflation of the means for high values of $\psi$ is significantly attenuated when a prospect possesses no small-probability outcome.  

(ref:choice-rates) Rates of False Risky and False Safe Choices

```{r choice-rates, fig.cap="(ref:choice-rates)"}
#prepare data

## determine normative choice according to latent EV and sampled mean

### latent EV

fr_rates_EV <- choices %>%
  mutate(norm = case_when(ev_ratio > 1 ~ "A", ev_ratio < 1 ~ "B")) %>% # determine normative choice
  filter(!is.na(norm)) %>% #exclude trials with equal EV
  group_by(s, boundary, a, rare, norm, choice) %>% #separate by model parameter and type of rare event
  summarise(n = n()) %>%
  mutate(rate = round(n/sum(n), 2), #compute response rates
         type = case_when(norm == "A" & choice == "B" ~ "Safe", #determine false response type
                          norm == "B" & choice == "A" ~ "Risky")) %>%
  ungroup() %>%
  filter(!is.na(type)) %>% #remove correct responses
  select(-c(norm, choice, n)) %>% 
  mutate(rare = case_when(rare == "none" ~ "\\in \\[.2,.8\\]", #change facet labels for rare events
                          rare == "attractive" ~ "\\in (0,.2)",
                          rare == "unattractive" ~ "\\in (.8,1)"),
         Norm = "EV")

### sampled mean

fr_rates_mean <- choices %>%
  mutate(norm = case_when(a_ev_exp/b_ev_exp > 1 ~ "A", a_ev_exp/b_ev_exp < 1 ~ "B")) %>% # determine normative choice
  filter(!is.na(norm)) %>% #exclude trials with equal EV
  group_by(s, boundary, a, rare, norm, choice) %>% #separate by model parameter and type of rare event
  summarise(n = n()) %>%
  mutate(rate = round(n/sum(n), 2), #compute response rates
         type = case_when(norm == "A" & choice == "B" ~ "Safe", #determine false response type
                          norm == "B" & choice == "A" ~ "Risky")) %>%
  ungroup() %>%
  filter(!is.na(type)) %>% #remove correct responses
  select(-c(norm, choice, n)) %>% 
  mutate(rare = case_when(rare == "none" ~ "\\in \\[.2,.8\\]", #change facet labels for rare events
                          rare == "attractive" ~ "\\in (0,.2)",
                          rare == "unattractive" ~ "\\in (.8,1)"),
         Norm = "Mean")

fr_rates <- bind_rows(fr_rates_EV, fr_rates_mean)

# labeller functions
label_theta <- function(string) {
  TeX(paste("$\\theta=$", string, sep = "")) #threshold parameter theta
}

label_rare <- function(string) {
  TeX(paste("$\\p_{High}$", string, sep = "")) #type of rare event
}

#plot

fr_rates_risky <- fr_rates %>% filter(type == "Risky")
fr_rates_safe <- fr_rates %>% filter(type == "Safe")

fr_rates_risky %>%
  ggplot(aes(s, rate, color = Norm)) +
  facet_grid(rare~a, labeller = labeller(rare = as_labeller(label_rare, default = label_parsed), 
                                         a = as_labeller(label_theta, default = label_parsed))) +
  labs(x = expression(psi),
       y = "False Response Rate",
       color = "Norm",
       shape = "False\nResponse") +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, .5)) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, .5)) +
  geom_point(aes(shape = type), size = 2) +
  geom_line() + 
  geom_point(data = fr_rates_safe, aes(shape = type), size = 2) +
  geom_line(data = fr_rates_safe) +
  scale_shape_manual(values = c(4, 16)) +
  scale_color_viridis_d(option = "magma", begin = .3, end = .8) +
  theme_apa()
```

### Simulation-Implied Cumulative Prospect Theory

Figure\ \@ref(fig:weighting-parameters) displays the estimated means and the 95%-intervals of the posterior distributions of the curvature parameter $\gamma$ and the elevation parameter $\delta$, as well as the resulting graphical shape of the weighting function.
Note that the estimates for the parameter combination $\theta = 1$ and $\psi = 1$ have large posterior intervals.
In these cases, no relative frequencies other than 0 and 1 are supplied to the weighting function, which may explain that the values in between cannot be reliably accounted for.
However, for $\theta = 1$ and $\psi < 1$, the estimates imply a linear weighting pattern, in line with the explanation that if the mind were to infer the latent objective probabilities of outcomes from the sampled relative frequencies and to follow the principle of EV maximization, sampling error can account for any deviations from it.^[<!--Open footnote-->Note that the same line of reasoning holds, were the mind to approximate the solutions of EV maximization by choosing the prospect with higher sampled mean.<!--Close footnote-->]
Note also Figure\ \@ref(fig:choice-rates), where there are no choices for $\theta = 1$ that do not maximize the sampled mean.

For $\theta > 1$, the estimates substantiate the prediction that $\gamma$ takes values $\geq 1$ which increase with $\psi$, resulting in a increasingly pronounced S-shaped weighting function.
In other words, the higher the value of $\psi$, the more severe is the underweighting (overweighting) of the high-rank outcome of the risky prospect in CPT, if it is of small (large) probability.
$\delta$ takes values $\geq 1$ and increases slightly with $\psi$, causing the overweighting of the high-rank outcome of the risky prospect to extend across the mid-point. [...]

(ref:weighting-parameters) Parameter Estimates and Graphical Shape of the Weighting Function

```{r weighting-parameters, fig.cap="(ref:weighting-parameters)"}

#plot estimates

##gamma
gamma <- cpt %>%
  filter(parameter == "gamma") %>%
  ggplot(aes(s, mean)) +
  facet_wrap(~a, nrow = 1, labeller = labeller(a = as_labeller(label_theta, default = label_parsed))) +
  scale_y_continuous(limits = c(0,2), breaks = seq(0,2.0,1.0))+
  labs(x = element_blank(), #omit axis title
       y = expression(gamma)) +
  geom_errorbar(aes(ymin=`2.5%`, ymax=`97.5%`)) + 
  geom_point() +
  theme_apa() +
  theme(axis.line.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.x = element_blank())

##delta
delta <- cpt %>%
  filter(parameter == "delta") %>% 
  ggplot(aes(s, mean)) +
  facet_wrap(~a, nrow = 1) + 
  scale_y_continuous(limits = c(0,10), breaks = seq(0,10,5)) +
  scale_x_continuous(limits = c(0, 1.1), breaks = seq(0,1,.5)) +
  labs(x = expression(psi),
       y = expression(delta)) +
  geom_errorbar(aes(ymin=`2.5%`, ymax=`97.5%`)) + 
  geom_point() +
  theme_apa() + 
  theme(strip.text.x = element_blank()) # omit facet labels

wf <- cpt %>%
  select(s, boundary, a, parameter, mean) %>%
  pivot_wider(names_from = parameter, values_from = mean) %>%
  select(-c(alpha, rho)) %>%
  expand_grid(p = seq(0, 1, .05)) %>%
  mutate(w = round(  (delta * p^gamma)/ ((delta * p^gamma)+(1-p)^gamma), 2))

##weighting function
weight <- wf %>% #graphs for absolute boundary
  ggplot(aes(p, w, group = s, color = s)) +
  facet_wrap(~a, nrow = 1, labeller = labeller(a = as_labeller(label_theta, default = label_parsed))) + 
  labs(x = "Experienced Probability",
       y = expression(pi),
       color = expression(psi)) +
  scale_x_continuous(breaks = seq(0, 1, .5)) +
  scale_y_continuous(breaks = seq(0, 1, .5)) +
  geom_line(size = .5) +
  scale_color_viridis(option = "viridis") +
  theme_apa() + 
  theme(strip.text.x = element_blank()) #omit facet labels


##merge figures
ggarrange(gamma, delta, weight, ncol = 1, nrow = 3, common.legend = TRUE, legend = "right")
```

Figure\ \@ref(fig:value-parameters) displays the estimated means and the 95%-intervals of the posterior distributions of the outcome sensitivity parameter $\alpha$ and the choice sensitivity parameter $\rho$, as well as the shape of the value function.
An increase in $\psi$ leads to a decrease in $\alpha$, reflecting that outcome information are largely ignored if choices are based on ordinal comparisons of single outcomes rather than on means of large samples.
The respective shape of the value function is more concave, the higher the values of $\psi$. 
Finally, $\rho$ varied systematically such that the choices were more sensitive to the valuations of the core CPT model for low values of $\psi$.
However, even for high values of $\psi$, the choices did not appear to be random. [...]

(ref:value-parameters) Parameter Estimates and Graphical Shape of the Value Function

```{r value-parameters, fig.cap="(ref:value-parameters)"}
#plot estimates

##alpha
alpha <- cpt %>%
  filter(parameter == "alpha") %>% 
  ggplot(aes(s, mean)) +
  facet_wrap(~a, nrow = 1, labeller = labeller(a = as_labeller(label_theta, default = label_parsed))) +
  scale_x_continuous(limits = c(0, 1.1), breaks = seq(0,1,.5)) +
  scale_y_continuous(limits = c(0,1), breaks = seq(0,1, .5)) + 
  labs(x = element_blank(),
       y = expression(alpha),
       color = expression(psi)) +
  geom_point() +
  geom_errorbar(aes(ymin=`2.5%`, ymax=`97.5%`)) + 
  theme_apa() + 
  theme(axis.line.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.x = element_blank())

###value function
vf <- cpt %>%
  select(s, boundary, a, parameter, mean) %>%
  pivot_wider(names_from = parameter, values_from = mean) %>%
  select(-c(gamma, delta, rho)) %>%
  expand_grid(x = seq(0, 20, .5)) %>%  
  mutate(v = round(x^alpha, 2)) 

value <- vf %>% 
  ggplot(aes(x, v, group = s, color = s)) +
  facet_wrap(~a, nrow = 1)+ 
  labs(x = "Objective Outcome",
       y = "Subjective Value",
       color=expression(psi)) +
  scale_x_continuous(limits = c(0,20), breaks = seq(0, 20, 10)) +
  scale_y_continuous(limits = c(0,20), breaks = seq(0, 20, 10)) +
  geom_line(size = .5) +
  scale_color_viridis(option = "viridis") +
  theme_apa() + 
  theme(strip.text.x = element_blank())

##rho
rho <- cpt %>%
  filter(parameter == "rho") %>% 
  ggplot(aes(s, mean)) +
  facet_wrap(~a, nrow = 1) + 
  labs(x = expression(psi),
       y = expression(rho),
       color = expression(psi)) +
  scale_x_continuous(limits = c(0, 1.1), breaks = seq(0, 1,.5)) +
  scale_y_continuous(limits = c(0, 5), breaks = seq(0, 5,2.5)) +
  geom_point() +
  geom_errorbar(aes(ymin=`2.5%`, ymax=`97.5%`)) + 
  theme_apa() + 
  theme(strip.text.x = element_blank())

##merge figure
ggarrange(alpha, rho, value, ncol = 1, nrow = 3, common.legend = TRUE, legend = "right") 
```

# Summary and Discussion

The presented sampling and accumulation model ought to sketch how the mind might exercise the capacity of making decisions from experience.
In DFE, one cannot rely on symbolic descriptions of all relevant properties of the choice alternatives, but only on samples of past experiences, e.g., when the alternatives were chosen in similar decisions in the past.
To study DFE experimentally, one may use the sampling paradigm where people can sequentially sample the outcomes of prospects according to their probability of occurrence before making a final choice.
To model the process by which people arrive at the final choice in this paradigm, one must explicate a number of assumptions, e.g.: How do people decide when to switch between prospects; how do they integrate the information obtained from the sampled outcomes; and, because they can sample as much as they want, how do they decide to stop sampling.
To approach these questions, this paper presented a model that integrated a proposed link between sampling and decision strategies into the computational framework of sequential sampling models.

## Modeling Sampling Strategies and the Decision-Making Process

@hillsInformationSearchDecisions2010 proposed---and provided tentative empirical support for---a systematic relation between what they considered two paradigmatic sampling and decision strategies. 
That is, the authors assumed that people either switch prospects after each sampled outcome (piecewise sampling) to compare single outcomes over repeated rounds (round-wise decision strategy), or they switch only once between prospects (comprehensive sampling) and make only one comparison between the means across all sampled outcomes (summary decision strategy).
In sequential sampling models [see, e.g., @ratcliffComparisonSequentialSampling2004], the decision-making process is modeled as accumulation of evidence for or against a prospect over time, with the prospect being chosen for which the accumulated evidence first reaches a threshold.
The sampling and accumulation model that was presented in this paper integrates both propositions by assuming that people accumulate the wins and losses resulting from a sequence of mean comparisons, with switching probabilities determining the size of the samples underlying each comparison and thresholds determining how much (more) comparisons a prospect must win in order to be chosen.
Hence, the model treats the two paradigmatic sampling and decision strategies as the extreme ends of possible continua, allowing the switching probability not only to take values close to 0 and 1 but also values in between, and allowing single and multiple mean comparisons between prospects to be based on samples of varying size.
Moreover, the model explicates the suggested link between sampling and decision strategies by assuming that the accumulation process more closely resembles the round-wise strategy for high switching probabilities, and, vice versa, the summary strategy for low switching probabilities.
This is because high (low) switching probabilities cause the accumulation process to unfold over mean comparisons based on samples of small (large) size.

## Sampling Strategies Might Shape Final Choice

In a computational analysis, the presented model was used to simulate how the proposed sampling and accumulation processes shape the final choice.
Therefore, the switching probabilities and evidence thresholds were systematically varied, while simulating choices between a safe and a risky prospect.
The model predicts the choices to vary systematically with the switching probability.
Specifically, whereas the choice patterns correspond closely to the solutions of EV maximization for low switching probabilities, increases in the switching probability can lead to an increasingly pronounced as-if underweighting of rare outcomes pattern.
This is because for high switching probabilities, each independent mean comparison is based on small samples, in which small-probability outcomes are underrepresented rather than overrepresented.
As a consequence, then, small-probability outcomes are predicted to contribute to the accumulated evidence less rather than more than would be warranted by their objective probabilities.
Accordingly, the analyses showed that the structure of the choice problem---i.e., whether a small-probability outcome is present and which value it takes---is an important moderator for the effect of sampling strategies on the choice pattern. 
Importantly, while this explanation emphasizes that the reliance on small samples is a strong contributor to the as-if underweighting of rare outcomes pattern in DFE, it has little to do with the common notion of sampling error [e.g., @foxDecisionsExperienceSampling2006; @hertwigDecisionsExperienceEffect2004].
That is, the as-if underweighting of rare outcomes is not caused by a deviation of the sampled relative frequencies from the objective probabilities, but by the way the sampled outcomes are integrated and processed to evaluate the prospects.
The analyses made this clear by showing that an as-if underweighting of rare outcomes pattern occurred, even if the sampling error is controlled for.
In sum, by explicitly modeling their link, the current computational analysis showed how the interplay of sampling and decision strategies can shape the final choice.

## CPT Can Reflect the Adoption of Sampling Strategies

The current paper further extended the analysis of @hillsInformationSearchDecisions2010 by showing that the effects of sampling and decision strategies on the final choice can translate to characteristic signatures in cumulative prospect theory's value and weighting function. 
Having controlled for sampling error, the estimated CPT parameters and the resulting shapes of the value and weighting function primarily reflected how sensitive the combinations of sampling and decision strategies are to particular changes in the outcomes and their probabilities.
Thereby, the analyses extends the notion that CPT can be used as a measurement tool to characterize information processing [@pachurHowTwainCan2017] from risky to uncertain choice.

## Why Sampling and Decision Strategies?

[...]

# Conclusion


\newpage

# References
